{"documentId": "lecture1", "section": [{"text": "As you probably have noticed, I put up the gear on my shared screen, the part of the webpage most of the class is driven from the schedule. I'll talk a little bit later about it, but hopefully you find the URL and you found the schedule. I'll return to that a little bit later in more detail.", "metadataJson": "{\"start\":1,\"end\":22}"}, {"text": "So what's the plan for today?", "metadataJson": "{\"start\":25,\"end\":27}"}, {"text": "I'm going to talk a little bit about what is a distributed system. So what is it? And maybe give a little bit of historical context, you know, how distributed systems have developed over the last couple of decades, then hit a little bit on the core structure, like what you should expect, then talk what are the main topics or the main recurring topics that we'll see throughout the term? And then we'll see actually first illustration of those main topics by the case study that was assigned for today, the paper Mapreduce, which is also the topic of the first lab. And if you watch the piazza, you know, we just posted that lab on Piazza URL so that you can get going, and it's due next Friday.", "metadataJson": "{\"start\":30,\"end\":89}"}, {"text": "All right, so let's start with the basics. I'll talk a little bit about what is a distributed system, and sort of, you know, maybe you should start with a little picture. The Internet cloud computers connected to these clients. And it may be servers, maybe have servers that actually are complete data centers.", "metadataJson": "{\"start\":90,\"end\":122}"}, {"text": "Clients and the data centers themselves may be internally distributed systems that are connected by internal networks. The data centers themselves might have internal connections outside of the Internet with a large collection of computers connected by networks. And informally, the way I think about it, what a distributed system is that there's a multiple, more than one computer networked, so they can interact only through sending and receiving packets, as opposed to, say, a multiprocessor, where you can interact through shared memory, and they're cooperating to deliver some service.", "metadataJson": "{\"start\":129,\"end\":169}"}, {"text": "Those are sort of the four keywords that define for me, distributed systems. Often you might not be aware that you're interacting in the distributed system. You might be using some clients, for example, the Zoom client. But at the back end on the zoom client, there are huge data centers, or multiple data centers supporting actually this particular distributed application. And in some ways, we wouldn't be having these zoom lectures if there weren't the, even there weren't distributed systems.", "metadataJson": "{\"start\":174,\"end\":205}"}, {"text": "So they often form the backbone of the infrastructure that supports applications. Okay, so why are distributed systems interesting? Or what are the main sort of use cases for distributed systems?", "metadataJson": "{\"start\":206,\"end\":227}"}, {"text": "And those are, broadly speaking, there are basically four main reasons. One is to just connect physically separated machines you might have all of us, many of us, as we just saw in the introduction, in different locations, and yet we're connecting with our laptop or our phone or our iPad to some server that actually sits in a completely different part of the world. That's probably the basic reason why you care about distributed systems, because, you know, just one, they have two machines that are physically separated in space and you want to connect to them. And once you can connect them, you know, that has an additional benefit that actually allows sharing between users.", "metadataJson": "{\"start\":230,\"end\":291}"}, {"text": "So if you and I can actually connect to the same computer, then actually we can start sharing data. And, you know, that enables all kinds of collaborative possibilities. And you know, whether it is like file sharing, you know, whether it is sharing of screens, you know what it's sharing of computing infrastructure, it's all enabled because we can connect to physically separated machines. So they're probably a very important reason. There are a couple other really important reasons.", "metadataJson": "{\"start\":293,\"end\":321}"}, {"text": "One is to another, one is to increase capacity through parallelism.", "metadataJson": "{\"start\":322,\"end\":330}"}, {"text": "The paper that we assigned for today. And what is the topic of the first lab, the Mapreduce paper, there was a good example of that. But the other example is, for example, there are many, many Zoom sessions going on at the same time. Zoom.com has the support at all, and it requires a lot of computers to basically increase the capacities, and you can support all those in parallel zoom sessions. Another important reason is to tolerate faults.", "metadataJson": "{\"start\":334,\"end\":360}"}, {"text": "So, for example, because the computers might be physically separated, one part can go down and hopefully won't affect another part of the service. So that service can always be delivered, so you can get high availability. We'll see that as a major theme for this class. And then the final one is going to be also takes advantage of this physical separation, which is to achieve security.", "metadataJson": "{\"start\":365,\"end\":392}"}, {"text": "For example, if you have a very sensitive service, like the service that manages your password for your customers or for login to your service, you would like to really guard that one machine and not share it with anybody else, or not share any other application or run any applications on it. You have a very narrow interface to that machine and it allows you hopefully to get better security, because you just have to protect that one small interface. And so by putting things on separate computers and isolate them, you might actually be able to. It's a good stepping stone to get security. These are some major reasons, main, four reasons, I think, why one wants to, why distributed systems are popular.", "metadataJson": "{\"start\":397,\"end\":447}"}, {"text": "I want to talk a little bit about giving a little bit of a historical context for distributed systems. And so where they came from and what sort of has happened over the, over the decades, actually, and sort of basically sort of the student systems as we sort of now look at them or the way we recognize them, probably started around the same time that local area networks happened here, think early eighties.", "metadataJson": "{\"start\":449,\"end\":488}"}, {"text": "And so, for example, you would have a campus network like at MIT, and connecting, for example, the workstations, like in Athena clusters, to the Athena servers, like AFS. And so that was sort of the sort of your typical distributed system at that point. And afs also dates from that period of time. Of course, the Internet was there too, but there was really not sort of large scale Internet applications the way we are using them now. And so the main sort of Internet scale side type distributed systems was DNS, the domain name system we still use, and basically email.", "metadataJson": "{\"start\":491,\"end\":526}"}, {"text": "And so when I early, in the early days when I lectured to stupid systems, those are basically the main examples that we had to discuss. Now, things have changed quite dramatically since the 1980s, and the importance of distributed systems has tremendously increased. And one significant point was data centers, the rise of data centers that went along with basically the big websites.", "metadataJson": "{\"start\":528,\"end\":556}"}, {"text": "And here we're talking sort of the, roughly speaking, in the 1990s or early 1990s. And so what happened basically is that, you know, somewhere in the late eighties and early eighties, the government or Congress allowed commercial traffic on the Internet. And that basically resulted in a boom where, you know, you started getting big websites that were supporting large number of users. And, you know, the applications from those times are like, for example, you know, web search, being able to search all the different web pages that actually were on the World Wide Web shopping. And so these applications gave rise to two sort of things.", "metadataJson": "{\"start\":559,\"end\":605}"}, {"text": "One, huge datasets, sort of indexing. To support web search, you have to index all the web pages on the Internet. So that means like you have to gather, crawl all the web pages, then computer refers index, and then you could use that for your search engine. That was just a tremendous amount of data that didn't fit on one computer. And the amount of computation to actually do the first indexing, there was also too much for a single computer.", "metadataJson": "{\"start\":605,\"end\":627}"}, {"text": "As a result, the data centers came about where companies started to put lots and lots of computers in data centers so that they can support those kinds of applications. That's one, a lot of data. And the second one is just a lot, a lot of users, not uncommon for big websites. They have hundreds of millions of users, and that just requires a lot of machines to actually support all those users. And so we see a tremendous amount of innovation in this period of time, it's still continuing.", "metadataJson": "{\"start\":627,\"end\":657}"}, {"text": "And some of the papers that we read, like the Mapreduce paper, actually sort of started from that period of time. That whole thing sort of sort of excellent. That whole development accelerated with the emerging of cloud computing, another early, mid, late two thousands. And so here where we see a move where users or customers basically move their computation and the data to data centers run by other people, like Amazon, Google, Microsoft, you name it. And so a lot of the computation, daily computation that people just used to run on their, you know, their desktop or on the laptop, just moves inside of the cloud computing.", "metadataJson": "{\"start\":657,\"end\":708}"}, {"text": "And like, application change, you know, all. And instead of like running an application on your local computer, you run actually the application inside of the cloud. And that means, you know, that these data centers, you know, had to grow further and support this new set of applications. And not only that, you know, the sort of the customers that were outsourcing their computing to cloud computing also started to run large websites themselves and do gigantic computations on themselves, whether it's machine learning or large datasets or any other kind of type of computation. And so you see is that the users themselves wanted to build large scale distributed systems.", "metadataJson": "{\"start\":708,\"end\":750}"}, {"text": "And that meant, like the cloud providers started building a lot of infrastructure to allow other people to scale up their systems through a large number of machines and achieve high parallelism, high performance, and store lots of data. And so as a result, what the current state is basically that it's a very active area of research as well as development.", "metadataJson": "{\"start\":750,\"end\":777}"}, {"text": "In fact, so hard or so active that it's difficult to sort of keep up to date. There's a lot, a lot of developments, and even in this class, we're going to spend some full semester in distributive systems. We're going to only be able to sort of look at a number of small fraction of all the stuff, all the kind of distributed systems, actually, that people are building in practice. Now, one thing that is cool for us as teachers and students of distributed systems is that the people that built these data centers early on, even though they were building distributed system for their own internal infrastructure, they published papers about it. And we can read those papers.", "metadataJson": "{\"start\":780,\"end\":825}"}, {"text": "And so, in fact, during the semester, we'll read a number of those papers that were built by people that really have large scale distributed system challenges, and we can see how they were solved and learn from them. This accelerated even more with cloud computing, where in the early days of data centers, many of these services were internal for either for Microsoft or Google or Amazon or Yahoo or for them themselves. With the rise of cloud computing, these services became public services that were used by other people. And so suddenly there's even more sort of systems infrastructure that is well documented and usable. And so we can even, we will study some of those cases too.", "metadataJson": "{\"start\":825,\"end\":868}"}, {"text": "And so if you sort of look over these sort of four decades, there's a tremendous rise of the importance of distributed computing. As I said earlier, I did my doctoral thesis in distributed systems actually somewhere in the 1980s, and it was like, it was an important field, but didn't blow you away in terms of significance. And the practicality was sort of limited to more of these local area clusters. And now it's just like completely booming research field and development field.", "metadataJson": "{\"start\":869,\"end\":901}"}, {"text": "Any questions a little about the historical context for distributed systems?", "metadataJson": "{\"start\":903,\"end\":908}"}, {"text": "Okay, let me talk a little bit about the challenges, and many of them you're going to face head on in the labs. So why is it hard? And were basically spending a semester learning about distributed systems. And there's sort of two things that drive the complexity and why distributed systems are hard. One is there are many concurrent parts, these data warehouses.", "metadataJson": "{\"start\":915,\"end\":955}"}, {"text": "Today the computers, you know, run 10,000, 10,0000 computers in parallel, and sometimes not all in the same like we see in the Mapreduce paper today, which is like from the early nineties, you know, 2000 machines, you know, trying to work on one single problem. So there's a lot of concurrent, you know, there's a lot of concurrent software, a lot of things happening concurrently. And so very hard to reason that through like what? And understand why, you know, things are correct. And this is compounded by the fact that, you know, these two systems must deal with, with partial failure.", "metadataJson": "{\"start\":955,\"end\":990}"}, {"text": "So, you know, one of these machines actually might go down, but that doesn't mean that the whole computation stops. In fact, you know, the rest of the machines probably, hopefully can continue running and maybe, you know, take over some of the responsibility of the machine that failed. But this drives, you know, these two things together basically drive complexity because it becomes harder and harder to reason about why the system actually is working. And particularly partial failures make things very complicated because one part of the system might think that another part of the system is down, but it's not really the case. The only thing that might actually happen is that there's a network partition.", "metadataJson": "{\"start\":998,\"end\":1035}"}, {"text": "And so both sides of the distributed system basically are keep on computing and maybe interact with clients, maybe even interact with the same set of clients because the clients can talk to do both parts, but the two into halves cannot talk to each other. And so this is a problem known as the split brain syndrome. And that makes designing distributed systems and protocol statistic systems complicated, as we'll see. So there's really sort of deep intellectual problems here. Then the final sort of really aspect in terms of challenges, it's actually tricky to realize the performance benefits that in principle are possible with distributed systems.", "metadataJson": "{\"start\":1035,\"end\":1080}"}, {"text": "So, so far I've mostly been talking of like, you know, you want to increase your capacity or you want to run things more in parallel. You buy more machines or you buy another data center and, you know, of course, you know, only when the task is completely embarrassing, parallel does that work. And often in practice that is just not the case. And so actually achieving that sort of high throughput and throughput scaling with the number of machines turns out to be not straightforward at all.", "metadataJson": "{\"start\":1085,\"end\":1111}"}, {"text": "So that brings me to the next topic, like why it takes 6824 at least, you know. So I think there's four reasons. One, it's interesting, as I said, like hard technical problems and with very powerful solutions. So hard problems but powerful solutions, we'll see, you know, those solutions, you know, through the terminal. Second reason is they're used in the real world and there's an enormous amount of appetite, you know, for people that actually understand and can build distributed systems.", "metadataJson": "{\"start\":1114,\"end\":1173}"}, {"text": "If you're a grad student or an undergrad and thinking about research, you know, it's a great area because it's a very active area of research.", "metadataJson": "{\"start\":1173,\"end\":1179}"}, {"text": "There's still many open problems and as we go through the semester, you know, we'll encounter them. So it's a good area for research. And finally, you know, if you like building things, it's sort of a unique style of programming. And so in case of 824, you're going to get hands on experience with that by building, you know, distributed, distributed systems in the labs. And you'll discover that one is hard to get them right and it sort of builds up another skill, type of skill of programming that you might not have done in the past.", "metadataJson": "{\"start\":1183,\"end\":1220}"}, {"text": "Let me pause for a second here and see if there are any questions. Also feel free to post in the chat. I'll try to monitor chat if there are questions there or raise your hand if you have any questions. And I'm sure the tas will also be paying attention to the raising hands and the chat. So in case I missed something, they'll remind me.", "metadataJson": "{\"start\":1223,\"end\":1244}"}, {"text": "Any questions? So far, everything is crystal clear.", "metadataJson": "{\"start\":1245,\"end\":1248}"}, {"text": "I'll interpret the silences. Things are crystal clear.", "metadataJson": "{\"start\":1255,\"end\":1258}"}, {"text": "So let me talk a little bit about the core structure after this sort of quick introduction to distributed systems.", "metadataJson": "{\"start\":1261,\"end\":1268}"}, {"text": "So the course structure is as follows. We have lectures like the one today and basically focuses on big ideas.", "metadataJson": "{\"start\":1274,\"end\":1281}"}, {"text": "The lectures are typically driven by a paper that we all sign, and these papers are often a case study of a particular big idea that we're covering in lecture.", "metadataJson": "{\"start\":1284,\"end\":1296}"}, {"text": "The papers are all published or posted on the schedule page. And for most papers, we ask you to answer a question as well as ask a question, and we'll try to cover those questions or answers during the lecture. And so it's important, part of the reason we do that is because we'd like you to read the paper in advance of the lecture so that we can go a little bit deeper into these papers. So I strongly encourage you to read them before class.", "metadataJson": "{\"start\":1300,\"end\":1331}"}, {"text": "So another component of the class is the labs, the programming labs. There are four of them. They're split in parts, but the four major ones. One is the Mapreduce lab that we just posted today and that's due next Friday, and where you build basically your own mapreduce library as sort of similar to the one that actually described in the paper. The second lab is a lab that focuses on replication in the presence of failures and partition networks.", "metadataJson": "{\"start\":1334,\"end\":1370}"}, {"text": "We're going to implement replication using a protocol that's called raft.", "metadataJson": "{\"start\":1370,\"end\":1377}"}, {"text": "And this is a lab that consists of multiple components, but at the end of it, you know, you'll have a library that you can use to what's called, you know, used to build replicated state machines, namely replicating a state machine on multiple machines, so that if one of them goes down, one of those machines goes down, then the service actually keeps running, and you're going to use that library to actually build a replicated service. And in fact, you're going to build a replicated key value service in lab three. So lab three is going to basically use multiple machines for fault tolerance or for applications to build one surface. Unfortunately, as we'll just see a lot more, is that just replication doesn't give you more performance because these machines actually have to perform the operating particular order. And so to actually get performance, what we're in lab four, you're going to build a sharded key value servers, and that basically consists of many instances of lab three running currently and basically taking care of a part or a shard of the key value surface, and so that you get parallelism and so that you can actually use this to actually drive throughput.", "metadataJson": "{\"start\":1381,\"end\":1470}"}, {"text": "And furthermore, we're going to actually move keys or key value pairs from one machine to another machine in response to load changes. So the labs basically labs two, three, and four built on top of each other. So if you have a bug in lab two, that might affect you. Actually, in lab four, we provide test cases for all of them. So all the test cases are public, and we grade you on those test cases.", "metadataJson": "{\"start\":1471,\"end\":1504}"}, {"text": "So you submit your solution. We run the same test on our computers and double check that you're passing the test. And if you pass all the tests, you get full score. Turns out these test cases are tricky and will try to tickle all kinds of corners in your systems. And so it turns out they are actually reasonable, hard to pass, and they're tricky to debug.", "metadataJson": "{\"start\":1504,\"end\":1533}"}, {"text": "They might actually have in a particular cornication error, and it may be very difficult to track down. When does it happen and why does it happen? So you know how to fix it. And so my advice for you is, like, start the labs early. It's often the case that, you know, if you just start the night or the two nights before, you're going to have difficulty passing all the tests because you know you're going to get stuck, you know, trying to debug one particular aspect and run out of time.", "metadataJson": "{\"start\":1534,\"end\":1558}"}, {"text": "To basically get the other test cases to work, there's an optional project.", "metadataJson": "{\"start\":1558,\"end\":1568}"}, {"text": "So instead of doing lab four, you can do a project. And the idea of the project is that you can work together or collaborate with a group of two or three students and do a project on your own. And the projects are form or similar type systems that we read about in the papers. You propose one that you would like to build. We'll give you some feedback and or tell you, well, maybe you should just do lab four, but if you're excited about doing project, we certainly like to stimulate that, and you should start thinking now and then.", "metadataJson": "{\"start\":1570,\"end\":1605}"}, {"text": "Hopefully we can have some discussion and settle on something that would be cool to do. Okay. Finally, one other component of the course is actually two exams, one roughly halfway the semester and one in finals week. And, you know, we expect your course. You have to do all the labs, submit a read, write homework questions for the papers, and do the two exams.", "metadataJson": "{\"start\":1605,\"end\":1631}"}, {"text": "If you look at the web pages for 6828 or 6824, you'll see exactly the balance in terms of grading for the different components. You know, the labs count for most. The two exams, I think, are 20 or 30% and then some class participation. But the details are on the web page to get you through the semester and help you along. We have excellent course staff.", "metadataJson": "{\"start\":1632,\"end\":1659}"}, {"text": "We have four tas running office hours. And to help you basically get labs. Let me do a quick round. Maybe the tas can introduce themselves so they can at least know who they are. Maybe.", "metadataJson": "{\"start\":1659,\"end\":1674}"}, {"text": "Lily, you want to go first? Sure. So I'm Lily. I am a third year grad student in pdas, and Franz is actually my advisor, so I know just how good he is at teaching. So you're in for a treat.", "metadataJson": "{\"start\":1674,\"end\":1687}"}, {"text": "Yeah. I'm looking forward to working with you this semester. I'll pass it off to David. Hi, everyone, I'm David. I am a second semester mn student.", "metadataJson": "{\"start\":1688,\"end\":1698}"}, {"text": "I took 6824 last spring when it was like half in person, half remote. So hopefully we can get the best of both worlds for the semester. I'm excited. Yeah. By Jose.", "metadataJson": "{\"start\":1698,\"end\":1708}"}, {"text": "Hi, I'm Jose. I'm a fourth year grad student working on machine learning problems, and I took this class my first year as a grad student, and I really, really enjoy it. So, yeah. Looking forward to teaching it.", "metadataJson": "{\"start\":1709,\"end\":1722}"}, {"text": "Yeah, I'm Cel. I use Aidan pronouns. I'm a first year master's student in pdos, like some of the others, and I took this class a few years back, had a great time taking it, so I'm excited to help everyone learn it.", "metadataJson": "{\"start\":1724,\"end\":1738}"}, {"text": "Okay, thank you. So there's a question in the chat. How is the system? How does the distributed system with the lab run? Is the machine systems simulated?", "metadataJson": "{\"start\":1741,\"end\":1751}"}, {"text": "Yes, we're basically simulating many, many machines by running many, many different processes. In fact, the labs have their own RPC library that, like, pretend, you know, you're running on separated physical machines, but in fact, you're running many, many processes on the same machine.", "metadataJson": "{\"start\":1751,\"end\":1769}"}, {"text": "Okay, any questions so far before I continue into the direction of actually some technical content?", "metadataJson": "{\"start\":1773,\"end\":1779}"}, {"text": "Is the real result of lab four? Is it similar to any existing, like, programs that exist? Yeah. In fact, what we'll be building has a lot of similarity to sort of popular key value services. You know, think redis or, you know, some of the other ones.", "metadataJson": "{\"start\":1782,\"end\":1800}"}, {"text": "You know, there will be differences, as we'll discover when we grow through this semester. But the key value server is a pretty well known and common service inside of a data center and run by many companies. And there are a couple of very popular ones that used by lots of people, and they basically struggle with exactly the same issues as you are going to be struggling with in the labs. We're going to build one that actually has pretty strong semantics, sometimes a little bit stronger semantics than some people actually do in practice, and we'll discuss why that happens, too. But yeah, it's very close to people doing practice.", "metadataJson": "{\"start\":1801,\"end\":1835}"}, {"text": "Raft is widely used in practice. For example, any other questions?", "metadataJson": "{\"start\":1835,\"end\":1843}"}, {"text": "Yeah, it's a good question about the labs. Again, if we have a bug on lab two that maybe doesn't even get caught by the testers somehow, do we get a like answer for the following labs, or do we just continue to use our code? No, you're going to continue using your code. We did our best to make the lab, the test is as good as possible, but I'm sure their case is done. We, you know, it's hard to do a complete good job, and, you know, every time we discover something that we missed, we basically improve the tests.", "metadataJson": "{\"start\":1849,\"end\":1887}"}, {"text": "So you'll be building, you know, once you pass the test, we're optimistic that you actually have an implementation that actually can support the other use cases that we're doing the rest of the semester.", "metadataJson": "{\"start\":1887,\"end\":1896}"}, {"text": "It's not uncommon for people to rewrite, rewrite their implementation once or twice, as you will see in lab two and lab three. You know, the structure. You have to can spend quite a bit of time thinking about the structure of your application or your library. And as you sort of learn, you may want to go back and redo it to help you along a little bit. This year, we're doing something different than we've done in the past years.", "metadataJson": "{\"start\":1899,\"end\":1923}"}, {"text": "I'm going to do run a couple of Q and a lectures where I'll share. We'll share our solutions with you, or we'll walk through our solutions and hopefully that will, you know, tell you a little bit about, you know, you can learn from that and see how that contrasts with your own solution and maybe, you know, pick up some ideas for future labs.", "metadataJson": "{\"start\":1923,\"end\":1941}"}, {"text": "Any other questions?", "metadataJson": "{\"start\":1947,\"end\":1948}"}, {"text": "Okay, again, interrupt me at any time. I'd like to make this more and more interactive. We'll take probably a couple lectures, but hopefully we'll get there okay. I want to talk a little bit, you know, sort of set ourselves up for the case study from today, but before doing that, I want to talk a little bit about a bit of a perspective for the class. Our focus in the class is going to be on infrastructure.", "metadataJson": "{\"start\":1954,\"end\":1981}"}, {"text": "You can more or less can tell that from the labs that, you know, we just discussed. So, you know, there's going to be somebody who's writing applications on these distributed systems, and we're not really concerned too much with the applications at all. We're going to be mostly concerned with the infrastructure that supports these applications, and the infrastructure sort of falls out in three different categories or, very broadly speaking, storage infrastructure. So, like devalue services, file systems, that kind of thing, computation, there are some frameworks to actually orchestrate or build a distributed application. And an example, the classic example is Mapreduce, that we'll talk about in a second.", "metadataJson": "{\"start\":1981,\"end\":2026}"}, {"text": "And then I guess the third category is communication and we'll spend less time on communication. It's almost more topic of 6829 network systems. But it will show up in the sense there's going to be some contract between the network system and the distributed system. And that will be a serious topic. For example, first day we're going to be talking about remote procedure call or pc, and that's the building block in which all labs are built.", "metadataJson": "{\"start\":2026,\"end\":2061}"}, {"text": "And that's our communication model. And the questions there are, you know, what kind of semantics does actually the RPC system provide? Is it at most once, exactly once, at least once. And we'll talk about that in first days lecture. But that's where we're sort of communication and distributed systems intersect.", "metadataJson": "{\"start\":2061,\"end\":2080}"}, {"text": "So if you look at these three, so basically storage to store data, durably computation to run computations, and communication to actually have these different pieces communicate with each other. So those are the three basic things that sort of from which we built distributed systems. And what we've been looking for are sort of abstractions that have been proven to be very helpful in building distributed systems. There's an abstraction like our, like remote procedure call or like a Mapreduce library or in a storage system like a key value service. And often, you know, often our goal will be to make the abstractions, distributed abstraction look very much like the sort of normal standard sequential abstractions that you may familiar with.", "metadataJson": "{\"start\":2081,\"end\":2126}"}, {"text": "So for example, when we build a storage system, we want our basically distributed storage system more or less behave like a single machine sequential storage server, like your regular file system on your laptop. Except we hope that the storage system is more fault tolerant because they use replication, maybe much more high performance because we use many, many machines. But like, the behavior of the system that we're looking for is sort of similar. The abstraction we're looking for is similar to a single one. Turns out in practice this actually is very hard to achieve.", "metadataJson": "{\"start\":2126,\"end\":2158}"}, {"text": "And we'll see that it looks like it, but it's not exactly. And this is a topic that will show up multiple times. In fact, that brings me to sort of like the main recurring themes in this class that we're going to see over and over.", "metadataJson": "{\"start\":2158,\"end\":2176}"}, {"text": "And the main topics are fault tolerance, not surprising. And that has sort of two aspects to define a little bit what fault tolerance means. One is availability. So we're going to be looking at techniques we're going to be looking at techniques to make systems highly available. What we mean that is that they continue to deliver their service despite their being failures.", "metadataJson": "{\"start\":2183,\"end\":2217}"}, {"text": "And so this is often expressed as like a number of knives, you know, 0.999 reliability. And so that's going to be one aspect of fault tones. The second aspect of fault tones that we care a lot about is what I'm going to call recoverability.", "metadataJson": "{\"start\":2217,\"end\":2231}"}, {"text": "When a machine crashes or fails, we like to bring it back into the system once it reboots so that we can keep up the availability. Because we didn't repair the system then basically all the machines would die one by one until we have zero machines and then we have no service anymore. So it's important that we repair the distributed system. The way we repair the distributed system is basically when the machine comes back up, it needs to recover its state and then, you know, start participating back into the distributed systems. And it turns out that is actually hard, that's a hard aspect.", "metadataJson": "{\"start\":2238,\"end\":2272}"}, {"text": "And key techniques, you know, for availability is going to be your application.", "metadataJson": "{\"start\":2272,\"end\":2276}"}, {"text": "And the key technique that we're going to use for recoverability is basically something called logging or transactions, writing things through durable storage so that when the power goes out, but the machine comes back up afterwards, the data is still there on disk.", "metadataJson": "{\"start\":2279,\"end\":2301}"}, {"text": "So that's the fault tolerance site. The second part is something what I'm going to call consistency.", "metadataJson": "{\"start\":2306,\"end\":2313}"}, {"text": "This is basically the contract that the servers is going to provide for operations with respect to concurrency and failure. So loosely speaking, when we think about consistency, basically the ideal is the same behavior as that a single machine would deliver. So we have a replicated, fault tolerant, high performance file system. Considering many machines, we like the behavior to be almost identical to the sequential machine. And so the key question always here is sort of on the forum, let's say we have a key value server.", "metadataJson": "{\"start\":2319,\"end\":2357}"}, {"text": "Does the get operation return the value of the last put?", "metadataJson": "{\"start\":2357,\"end\":2366}"}, {"text": "And if you run a single machine, you have nothing, no concurrent operation. So you run every operation one by one, like you do put, put, put, then again, then again. Then of course, like, you know, this question is trivial to answer. You would assume that the cat will return the value stored by the last put. But once we have concurrency and we have failures and we have many machines, this is actually not so obvious.", "metadataJson": "{\"start\":2374,\"end\":2397}"}, {"text": "You know, what the right, what the right way, what a good contract is. And we'll see actually many different contracts. We see ones that have strong consistency. You know, they almost behave like a sequential machine or ones that have a very loose guarantees and provide very different semantics for example, they provide eventual consistency. Eventually you will see a get, will return the result of a boot, but not immediately.", "metadataJson": "{\"start\":2397,\"end\":2428}"}, {"text": "And the reason there are sort of different types of consistency that's directly related with performance.", "metadataJson": "{\"start\":2429,\"end\":2434}"}, {"text": "Often one of the goals of the system is to deliver high performance scale, for example, with the number of machines, and to achieve that performance that's almost in conflict with consistency and fault tolerance. To actually achieve strong consistency requires communication between the different machines, which might actually reduce performance. Similarly, to achieve fault tolerance, we need to replicate data. That means we have to communicate data from one machine to another machine. And if we were to have to write that data also to durable storage, that operation is expensive, and so the replication can cost the performance.", "metadataJson": "{\"start\":2437,\"end\":2477}"}, {"text": "And so achieving these three things at the same time, it turns out to be extremely difficult. And in fact, what people do in practice is they make different trade offs. They will sacrifice some consistency to get better performance, or maybe some fault tolerance to get better performance. And so we'll see throughout the semester a wide spectrum of different types of designs that make that trade off differently. Just a small note of performance.", "metadataJson": "{\"start\":2478,\"end\":2505}"}, {"text": "There's two aspects to it. Like one is throughput, so you buy more machines, hopefully the throughput scales with the number of machines. But there's another sort of, part of aspect of performance is basically much harder to achieve, which is like low latency.", "metadataJson": "{\"start\":2505,\"end\":2522}"}, {"text": "And this is particularly important, like in these websites where you have thousands and thousands of machines. And maybe one user request when you click on a URL, actually causes a lot of these machines to participate. And if one of those machines is very slow, maybe it has some mechanical issues, or maybe the disk is not working 100%, or some other aspect where it doesn't really work well, that one slow machine can cause the whole user experience to be slow. And this is often referred to as tail latency. And there's a concern that will show up over and over throughout the semester as we discuss in different machines and even shows up in today's paper, the Mapreduce paper.", "metadataJson": "{\"start\":2525,\"end\":2569}"}, {"text": "So one other final topic that will show up a lot, at least in the class, particularly in the lab, is implementation aspects. And here's really how to manage concurrency, how to do procedure call implementation. And just building distributed systems by themselves can have actually serious implementation challenges that will come up over and over and over throughout the semester. Partly is because we want to achieve performance consistency and fault tolerance in depression crashes, crashes in concurrency, which drives complexity. So those are the main topics.", "metadataJson": "{\"start\":2570,\"end\":2612}"}, {"text": "Any questions about this part?", "metadataJson": "{\"start\":2613,\"end\":2617}"}, {"text": "Okay, then let's sort of dive in and look at the first case study through the Mapreduce paper.", "metadataJson": "{\"start\":2624,\"end\":2633}"}, {"text": "And this is an illustration of many of the topics in 6824. You know, talking, we're going to be talking about fault tolerance, we're going to talk about performance, you know, tail latency, all kinds of issues that actually you see throughout the semester, and we'll see one cut or one system that deals with that. So good illustration of many of the topics.", "metadataJson": "{\"start\":2642,\"end\":2663}"}, {"text": "The paper is also very influential, although Google internally doesn't use Mapreduce as described in this paper. Exactly. They have systems directly derived from this Mapreduce system that they are still using day to day. There are other libraries that look a lot like Mapreduce, that they are widely used. It also inspired different types of computation models than Mapreduce itself.", "metadataJson": "{\"start\":2668,\"end\":2699}"}, {"text": "And we'll see one or two more later in the semester. So hugely influential paper. And then finally, it's actually the topic of lab one, which is another good reason to talk about it. Now, many probably of you have seen the Mapreduce paper. It shows up in 633 if you're an undergrad here at MIT.", "metadataJson": "{\"start\":2699,\"end\":2719}"}, {"text": "Otherwise you might have seen it in other places. But we're going to go a little bit deeper than, for example, 603 because you actually have to implement your own Mapreduce library. And as always, when you implement something, problems that you know, you might not have really fought hard about before, you know, certainly start popping up. And so by the end of it, you really understand that. Reduce any questions.", "metadataJson": "{\"start\":2720,\"end\":2751}"}, {"text": "Let me give you a little bit of context for this paper. So this paper is written by two engineers from Google, very well known, and the context is these early data centers. So Google, as a search engine, needed to build a reverse index of the World Wide Web to basically allow users to query the Internet. And these kind of computations take multi hours to run, and they process terabytes of data, our computations on terabyte of data, bytes of data.", "metadataJson": "{\"start\":2764,\"end\":2818}"}, {"text": "And so think web indexing, web crawling, all of, particularly web indexing. So it's one of the driving application. And, you know, as Google, you know, built these sort of applications internally, like Sanjay and Jeff Dean, you know, the two offers, you know, they were very good at that kind of stuff, but they discovered that basically there were many other Google engineers that wanted to write those kind of certain types of applications, too. They wanted to be able to write their own data analysis over all the web pages that had been crawled.", "metadataJson": "{\"start\":2821,\"end\":2854}"}, {"text": "And they realized that writing these kinds of applications was difficult because if you're running multi hour computation in many, many machines, it is very likely that one of those machines will crash during that computation. And therefore you have to build in some plan for fault tolerance. And once you start doing that, then basically it requires that you basically have taken something like 6824 and able to build these kinds of complicated systems. And their goal was to basically get out of that sort of dilemma and make it basically easy for non experts to write distributed applications. And so that's the motivation for this, for this paper and why they're very excited about it.", "metadataJson": "{\"start\":2856,\"end\":2909}"}, {"text": "And so the approach they take, that map produce takes is it is not a general purpose library. You know, you can't like write, take any application and use Mapreduce to actually make it basically fault tolerance. And so it has to be written in a particular style, namely using these map functions and reduce functions. And those functions are basically functional or stateless, but those are, you know, the programmer writes these sequential code, and then hence, you know, these two functions. You know, the map and the reduce function to sort of the framework.", "metadataJson": "{\"start\":2909,\"end\":2950}"}, {"text": "And then the framework, the Mapreduce framework deals with all the distributedness.", "metadataJson": "{\"start\":2950,\"end\":2955}"}, {"text": "So it will arrange that, you know, the application or the binary for the programs that run on many machines or install the many machines, runs on many machines. It deals with load balancing. It deals with certain machines that are slow. It will deal with the machines that crash. So the application writer itself, who wrote the Mapreduce function, don't really have to be concerned about this at all.", "metadataJson": "{\"start\":2964,\"end\":2985}"}, {"text": "And they basically get all that stuff, if you will, transparently and again, to make that happen. The library is actually not in general purpose. For example, if you wanted to write a key value service, you couldn't use the Mapreduce library because it assumes a particular computational model, and your application has to fit in that. In the computation model it fits is something that they saw a lot at Google, which is people wanted to do big data analysis on basically all the web pages in the world. There are many types of computations that just have to process lots and lots of data and compute values based on that data.", "metadataJson": "{\"start\":2985,\"end\":3020}"}, {"text": "That's the type of applications that Mapreduce targets. Any questions about the context and the motivation for this paper?", "metadataJson": "{\"start\":3021,\"end\":3031}"}, {"text": "Okay, let me proceed. So let me first draw sort of an abstract view of what's going on, and then we'll dive into more detail. So the view that you sort of need to have in the background to understand actually how Mapreduce works, which is going to be very important for you when you're doing lab one, is there's a bunch of input files, whatever f one f two, f three, let's say. Of course there are going to be many, many more in Google's case, but there's more pedagogical reasons we're going to, and the size of my display, I'm going to just have three files. And basically for every file is processed by this map by map function.", "metadataJson": "{\"start\":3041,\"end\":3096}"}, {"text": "So one written by the programmer and produces some output, some intermediate output. So for example, the classic example to discuss Mapreduce is word count. Basically counting how many times a word occurs in the data set, where the data sets consists of many, many, many files. So for example, like, you know, we're running the word count function on file one and it will produce for every word and a key value pair. And the key value pair consists of the key, which is the word and account one.", "metadataJson": "{\"start\":3096,\"end\":3130}"}, {"text": "And if a appeared multiple times in this file f one, then you know, it'll be multiple and record to multiple key value pairs, a one. And so maybe this file contains none of many words. Maybe it has a one and b one. So the file contains two words. Similarly, the map function does the same thing for the file f two and will produce some key values.", "metadataJson": "{\"start\":3131,\"end\":3156}"}, {"text": "Let's say maybe there's only the word v appears in the file once and maybe f three. The map function also runs in the file f three. And let's assume, let's just assume that a shows up once and the word c shows up once. Basically these map functions all run in parallel, completely independent of each other. There's no communication between them on their input files.", "metadataJson": "{\"start\":3156,\"end\":3186}"}, {"text": "This is going to give us hopefully high throughput or anxious to scale to much, much much bigger datasets. And they're produced on these intermediate values, these key value pairs, like a one, b one, b one alone, or a one and c two. And then sort of the second step, this is often referred to as the shovel, is that basically you're going to run the reduce functions on basically each row. So here we got the row of all the a's and we're going to run a reduce function. And in the reduce function basically takes the one key aggregates all the reduced function gets its input, the key plus the aggregated values, or not the aggregated value, but the combined values from the different outputs of maps.", "metadataJson": "{\"start\":3186,\"end\":3235}"}, {"text": "So in this case, the reduced function would get two intermediate results, both a with key a and two values, one and one. And in this case, in the case of a mort count, you know, we just add them up and so, you know, it would produce the value, you know, key value pair a two and we're doing, basically we're doing, and basically what we do is we're doing, we do run the reduce for every, you know, row. And so this will produce whatever v two. And then similarly, you know, in the end, you know, c one for the last one. And again, once we've done circus shovel, these reduced functions can totally run independently of each other.", "metadataJson": "{\"start\":3235,\"end\":3276}"}, {"text": "They can just process whatever row data they had and be done with it. The only really expensive piece in this is this shovel in the middle, where the reduce functions need to obtain, you know, their inputs from basically every mapper. So when all the mappers are done, you know, the reduce function basically gets, you know, needs to contact every mapper, extract, you know, the output for the output for the mapper, for that particular reduce function, and, you know, sort by key. And then, you know, basically run the reduce function. And so basically we're sort of assuming, as the paper sort of points out, the expensive operation is really that shuffling of data between the mappers and the reducers.", "metadataJson": "{\"start\":3276,\"end\":3327}"}, {"text": "Any questions about this abstract picture?", "metadataJson": "{\"start\":3330,\"end\":3333}"}, {"text": "Sorry, I had a question. So is there, I know that not all problems can be expressed in Mapreduce stage, but is for example like sorting an array, is it possible to do? Yeah, so sorting is one of the applications that they tout a lot actually in the paper. This would be something that's totally done with Mapreduce. So basically you split the input files, correct in many things.", "metadataJson": "{\"start\":3339,\"end\":3368}"}, {"text": "The mappers sort their piece and then they split the output, say like r buckets. And then each reduce function is going to basically sorts that particular r bucket and that gives a total sorted file.", "metadataJson": "{\"start\":3368,\"end\":3383}"}, {"text": "I see. Okay. And in this case, you know, in sort is interesting because basically the input, the intermediate values and the output are the same size, like in some other functions. Like maybe the map function will reduce the intermediate state to something much smaller than the input size. In the case of sort, that is not the case.", "metadataJson": "{\"start\":3385,\"end\":3407}"}, {"text": "Okay, let's look at the paper actually, and get a little bit of sense actually how you write them.", "metadataJson": "{\"start\":3409,\"end\":3414}"}, {"text": "See if I can actually, that is annoying.", "metadataJson": "{\"start\":3419,\"end\":3424}"}, {"text": "Lost my menu. Let's hold on 1 second.", "metadataJson": "{\"start\":3426,\"end\":3430}"}, {"text": "Okay, that's not so cool. Give me a second to. Ah, here we go.", "metadataJson": "{\"start\":3442,\"end\":3447}"}, {"text": "Good. Here we go. Let's do.", "metadataJson": "{\"start\":3450,\"end\":3453}"}, {"text": "Okay, can everybody see this?", "metadataJson": "{\"start\":3455,\"end\":3457}"}, {"text": "Okay, there are a couple questions.", "metadataJson": "{\"start\":3461,\"end\":3463}"}, {"text": "Let me postpone some of these questions because I will see them in, we'll discuss them in a second in more detail. If I don't answer your question, please ask it again. So the first thing I want to do is actually look at one of the examples in the paper of a map and a reduce function corresponding to the word count example that we just sort of abstractly discussed. So here's the code for the map function. You see that the map function takes a key value.", "metadataJson": "{\"start\":3465,\"end\":3494}"}, {"text": "The key is really not that important here. It's the document name. So f one or f two, and string. The value is basically the content of the file. So all the words that actually appear in the file, f one.", "metadataJson": "{\"start\":3494,\"end\":3507}"}, {"text": "And then basically it goes through the piece of code, goes through the warts in the file, and as an intermediate value, it emits these a one, b one, c one, et cetera. Like from the programmer point of view. Correct. You don't really see these intermediate key value pairs at all. You just write this one simple map function.", "metadataJson": "{\"start\":3507,\"end\":3527}"}, {"text": "And then the reduce function is also more or less as expected. It takes two arguments, the key, like a and values, in this case, reward count. That would be one one one, sort of the number of times that the word a actually showed up in the intermediate output. And basically what the function does, it just goes over the, iterates over the list of values and basically adds one plus one plus one plus one, and then emits the final result. So that's basically, you can see from this code, the programmer basically almost writes complete, straightforward sequential code.", "metadataJson": "{\"start\":3528,\"end\":3567}"}, {"text": "This application is very simple, admittedly, but the code for an even more complex application would also be straight. You know, be sequential, might be more code, but it would be straightforward sequential code. And in this code, the program doesn't really worry about the fact that all the machines might crash. You know, there might be load imbalance. That's just basically all taken care of, the Mapreduce library.", "metadataJson": "{\"start\":3567,\"end\":3587}"}, {"text": "So this is. And so, you know, the hope, and I think this hasn't proven out to be true, is that this actually enabled lots and lots of people to write distributed applications and process gigantic data sets that could no way fit on a single machine. Like for example, the whole world Wide Web.", "metadataJson": "{\"start\":3588,\"end\":3605}"}, {"text": "Does that make sense in terms of what the programmer actually sees?", "metadataJson": "{\"start\":3607,\"end\":3612}"}, {"text": "Okay, I want to talk a little bit about the implementation.", "metadataJson": "{\"start\":3615,\"end\":3618}"}, {"text": "So I'm using the diagram here from the paper.", "metadataJson": "{\"start\":3622,\"end\":3625}"}, {"text": "So we got the user program. So the user program is like the map and the reduce function that we just saw. You submit the map and the reduce function to the, you link it with the Mapreduce library and that forms a binary. And then you give this to the Google drop scheduler, and it will basically find a whole bunch of machines and run what they call workers there. So like, you know, the scheduler will, for example in the evaluation, as we'll see in a second, you know, there are about like 1800 machines on these 1800 machines, you know, the scheduler will run a worker process that actually does the actual work and invokes, you know, map and reduce functions when appropriate.", "metadataJson": "{\"start\":3628,\"end\":3675}"}, {"text": "There's one other process that is important. In the paper they call it the master process. In the lab we call it the coordinator, and the coordinator orchestrates the workers and hands jobs or maps jobs to them. So like the terminology here, is that a complete application is one job, a mapreduce job, and then a reduce, reduce or anification of map is what is called a task. And so basically the coordinator will assign files to particular workers and the worker will then invoke the map function on that particular file and that will produce some intermediate results.", "metadataJson": "{\"start\":3676,\"end\":3723}"}, {"text": "And here are the intermediate results, and those intermediate results are stored on the local disk of the machine that actually runs that particular map function. And when, you know, worker has run or completed a particular map function, basically tells the master, I'm done with that map function, and you know, and tells them the master where the intermediate results are. Then at some point when all the sort of maps are basically done, you know, the coordinator will start running reduce functions and the reduce functions will collect the intermediate results from the different mappers from the locations that are specified in the result record, retrieve that data sorted by key, and then basically invoke the reduce function on every key and the list of values. And that produces an output file. And that is the, there's going to be one output file per reduce function.", "metadataJson": "{\"start\":3723,\"end\":3784}"}, {"text": "And you know, you can aggregate, you know, these output files or concatenate the output files to get the final output. And that's sort of the structure. The input files live in a global file system that's called GFS, although Google uses a different global file system now. But you know, the paper uses GFS and we'll actually read about GFS next week. And the output files also go into GFS.", "metadataJson": "{\"start\":3784,\"end\":3807}"}, {"text": "The intermediate files don't are not stored in GFS, they're stored on the local machines where the workers run.", "metadataJson": "{\"start\":3808,\"end\":3816}"}, {"text": "Any questions about the sort of rough sketch of the implementation?", "metadataJson": "{\"start\":3818,\"end\":3822}"}, {"text": "I have a question about the process file for the remote read. So in the remote read process is the file actually transferred to the reducer? Yes, so the, exactly. So the intermediate results are produced or stored on the disk of a machine that run the mapper or that map function. And then the reduce goes out and basically fetches its set of keys from every mapper.", "metadataJson": "{\"start\":3825,\"end\":3854}"}, {"text": "And so at that point the data is transferred across the network. So the network communication that happens is here.", "metadataJson": "{\"start\":3854,\"end\":3860}"}, {"text": "The reason that there's little network communication, no network communication here at all, is because the workers, the way the coordinator assigns files to workers is that basically the worker is run on the same machine. So every machine runs both a worker process and a DF's process, and the workers are basically assigned to or the map functions run on the machine that actually has that file locally stored in gfs. Basically this actually corresponds to basically local reach through gfs to a local disk. And then the files are produced or mapped or produced into the intermediate files are stored on local disk too. So there's no communication happening in this part of the picture.", "metadataJson": "{\"start\":3864,\"end\":3911}"}, {"text": "And then when the reduce functions run, they actually retrieve the files across the network and then write it out in gfs. And there's going to maybe some network communication here when the workers actually produce the files in the global file system.", "metadataJson": "{\"start\":3912,\"end\":3926}"}, {"text": "I have another question.", "metadataJson": "{\"start\":3929,\"end\":3930}"}, {"text": "Is the coordinator responsible for partitioning the data and putting it on each worker or no machine? No, no, really. Basically the Mapreduce, when you run the user program you're basically saying like, you know, I want to run it on f one, f two, f three, f four, whatever. All the input files and those input files live in gfs. So the part of the job specification, you should say like which input files need to be processed.", "metadataJson": "{\"start\":3932,\"end\":3966}"}, {"text": "Sorry, how does the sorting work? Does like who does the sorting and how the Mapreduce library does a little bit of sorting before it hands it off to the Mapreduce to the reduce function. So for example, the intermediate results might have like, you know, basically maybe all the intermediate cells for keys a, b and c go to one word and you know, there's just a whole bunch of key value pairs, like a one, b one whatever, a one again, you know, c one whatever. And basically what the Mapreduce library does, it sorts it first by key. So first all the a's together and then all the b's together and then all the c's together and then basically concatenates all the values for one single key and hands that off to the reduce function.", "metadataJson": "{\"start\":3973,\"end\":4026}"}, {"text": "Thank you.", "metadataJson": "{\"start\":4028,\"end\":4029}"}, {"text": "Okay, so I want to talk a little bit about fault almost now and sort of go back to.", "metadataJson": "{\"start\":4037,\"end\":4043}"}, {"text": "Could I ask a question about the Mapreduce paper real quick before we move on? So is the larger idea that a lot of functional programming could be reduced to the Mapreduce problem? Yes. Okay.", "metadataJson": "{\"start\":4052,\"end\":4067}"}, {"text": "Yeah. Okay, sorry. Yeah. In fact, the name hints at that, right? Because basically there are two.", "metadataJson": "{\"start\":4069,\"end\":4073}"}, {"text": "The notion of a map and reduce function is something very common in functional programming languages and used widely in functional programming languages or any sort of functional programming style. So they basically, that's where the inspiration came from.", "metadataJson": "{\"start\":4073,\"end\":4087}"}, {"text": "Okay, so actually there's a good segment to fault tolerance, because the idea is that if a worker fails, then the coordinators are in charge of noticing that the worker fails and basically restarts that task. And so the coordinator reruns map and reduce functions.", "metadataJson": "{\"start\":4090,\"end\":4119}"}, {"text": "Of course, the coordinated itself doesn't rerun them, but basically the coordinated sites that a particular map function needs to be run again, because it appears to the coordinator that the machine that it handed the task to actually is not responding. And so the typical thing is like if a machine doesn't respond in some certain amount of time, the coordinator is going to assume that the machine crashed.", "metadataJson": "{\"start\":4122,\"end\":4143}"}, {"text": "And so that means that when another worker becomes free and is looking for a new task, and it will hand out the same task that it actually handed out earlier, and we'll hand it out again. So that's sort of the basic plan for fault tolerance, is that if the coordinator doesn't hear about a particular worker reporting back that the task is done, it will rerun the task again. So an interesting question is, can a map function, can a map run twice, even complete twice?", "metadataJson": "{\"start\":4146,\"end\":4182}"}, {"text": "Is it possible in this framework that, you know, a particular mapper will run twice? I guess it is, because if the machine is down, you can really tell at which point. So how many of the map tasks that it executed during the specific map reduce instance were actually completed. So you would just have to rerun all of them, I guess. Yeah.", "metadataJson": "{\"start\":4190,\"end\":4218}"}, {"text": "So mostly we just think about this one task at a time, but so the machine, like, does one task, then goes back to the coordinator, asks for the next task, and that might be another map task. And so when the coordinator doesn't hear back, it will say, like, okay, we'll ask another worker to run that map test too. But it could be the case that as you point exactly out, that the first worker, the first machine, didn't actually crash, it just happened to be a network petition. Or like the word coordinator was not able to communicate with the machine, but it actually is just running happily and actually doing the map task, and it could produce an intermediate set of results. So the same map function can actually exactly run twice.", "metadataJson": "{\"start\":4218,\"end\":4257}"}, {"text": "And so it's actually, one of the reasons that mapping dues are functional is because that's okay if it's a functional program. If you run the same program on the same input, if you run a functional program on the same input, it will produce exactly the same output. So it doesn't really matter that it runs twice, in both cases will produce exactly the same output. This is where this functional aspect is actually really important. It basically has to be functional or deterministic, because every run of this map function must produce the same output because we're going to use one of them into the total, in the total computation.", "metadataJson": "{\"start\":4258,\"end\":4301}"}, {"text": "So similar. Can a reduced function run twice?", "metadataJson": "{\"start\":4302,\"end\":4304}"}, {"text": "Yes, I believe so, yeah, exactly for the same reason. I mean, if your machine runs a reduced function, there's no different than a map task. There's really no, from the fault tolerance perspective, there's no really big difference between a map task and a reduced task. If the machine running the reduced task doesn't report back but happens to also finish the job, another machine might be running exactly the same reduced function and they will produce output. Now the only sort of interesting aspect in this is that both reduce functions will write the final output file into gfs.", "metadataJson": "{\"start\":4320,\"end\":4354}"}, {"text": "And if you paid attention to it, you will notice that what they do is actually they first produce the file in an intermediate file in the global file system, and then do an atomic rename to name, move the file or rename the file into its actually final name. And because again, it's atomic, one of the two reduce functions will win. But it doesn't really matter which one wins because they're going to produce exactly the same outcome because they're functional.", "metadataJson": "{\"start\":4355,\"end\":4385}"}, {"text": "So just to double check, if we have a machine that's doing a map task, so a single machine can do multiple math tasks. So let's say that it's doing like ten math tasks and it's in the 7th task, and then for some reason it fails, and then the master knows that this machine failed. So then the master will order for all of the seven math tasks that were completed to be re executed distributedly, maybe on different map machines. Yeah, except, you know, that's right. Although I think in general it just goes one map at a time.", "metadataJson": "{\"start\":4388,\"end\":4421}"}, {"text": "So basically one machine runs one map function or one reduce function, not multiple. Okay, awesome, thank you. But after a worker is done running the map task, does it immediately write its file somewhere that's visible to other machines, or does it just keep that file in its file system for the time being? It keeps a map function always produces the results on the local disk, and so it sits in this local file system. Right.", "metadataJson": "{\"start\":4421,\"end\":4448}"}, {"text": "So then even if you were doing map tasks one at a time, in the scenario where you did multiple, and then the machine crashed, you would lose the intermediate work, right? No, it sits in the file system. So when the machine comes back up, maybe the stuff is there. Oh, I see. So the data is actually in store durably.", "metadataJson": "{\"start\":4448,\"end\":4468}"}, {"text": "Oh, I see. Okay. And the mappers or the reduce function directly talk to the map functions, the machines that actually have the intermediate results. Okay, so let me talk quickly about a couple other failures.", "metadataJson": "{\"start\":4469,\"end\":4481}"}, {"text": "I know that all the questions you're asking are great questions. Correct. In fact, all will show up when you're actually implementing what Mapreducer. You will have to decide exactly how you're going to do things. So a couple other things.", "metadataJson": "{\"start\":4487,\"end\":4497}"}, {"text": "Can the coordinator fail?", "metadataJson": "{\"start\":4497,\"end\":4499}"}, {"text": "I don't think so. That's correct. Like you're the cat. Excellent. Yeah.", "metadataJson": "{\"start\":4509,\"end\":4517}"}, {"text": "The coordinator can not fail. So basically when the coordinate fails, the whole job has to be rerun. You know, in this particular implementation, they have no plan for failures of the coordinator. And that's making the fall according to more fault tolerance is actually a little bit more tricky because it actually has state state that gets modified every time a map function completes or reduce function completes. And so it actually turns out to be more complicated.", "metadataJson": "{\"start\":4518,\"end\":4544}"}, {"text": "And so basically in this particular library, the coordinator cannot fail. And we'll see later in semester techniques that we can use to make the coordinate of all tolerant if we wanted to. But they decided not to do so. One reason they decided not to do so is because like a single machine, they're hoping basically that the single machine that just runs the coordinator is unlikely to crash, while it's very likely that one of the thousands of machines that run some mapper will crash. Okay, how about slow workers?", "metadataJson": "{\"start\":4544,\"end\":4574}"}, {"text": "Sort of another type of failure. We discussed this issue of, like, where machines might be slow because like some other computation is running on it. Like GFS is also running on the same machine. Maybe it actually is using a lot of the cycles or bandwidth, or maybe there are like problems with the hardware itself. Is there anything special that they do?", "metadataJson": "{\"start\":4581,\"end\":4598}"}, {"text": "I think I recall reading something about when the job is getting somewhat close to finishing, the coordinator will assign the remaining tasks to additional machines, just in case there are like machines that are lagging and then they will take the results that finish first. Yeah, exactly. So these slow workers are called stragglers. And what they do is like, they sort of do backup tasks. So, for example, when they're close to the, indeed, as you say, when, you know, the computation is almost done, to say, like, there's a handful of reduced tasks left or a handful of map tasks left, the coordinate actually just basically runs a second instance or maybe third instance of that task on a separate machine, and that's totally okay to do so.", "metadataJson": "{\"start\":4599,\"end\":4644}"}, {"text": "Correct. Because it's functional. So it's no problem. We will run the same computation several times because it will produce exactly the same output, because it's given the same input. And the hope is that one of these other guys will finish quickly.", "metadataJson": "{\"start\":4644,\"end\":4659}"}, {"text": "And so therefore then the performance is not limited by the slowest worker, but basically the fastest of the ones that got replicated. And so this is like one of issues where like, you know, basically this is a common idea to deal with straggler or to deal with tail latency, is to try to basically replicate tasks and go for the first that finishes.", "metadataJson": "{\"start\":4660,\"end\":4684}"}, {"text": "Okay, I think this is time to wrap up so that you can go to other classes. But these are sort of the major issues that show up in the Mapreduce library and you will definitely be struggling. Mostly the hard part of actually implementing the Mapreduce library is actually doing default tolerance aspects. But you should keep in mind as you're doing that, all the programmers that are using your library or would use your library don't have to worry about all the distributedness that you have to deal with. So you're in the fortune situation.", "metadataJson": "{\"start\":4689,\"end\":4720}"}, {"text": "You're not the target of the Mapreduce paper, making your life of running Mapreduce applications easy. You're on the bad side of the equation here. You actually have to deal with the distributedness and become an expert. Okay, I'm going to hang around for a little while, so if people want to go, feel free to go. If you want to ask a couple more questions, feel free to do so and I'll see you first day.", "metadataJson": "{\"start\":4720,\"end\":4745}"}]}