{"documentId": "lecture3", "section": [{"text": "Okay, so the plan for today is talking about GFS. I'm going to do it in sort of multiple steps. One, I'm going to talk a little bit about storage in general and why it's so important, and why we'll spend a lot of time in this class talking about it. Then I'm going to talk a little bit about intro to GFS. And its main design will focus on consistency, which will be sort of the main theme through this lecture.", "metadataJson": "{\"start\":1,\"end\":30}"}, {"text": "And as part of the consistency, again, we probably do a, hopefully there's still time, do a breakout room and talk a little bit about in the breakout rooms about the lecture, the question that was posed for lecture, and then we'll resume discussion of consistency. Okay, so let me talk a little bit about storage systems in general and why they so feature so prominently in 6824. And the main reason is it is a fantastic building block for fault holiday systems. And so the basic idea is that if you can, if you can build a durable storage system, then you can structure your application as the app is basically stateless, and then the storage holds all the persistent state that simplifies the design of the apps tremendously, because the app basically doesn't really have any stable storage, it has to maintain itself. In fact, factored it all out to the storage system.", "metadataJson": "{\"start\":31,\"end\":112}"}, {"text": "You can start up a new application very quickly, it crashed, doesn't really matter because it only has soft state, not any hard state. And you start it up again, and then once you start up again, you just reach the state from the distributed storage system and you can see this. Look at any website. Basically it's structured in that way. There's a storage backend, that state, and then there's the application middle tier that does the application specific computation or whatever, runs JavaScript go or whatever, and front ends that out to clients on the Internet.", "metadataJson": "{\"start\":113,\"end\":147}"}, {"text": "Storage is just this fantastic building block. And I think this is one reason that we're going to see this over and over in this class. That means that the storage system itself, of course, has to be highly fault tolerant and as turns out to be a very tricky thing to do.", "metadataJson": "{\"start\":149,\"end\":164}"}, {"text": "The flip side of it, we'll make life for the application easy, but the designing an actual storage system is not easy, so why is it hard?", "metadataJson": "{\"start\":168,\"end\":176}"}, {"text": "And it basically comes down to one region that drives these designs, which is like we generally want high performance. When you think about the storage system for today, GFS, its main goal is to basically support Mapreduce types applications, and so it really needs high performance. Well, what does that mean? Well, it means that you typically have to shard data across servers. So you can't use one server, you have to use multiple servers.", "metadataJson": "{\"start\":180,\"end\":214}"}, {"text": "And the reason you want to read from the disk, and often particular machine has a limited throughput. If you want to read more than actually a single disk is sustained, you have to use multiple disks and you have to use multiple network cards. And so you can immediately get into this sort of large scale systems as in the DF's papers, where you have thousands of machines, but if you have many servers, some are going to fail and you're going to get failures. Or maybe just can say more explicitly, you have constant faults and you know, let's say that a computer crashes, you know, once a year, you know, now let's say you have 1000 machines like in the GFS paper, or actually many more than 1000 machines, but a minimal 1000 machines. How many failures are you going to see per day?", "metadataJson": "{\"start\":214,\"end\":264}"}, {"text": "Roughly around three. Yeah, around three. That means that a failure of a computer, unlike my laptop at the beginning of the lecture, is just a common scenario. And so if you're going to move up to more than 10 00, 10,000 machines, you know, 100,000 machines, and you run the application, you use that kind of number of computers, you're going to get faults. And so that means you want a fault hollow design and you know, the get fault hollows.", "metadataJson": "{\"start\":264,\"end\":303}"}, {"text": "You know, at least in the case we have storage is the typical approach. The way to go is, you know, go with replication, copy the data on multiple disks so that if one disk fails, the other disk hopefully have the data. But if you go into replication and so the data is in multiple places, that runs into the challenge that the data may be out of sync. And so you actually get into inconsistent potential inconsistencies.", "metadataJson": "{\"start\":303,\"end\":330}"}, {"text": "You know, to avoid these inconsistencies, you know, if you desire a strong consistency, basically your replicated system behaves as if it's has the same behavior as an unreplicated system. Then you will need some consistency protocol and that can require some maybe sending messages and may lower performance, maybe the messaging themselves is not really huge performance overhead. But as we'll see, you might actually have to read or write to durable storage as part of that protocol. Reading and writing to storage intentionally tends to be quite expensive. Here we see this conundrum.", "metadataJson": "{\"start\":339,\"end\":381}"}, {"text": "Like we want high performance, we want fault tolerance. Because we have many servers, we want high performance. So we want many servers, we have many servers. We need fault tolerance. That means replication, that means inconsistencies.", "metadataJson": "{\"start\":381,\"end\":394}"}, {"text": "Because you have data in multiple places. To fix the inconsistencies, we need sophisticated protocol that might lower performance here. This fundamental challenge in designing these distributed storage systems, that there's a struggle between consistency and basically performance. We'll see that throughout the term.", "metadataJson": "{\"start\":395,\"end\":413}"}, {"text": "Let me talk a little bit about consistency, and that's at a very high level, I'll promise you, for addressing the semester. We're going to go in more detail as we go. So first let's talk again about the ideal consistency. An ideal consistency. The way, the simplest way to think about it is that basically the machine behaves as if it's a single system.", "metadataJson": "{\"start\":415,\"end\":439}"}, {"text": "That's the desired behavior. And there's sort of two things that make that desired behavior, or two hazards that make that desired behavior hard to achieve, or at least requires some thinking. And one is concurrency. And second thing is failures. So let me start with concurrency, because even if you have a single machine with multiple clients, so you have concurrency within the single machine, you actually have to think about consistency.", "metadataJson": "{\"start\":448,\"end\":479}"}, {"text": "This course could be quite obvious. Let's say we have one machine, we have one disk. Multiple requests come in from different clients, and if the machine is a multi processor machine, they might actually run these requests in a true in parallel. So let's think a little bit about it. What does it mean?", "metadataJson": "{\"start\":479,\"end\":498}"}, {"text": "So let's say we have inclined one. It does a write operation to a key x and write one, and at the same time there's a request coming in that writes to x two, but actually writes the value two. Now, if you want to specify or state what consistency means, we need some rule about what will happen. And the rule is typically phrased from the perspective of the reader. So let's say there's another reader coming in, or another request coming in from another client, and it actually reads x.", "metadataJson": "{\"start\":499,\"end\":531}"}, {"text": "And the question is like what is the value that actually that reader or that this client observes like a little bit more complicated or more interesting? Let's say we have a fourth client, which we're going to bring out this issue of persistency definitions and more clearly, and it also does a read of x well after the client 3d actually read x. So now we have some sort of stake, like what is the desired outcomes and what are incorrect outcomes, and that's sort of really what defines consistency. So let's take the first case, c. Three, what would be reasonable outcome for the read of c?", "metadataJson": "{\"start\":533,\"end\":567}"}, {"text": "What is a reasonable outcome for the read of c three to return what values, what value would make you happy? What would make an application programmer happy? Two. Two would be very reasonable. Any other reasonable values?", "metadataJson": "{\"start\":568,\"end\":584}"}, {"text": "One. Yeah, one would be reasonable because the operations happen concurrently. So maybe we don't really know which one. We don't really want to restrict in what particular order they go. So we do say, like, either one is fine because they're concurrently.", "metadataJson": "{\"start\":584,\"end\":596}"}, {"text": "What are some values that we would like not to see for the c three of ethan? Seven. Yeah, seven. Any other value? Right, because nobody wrote that, so that would be undesirable.", "metadataJson": "{\"start\":597,\"end\":607}"}, {"text": "Okay, so good. So like we agree that probably the reasonable outcome for c three would be either one or two. Okay, how about c four? The same as c three. Really?", "metadataJson": "{\"start\":608,\"end\":620}"}, {"text": "Exactly the same. So let's say c three returned one. What do we expect c four to return? Whatever c three saw. Yeah, because it ran well after c three.", "metadataJson": "{\"start\":620,\"end\":632}"}, {"text": "So if one was returned, we expect one here too. If two was returned, we expect two here. Does that make sense?", "metadataJson": "{\"start\":632,\"end\":642}"}, {"text": "Okay, so there's like a super brief sort of introduction of saying like how can we define consistency? And then a typical way we do it is using traces. And we argue about like what are correctness, you know, for particular traces, and we'll see more of that. And of course, the server can enforce this kind of consistency by, for example, using walks as you have done in, as you do in the mapreduce or in any concurrent goal program that you write. That's the standard technique to enforce consistency in terms of.", "metadataJson": "{\"start\":645,\"end\":674}"}, {"text": "In the depressions of concurrency, is to use locks. Now in a distribut system, the ideal consistency, as I said, there are two hazards and the second hazard is basically failure. So just replication in general. We have two servers now. Assembly here's s one, here's s two.", "metadataJson": "{\"start\":674,\"end\":695}"}, {"text": "And, you know, both have a disk and you know, we have our same clients as before, you know, c one and c two. And they write, you know, two x and you know, let's say, you know, just to illustrate like what kind of complications, what kind of, you know, just to illustrate that we actually have to do something. Let's start with like the most, you know, dumb replication plan. So like very bad replication plan.", "metadataJson": "{\"start\":696,\"end\":723}"}, {"text": "So in this particular replication kit and plan, what we're going to do is like, we're going to allow a client, you know, when a client actually wants to update or write, we going to tell it to. Basically the protocol that we're going to follow is the client writes it to both servers, you know, whatever. Don't really coordinate that, you just write it to both. And so for example, if we have client one and client two running, then maybe client two does the same thing.", "metadataJson": "{\"start\":731,\"end\":757}"}, {"text": "And then we're going to ask ourselves the same question. What does c three, c one actually reads? And let's assume that for reading we're going to say like, ah, we're, either way we're going to read from au replica. As I said, this is a very bad replication plan. Basically there's no restrictions.", "metadataJson": "{\"start\":759,\"end\":778}"}, {"text": "So what are the possible outcomes? So this guy writes one, this guy writes two, and, you know, we know we see three. What are the possible outcomes for each read?", "metadataJson": "{\"start\":779,\"end\":792}"}, {"text": "One and two again. Yeah, one and two. Nothing really that bad happening. Correct. How about c four?", "metadataJson": "{\"start\":795,\"end\":801}"}, {"text": "We do a read of x, well, after c three, x like this in the previous board also one and two. Yeah, one and two again. But what happens if c three reads one? What does c three, c 4 may return one or two, one or two, and it's not what we want or not. No, no, I mean, again, correct.", "metadataJson": "{\"start\":802,\"end\":828}"}, {"text": "It would be difficult for an application writer to actually reason about this, particularly even if c three and c four were the same thing. You first read through teacher one, no modification made, and the next second read returns another value. How is that possible? And it makes application programmers difficult to write.", "metadataJson": "{\"start\":828,\"end\":843}"}, {"text": "The reason of course, that this inconsistency shows up here is because we basically have no protocol to coordinate the clients, the readers and the writers. So we need some form of distributed system. Typically we need some form of a protocol to fix these, get the desire to enforce that, we get the desired consistency. What we're going to see in this rest of the semester is a whole bunch of different types of protocols that have different trade offs in terms of fault tolerance and inconsistency.", "metadataJson": "{\"start\":845,\"end\":871}"}, {"text": "In fact, to get our head in that kind of thinking, the we're going to use a whole bunch of different case studies. And, you know, the case study for day is GFS, and this is an instant case study. You know, otherwise we wouldn't have signed it. And one reason it's an instant case study because it brings out sort of all these core issue dfas, you know, design designed to get high performance.", "metadataJson": "{\"start\":874,\"end\":907}"}, {"text": "That means it actually uses replication and fault tolerance.", "metadataJson": "{\"start\":910,\"end\":919}"}, {"text": "And, you know, it struggles with this consistency. So it's like the few four sort of themes that we're going to be consistently seeing throughout the semester, you know, show up in this one paper. The other side of this, why there's an instant case study, because it's a successful system.", "metadataJson": "{\"start\":922,\"end\":937}"}, {"text": "Google doesn't actually use DF's, at least at this point, is my understanding. There's a successor file system called Colossus, but it's inspired by DF's.", "metadataJson": "{\"start\":942,\"end\":951}"}, {"text": "But there are other cluster based file systems for map type use, like hdfs. They're also very much inspired by the design of DF's.", "metadataJson": "{\"start\":954,\"end\":963}"}, {"text": "And one thing that is actually interesting, at the point that this paper was written in late, early two thousands, it was pretty distributed. File systems were well understood topics. People knew about cell tolerance, people knew about replication, people knew about consistency. All that kind of stuff is pretty well understood. However, nobody actually built a system at the scale of thousands computers of.", "metadataJson": "{\"start\":965,\"end\":991}"}, {"text": "And that sort of brings out a number of challenges that previous system have to not address. And in fact, the design is not completely standard. So in design that we're reading about was not sort of the standard design that you would see in academic papers at that time. And there were sort of two aspects that make it non standard, both of which we'll get more, we'll spend more time on. One is there's actually a single master, so the master is not replicated.", "metadataJson": "{\"start\":991,\"end\":1022}"}, {"text": "There's a single machine that is in charge of almost all the coordination in the system. And so that is unusual. Why would you build this fault tolerance system, which has a single point of failure? That means not something that people in the academic literature were doing at that time. And the second thing is that it actually has, it's not consistent, you know, can have inconsistencies.", "metadataJson": "{\"start\":1022,\"end\":1048}"}, {"text": "And again, mostly in the literature, in that particular time, people were really sweating actually to build distributed systems that actually have strong consistency and don't have the anomalies that we saw in the previous work.", "metadataJson": "{\"start\":1053,\"end\":1066}"}, {"text": "So even though a lot of the core techniques were well known, the way they were putting together was actually quite different. And so that makes me understand particularly the scale which the system actually operates is impressive and pretty common even for today. So this issue of struggle between fault tolerance, replication, performance and consistency is a standard problem, recurring problems for almost any distributed storage systems that people build today and changes over time, like s three for a while, didn't really have to have that strong consistency. Lately it's gotten much stronger consistency. Okay, so I want to, since the paper is really driven and the design is driven by fault, by performance, I wanted to go back to the Mapreduce paper for a second, and this is a graph of the Mapreduce paper.", "metadataJson": "{\"start\":1069,\"end\":1127}"}, {"text": "And one way to think about GFS is that it's the file system for Mapreduce.", "metadataJson": "{\"start\":1127,\"end\":1135}"}, {"text": "So the goal is to actually run many Mapreduce jobs and get high performance. And we know that they're basically from, we can tell from the Mapreduce paper already that g vest is impressive in that manner, in terms of performance. So if you look at this side of this graph, this is straight out of the Mapreduce paper. This is the normal execution of one of the Mapreduce jobs, and it has three parts to it. One is, the first part is input, like reading the input files, the inputs to the map from the file system.", "metadataJson": "{\"start\":1140,\"end\":1173}"}, {"text": "And in case the paper didn't say much about it, but those are written read from GFS. There's the internal shuffle that we don't really care about. And then at the end, the reduced jobs write back the results into gfs.", "metadataJson": "{\"start\":1173,\"end\":1186}"}, {"text": "The performance part of the performance of these Mapreduce tasks are determined by the rate at which the mappers can actually read data from the DF's file system. So we have running many, many mappers at the same time. In fact, some mappers from different jobs may be reading the same files. So we look at the input like this top graph shows the input in terms of megabytes per second at the rate at which the mappers actually jointly, collectively, for one particular job, can read from the file system. As you can see, it goes over well over 1000 or 10,000 megabytes per second.", "metadataJson": "{\"start\":1189,\"end\":1228}"}, {"text": "The first question to ask is maybe, is that an impressive number of.", "metadataJson": "{\"start\":1231,\"end\":1235}"}, {"text": "Yeah. Should we be impressed with a number? Or we're thinking, well, you know, give me one disc and I do it too.", "metadataJson": "{\"start\":1237,\"end\":1243}"}, {"text": "Anybody? I think because it's older, maybe. Yes. Okay, good. SSD, how much rates can you write?", "metadataJson": "{\"start\":1250,\"end\":1261}"}, {"text": "Ready?", "metadataJson": "{\"start\":1261,\"end\":1263}"}, {"text": "Okay, let me tell you this. Roughly the throughput of a single disk at the time that this paper was around like 30 megabytes per second, like somewhere in the tens of megabytes per second. So here we're looking at, you know, well over 10,000 megabytes per second. Correct? And so that is an impressive number.", "metadataJson": "{\"start\":1268,\"end\":1288}"}, {"text": "And, you know, you have to do work. As you know, we'll see in the GFS design that allows that kind of throughput. And of course, if the disk technology, in the case of the GFS, of course, if the disk technology was faster, you know, it would be, you know what the real goal here correctly is. Like, we have a thousand machines, maybe each one has a disk, and each one can read at 30 megabytes per second. We just want a thousand times 30 megabytes per second to get out of it.", "metadataJson": "{\"start\":1289,\"end\":1311}"}, {"text": "Okay? And so as drives, a lot of this design is to allow these mappers to read in parallel from the file system, from this joint file system. Okay, so let me say a little bit more about this. Like what are the key properties that GFS has? You know, one big large dataset without.", "metadataJson": "{\"start\":1311,\"end\":1338}"}, {"text": "And so the dataset, you should think about this like the Mapreduce data sets. So for example, the whole complete crawl of the World Wide web is stored in this distributed file system. Has to be fast as we talked about. And the way they get like high performance is doing automatic sharding, shard the files across multiple disks. They allow multiple clients to read from those disks in parallel.", "metadataJson": "{\"start\":1343,\"end\":1369}"}, {"text": "All right, then it goes global.", "metadataJson": "{\"start\":1369,\"end\":1372}"}, {"text": "And with that meaning it's shared where all apps see same file system.", "metadataJson": "{\"start\":1375,\"end\":1382}"}, {"text": "And that's convenient. Like if you have multiple Mapreduce jobs that operate on the same set of files, they can first of all read all the same set of files, but they can produce new files and then other Mapreduce can use those files again. And so this is a very convenient, you know, sharing between applications. So it's very convenient to have. And of course, you know, DF's has to be fault hollow.", "metadataJson": "{\"start\":1386,\"end\":1408}"}, {"text": "I mean it's likely there are going to be failures. And so we want like automatic, close to automatic fault homeless as possible. And you'll see GFS doesn't provide completely automatic, but does a pretty good job of actually getting high good fault.", "metadataJson": "{\"start\":1413,\"end\":1425}"}, {"text": "Okay, any questions about this part so far? Broad intro to this topic and a few intro words about GFS.", "metadataJson": "{\"start\":1428,\"end\":1439}"}, {"text": "Okay, let's then talk about the design.", "metadataJson": "{\"start\":1444,\"end\":1446}"}, {"text": "So here's the design is seen in, you know, from the figure one, I think in the paper, and there's a couple things I wanted to point out and talk a little bit more in detail about. So first of all, you know, we have an application and that, you know, the application again, you know, might be mapreduce, you know, considering multiple reduced tasks, multiple map tasks. And they link with the GFS library. And so it's not a Linux file system. This is not the file system you use to whatever edit your files or compile it is really intended as a specified reverse file system for these large computations.", "metadataJson": "{\"start\":1453,\"end\":1492}"}, {"text": "As I said before, our real goal is to achieve that impressive number. We want the number of megabytes from a single disk times the number of machines a single application should be able to exploit that. The way they arrange that is to have a master that is basically in charge of actually knowing where things are. And the client just periodically talks to the master to retrieve information. So for example, it opens a file, the open call will result in a message to the master and the master will respond back and say for this particular file name, the chunks that you need are here, or these are the chunks that you need.", "metadataJson": "{\"start\":1493,\"end\":1536}"}, {"text": "And these are called chunk handles, these identifiers for the particular chunks that constitute a file. And here are the servers that serve that chunk. So you get back a chunk handle as well as a bunch of chunk locations. And one file might basically file consists, if you think about a big file, it consists of many, many chunks, chunk zero, chunk one, chunk two, et cetera, chunk three, blah blah blah blah, etcetera. Any chunk is pretty big, 64 megabytes.", "metadataJson": "{\"start\":1536,\"end\":1570}"}, {"text": "Even the application wants to read the second 64 megabyte goes to the GFs and says like hey, I want to read the second chunk of this particular file. And then the GF answer will answer back with the handle for chunk one as well as the servers that actually holds chunk one.", "metadataJson": "{\"start\":1572,\"end\":1591}"}, {"text": "Multiple applications might ask for chunks from the same file and they will get, for example, one application might be reading chunk zero, another application might be already at chunk two. They will get different lists back for each of these chunks. Then the GFS client, once it knows the chunks and chunk locations and basically straight talks to the chunk servers and basically read the data at the speed of the network and maybe whatever disk that sits behind this particular chunk server directly to the application here you can see where we're going to get the big win because we're going to be able to read for multiple, multiple clients can be reading from multiple disks at the same time and we're going to get tremendous amount of performance. So for example, if here's a map task running, here's another map task running. And also as a client, they're all going to be talking to the set of servers that hold the chunks of all the collection, the data set, and those are going to read in parallel from all those different Chunk servers.", "metadataJson": "{\"start\":1595,\"end\":1658}"}, {"text": "And that's going to give us the high throughput number. Does that make sense? Is that the overall plan clear here just to complete it like a chunk server is nothing really else than sort of a Linux box Linux computer with disk to it. And in fact the 64 megabyte chunk is just stored as a Linux file in the Linux file system. Okay, so I want to zoom in on the different pieces and I'll start with the master because masters are really the control center here.", "metadataJson": "{\"start\":1659,\"end\":1700}"}, {"text": "So we'll talk a little bit about the state that X and the master maintains.", "metadataJson": "{\"start\":1700,\"end\":1704}"}, {"text": "Okay, so first of all, you know, it has the mapping from file name to an array of, of chunk handles.", "metadataJson": "{\"start\":1709,\"end\":1722}"}, {"text": "And as you saw in the paper, one of the goals of actually is to maintain all this memory, most of the information actually directly available in memory so that master can respond to clients very quickly. And the reason to do that is because there's one master, many clients. You want to execute every client operation as efficient as possible so that you can scale the master to at least a reasonable number of clients. And then for every chunk handle, the master maintains some additional number. Information particular maintains a version number and a list of chunk servers that hold a copy of that chunk.", "metadataJson": "{\"start\":1728,\"end\":1778}"}, {"text": "And as we'll see in a second, you know, one of them is named, one of those servers is a primary and the other ones are the secondaries.", "metadataJson": "{\"start\":1779,\"end\":1788}"}, {"text": "And the typical number that, you know, a chunk is stored out is a free service. We can maybe talk a little bit later about like why free? And then, you know, there is a lease associated with each primary, so there's a lease time maintained as well. Then there's two sort of other big stored components, and these are sort of the file system level things. And then in terms of implementation there's a log and there are checkpoints since the master, the crucial control center, whenever there's a change to the namespace, for example, you create a new file in GFS, or the mapping for file name to junk blocks changes.", "metadataJson": "{\"start\":1790,\"end\":1838}"}, {"text": "All those operations are written to this log, and the log sits on stable storage.", "metadataJson": "{\"start\":1839,\"end\":1844}"}, {"text": "And the basic idea is that before responding to decline that the change actually has been named. The master writes it to stable storage first and then responds to the client. And so this means that if the master fails or crashes, then later comes back up, it can replay the log to reconstruct the state of its internal state. And by writing it first to storage before responding to the client, the client will never observe strange results. You could do it the other way around.", "metadataJson": "{\"start\":1849,\"end\":1878}"}, {"text": "Correct? And that result in a problem because the client will think that the file has been created, server crash, it comes back up and then the file doesn't exist. So there's sort of another consistency point. Now, replaying always all the operations from the beginning of time to log is of course undesirable. That means that if the master crashes and we have only one of them will be down for a long time.", "metadataJson": "{\"start\":1878,\"end\":1900}"}, {"text": "So in addition to that, it actually keeps checkpoints in stable storage. So periodically the master makes a checkpoint of its terminal state and the mapping for file into array chunk handles and stores that on stable storage. And so then they only have to replay the last part of basically all the operations in the log after the last checkpoint. So the recovery is actually quickly. So there's another couple of interesting questions that we can ask ourselves, like what state does need to end up in stable storage, you know, for the mask that actually function correctly?", "metadataJson": "{\"start\":1901,\"end\":1936}"}, {"text": "So the first question that I ask is how about this array of chunk handles, the mapping for file name to chunk handles, does that need to be stably stored or can it be only memory?", "metadataJson": "{\"start\":1937,\"end\":1949}"}, {"text": "But if the master crashes I think it can like get that information from the servers, chunk servers. So maybe only main memory. Yeah, well that's an interesting question, what other people think. So it can be reconstructed from the log. So when the server crashes only the log needs to be in the hard storage and then it can reload it from the log to main memory.", "metadataJson": "{\"start\":1958,\"end\":1986}"}, {"text": "Yeah, so it definitely has to be in the log. Correct. So we agree that this array of check humbles basically has to be in store stable storage because otherwise we lose, like when we create a file and we didn't write the stable storage, we just lose the file. Right. And so this mapping for file name to chunk handles needs to be in a stable storage.", "metadataJson": "{\"start\":1986,\"end\":2005}"}, {"text": "How about this chunk handle to list of chunk for our servers? Does that actually need to be an alarm? I think in the paper they say that when the master reboots it asks the servers to tell the master what the chunks that they have are. Yeah, so this is not actually, this is basically just volatile state, not stable storage. So the same is only true of the primaries and the secondaries and true of the least, Hein?", "metadataJson": "{\"start\":2005,\"end\":2041}"}, {"text": "How about the version number?", "metadataJson": "{\"start\":2042,\"end\":2043}"}, {"text": "Does the master need to remember on stable storage the version number or not? Yes, because it needs to know if the chunks in the other servers are scale or not. Yeah, exactly. That is exactly right. So the master must remember the version number because if it doesn't and the whole system went down and the Turing servers came back up and maybe the Chunk server actually with the most recent data does not come up.", "metadataJson": "{\"start\":2050,\"end\":2079}"}, {"text": "Like an older guy comes up with like version number 14. Then the master has to be able to tell that Chunk server with version 14 was not the most recent Chunk server. And so it needs to maintain the version number on disk so that actually can tell which Chunk server actually have the most up to date information and which ones don't. Okay, I have a question here. Yeah.", "metadataJson": "{\"start\":2079,\"end\":2103}"}, {"text": "If, well I mean if, if the monster fails and then it has to come up, it's anyway going to connect to all of the Chunk servers and it will find out what the largest version is, right. This will find out what the last, first of all it will try to talk to all chunk servers, right. And some chunk servers might be down. Okay. And that may be just the Chunk server actually has the most recent version, right?", "metadataJson": "{\"start\":2103,\"end\":2134}"}, {"text": "Yeah. Okay. So you can't take the max of the live Chunk servers that we incorrect.", "metadataJson": "{\"start\":2134,\"end\":2141}"}, {"text": "Any other questions about this?", "metadataJson": "{\"start\":2146,\"end\":2147}"}, {"text": "Okay, let's look at the two sort of basic operations to really get down to consistency. And of course that's going to be reading and writing. So reading a file and then we'll talk about writing a file. So reading a file in some sense is straightforward. We talked about it.", "metadataJson": "{\"start\":2151,\"end\":2167}"}, {"text": "Basically the client sends a message with the file name plus offset to the master and basically ask please give me the chunk servers and the chunk handle that hold the data at that offset. And n finds the chunk handle. So read byte whatever, zero. It's pretty clear, correct. That has to be the first entry in the list from file name to Chunk Handle.", "metadataJson": "{\"start\":2167,\"end\":2205}"}, {"text": "And putting a master function, chunk handle basically replies, you know, with the master replies to the client with, you know, the chunk handle and list of chunk servers for that, for that handle and the version numbers.", "metadataJson": "{\"start\":2206,\"end\":2226}"}, {"text": "So basically the client gets back and message saying, you know, that's chunk, you know, 221 and here are the free machines or the ip address with the free machines that actually have it. And the version number is like version number ten.", "metadataJson": "{\"start\":2228,\"end\":2243}"}, {"text": "Then the client caches this list and then it basically sends a message to the closest reads from closest server.", "metadataJson": "{\"start\":2246,\"end\":2266}"}, {"text": "And so why does the client actually read cache this information?", "metadataJson": "{\"start\":2271,\"end\":2275}"}, {"text": "Yes, we see later, correct. That actually causes some problems. So it doesn't have to contact the master for some time if it wants to, to read again or write to that chunk. Yeah. And why is that important to like reduce the, I guess the traffic.", "metadataJson": "{\"start\":2277,\"end\":2295}"}, {"text": "And in general it takes less time if you have less communication with the master. Yeah. And then, you know the salient aspect. Correct. This design is that the master actually is a single machine and a single machine can just ask a limited amount of memory and a limit of your own network interface.", "metadataJson": "{\"start\":2295,\"end\":2311}"}, {"text": "And so if you have too many clients talking to it, you wouldn't be able to serve. Client caching is important to reduce the load on this single machine. Why read from the closest server?", "metadataJson": "{\"start\":2311,\"end\":2324}"}, {"text": "Minimize network traffic. Yeah, minimize network traffic. So the whole goal is to pump as much data to the client as possible at the highest throughput. And there's two problems that we have to cross the data center network. One, there's probably some topology and we maybe swamped like the top links of the topology and they actually increase latency to actually get to the other side.", "metadataJson": "{\"start\":2326,\"end\":2353}"}, {"text": "So it's important to be able to read to the closed site again to basically maximize the throughput. That joint set of clients can sort of experience when they're reading in parallel from many, many chunk servers.", "metadataJson": "{\"start\":2353,\"end\":2365}"}, {"text": "So the Chunk server s, you know, basically checks the version number.", "metadataJson": "{\"start\":2368,\"end\":2372}"}, {"text": "And if the version number is okay, then send data.", "metadataJson": "{\"start\":2375,\"end\":2378}"}, {"text": "Okay. Why is the check of the version number there?", "metadataJson": "{\"start\":2382,\"end\":2386}"}, {"text": "To check if it's to stale. Yeah, we usually do our utmost best to avoid reading stale data, as we'll see in second, we don't do GFET doesn't do a perfect job at, but tried hard to minimize the occurrences of clients reading stale data. Okay, that's reading reasonable, straightforward. So let's look at writing.", "metadataJson": "{\"start\":2390,\"end\":2417}"}, {"text": "So this is the picture from the paper. And so let's say a client wants the, you know, let's focus on a pin.", "metadataJson": "{\"start\":2420,\"end\":2428}"}, {"text": "And so they argued that the very common operation for them is to append a record to a file. And can we see why, you know, given what you guys know from Mapreduce and, you know, Google, does that make sense? Why append is so important?", "metadataJson": "{\"start\":2440,\"end\":2456}"}, {"text": "Because largely in doing Mapreduce you need to, or as the map function spits out information, it's largely just adding on information rather than changing previously spit out enumerations. Yeah. Maybe the map is not the best example because it writes it through the local files and not to gfs, but the reducer does the same argument also. The reducer, yeah. So, you know, the workloads that are writing is basically consume a lot of information and, you know, append records, you know, to file with the resulting computation, with the result of the computation.", "metadataJson": "{\"start\":2462,\"end\":2495}"}, {"text": "Okay, good. So, you know, step one, you know, we have a client, it will talk to the master to figure out like where to write to. And so the master looks in its table, the file name to Chunk handles and finds, you know, the chunk handles, and then, you know, looks at this table of, you know, Chunk handles servers to find the list of servers that it has that have a particular thing that have that particular Chunk. Okay, so what happens next? So there's two cases.", "metadataJson": "{\"start\":2495,\"end\":2537}"}, {"text": "One, there's already a primary, and the second case, the first case, well, the two cases having a primary or not a primary. So let's say this is the very first time that this particular client contacts the master for this particular chunk. Nobody else has done it so far. So there's no primary. So in that case we need to, the master needs to pick a primary.", "metadataJson": "{\"start\":2537,\"end\":2557}"}, {"text": "How does it do that? I think the master just picks any of the available Chunk servers, right? Yep. Picks one. So it picks one of those with primary and the other ones are the secondary.", "metadataJson": "{\"start\":2558,\"end\":2570}"}, {"text": "What other steps are involved in this? Sort of. Yeah. And then subsequently the master grants a lease to that primary and that lease has a certain like date of expiry. Yeah.", "metadataJson": "{\"start\":2570,\"end\":2583}"}, {"text": "What else does it have? There's one more, other piece of crucial information, even it increments the version number. Yeah, increments. Oh, oops. Yeah.", "metadataJson": "{\"start\":2583,\"end\":2596}"}, {"text": "Step one is increment the version number. Correct. Because you're going to make a new primary. And whenever time you make a new primary, you go sort of like one. One way to think about it is like a new epoch in the file system over this particular file.", "metadataJson": "{\"start\":2596,\"end\":2610}"}, {"text": "And so you create the version number because you're going to have a new mutator. So basically the master increases the version number it sends to the primary, the new version number and the secondaries and saying like, hey guys, we're going to start a new, we understand a new mutation. You guys are forming a replica group and your replica group with this particular version of whatever, like version number twelve. And the primary's in the secondary. Store that version number.", "metadataJson": "{\"start\":2610,\"end\":2640}"}, {"text": "What they do is they store that version number.", "metadataJson": "{\"start\":2640,\"end\":2642}"}, {"text": "Do they store it on disk, on their disks or in memory or, I don't know, anyone. What do we, what do we think? Okay, let's first do memory. Let's say it's distorted in memory. Would that be a good design?", "metadataJson": "{\"start\":2647,\"end\":2666}"}, {"text": "No, sorry. You can go. I guess it wouldn't, because if the chunk server goes down and then it comes back up, it, it should know what version it has. Yeah, because otherwise you couldn't convince the primary that it has the most recent one. Correct.", "metadataJson": "{\"start\":2668,\"end\":2684}"}, {"text": "Otherwise the prime, sorry, the master couldn't pick, you know, the chunks or the most recent data. So it has to be on disk. So basically this version number lives on disk, both at the chunk servers and actually at the master. Right. So when the chunk, when the master gets back, you know the acknowledgments from the primary and the secondary that they written the version number two disk and that the primary actually has received the lease.", "metadataJson": "{\"start\":2684,\"end\":2710}"}, {"text": "Then the master also writes its version number to disk and then responds to the client. Okay, so back to the client and responds with the list of servers, primary plus the secondaries, plus the version number. Okay. Then the next step. And again here we see the whole goal is to pipe a lot of data through the network is the client actually just sends the data that wants to write to the primary and the secondaries the way it actually does.", "metadataJson": "{\"start\":2710,\"end\":2744}"}, {"text": "It is sort of an interesting way, it basically contexts the closest secondary it knows of out of this list and sends the data there. And that secondary move the data over to the next person in the list and then to the next server in the list. And so that's the way the data is pumped from the client to the pipeline to all the replicas. And when the secondary receives, the first secondary receives some of the data immediately starts actually pushing the data further down the pipeline.", "metadataJson": "{\"start\":2744,\"end\":2776}"}, {"text": "The reason that this design is this way is basically this network interface that the client has that goes through the outside world. Usually uses a full network interface to push the data down the pipeline. That gives us high fruit.", "metadataJson": "{\"start\":2778,\"end\":2790}"}, {"text": "Okay, so then if this is all successful and the data has been pushed to all the servers, those servers don't store that information on disk yet. It just sits there sort of on the site to be used in the next step. So the next step is then basically for the client to send a message, like an append message, to the primary. And at that point the primary will check the version number, whether it actually version number corresponds to each version number. And if it doesn't correspond to each, they don't match, then the primary won't allow it.", "metadataJson": "{\"start\":2794,\"end\":2829}"}, {"text": "The primary checks is lease if the lease is valid, because the lease is not valid anymore, it cannot accept any mutation operations. Because if this lease is not valid, there might be another primary outside in the world. So it checks the lease. And then if basically the version numbers match, the least is still valid and basically picks an offset to write end.", "metadataJson": "{\"start\":2829,\"end\":2851}"}, {"text": "And then the next step is basically write the data that just came in this record to stable storage. So the primary actually at this point writes it to stable storage the data, and then sends messages to the secondaries saying, please write the data too. And since the primary picks the offsets, it tells the secondary where to write that particular record into the file. So maybe like whatever it takes, the offset 125 and then it will tell the secondaries all to write the data that came in earlier at offset 125. Then if everything works out, everybody, all the secondaries in the primary successfully write their data back to disk, then actually responds back to the client saying like, okay, success, your append actually has happened.", "metadataJson": "{\"start\":2854,\"end\":2905}"}, {"text": "There's a way that the write actually might be not successful or the append might not be successful. And namely for example, the primary is written to its own disk, but it fails to write it. It fails to connect to one of the secondaries. Maybe the secondary actually crashed, or maybe the secondary just has a network connection that doesn't work. And in that case the primary actually returns an error to the client.", "metadataJson": "{\"start\":2907,\"end\":2930}"}, {"text": "So error if one secondary didn't respond and in that case the client library. What it will do is usually try a retry. It will reissue the same appendix and we'll try again in the hope that the second time around that data actually gets through. And so this is what they recall like you do the least once.", "metadataJson": "{\"start\":2931,\"end\":2965}"}, {"text": "If you retry will the primary pick the same offset? I don't think so, no I don't. It takes a new asset and writes it at this new particular offset. So that means if you look at the disks of the year of the file on the three replicas, the primary, s one and s two, it might be the case that we wrote at 125 the data we succeeded maybe in s two but s two actually doesn't have no data. And then we retry again.", "metadataJson": "{\"start\":2970,\"end\":3009}"}, {"text": "Then we might drag the read the same data to x and maybe we'll succeed in all three. We can see here that basically records can be duplicated.", "metadataJson": "{\"start\":3009,\"end\":3018}"}, {"text": "Is this something that can happen in a standard file system like your linux file system on your laptop or your computer?", "metadataJson": "{\"start\":3020,\"end\":3026}"}, {"text": "No. No. Would you be surprised if this computer did this?", "metadataJson": "{\"start\":3031,\"end\":3036}"}, {"text": "I mean, yeah, this is not how standard file writes work. Yeah. And it would be inconvenient to have this property or is it like does matter inconvenience? Yeah, inconvenient. It would be pretty bizarre.", "metadataJson": "{\"start\":3039,\"end\":3057}"}, {"text": "Like presumably like you know, you, compiler produces output in a file and then maybe, you know, certain blocks are written twice and then you can't run the program anymore. Like you know, the whole thing is just garbage at that point. So it would just be weird. Like you write an email message and basically the body of the email message shows up twice. So this is not what your typical file system would do.", "metadataJson": "{\"start\":3057,\"end\":3079}"}, {"text": "And so this is like slightly bizarre. And you know, what is the justification? Why do you think this is a good idea?", "metadataJson": "{\"start\":3079,\"end\":3087}"}, {"text": "I'm not sure what this is a good idea but I'm confused how that works for mapreduce specifically. So if you run word count and you do that and like some files and you count and they're like word a it shows up once but you do it twice because something failed and now you have a one. A one. So your account is going to be wrong how? Yeah, I'm confused.", "metadataJson": "{\"start\":3089,\"end\":3114}"}, {"text": "Yes. How do they work? It seems like, you know, if you don't do anything then this is really highly inconvenient. Or actually returns the application will compute the wrong result.", "metadataJson": "{\"start\":3115,\"end\":3125}"}, {"text": "They said they use checksums and unique ids to check you know that every. Yeah, every record was like my ones. Additionally, when you do record append, the response which is returned from the primary to the client gives you the offset into the file where your data was actually written and the rest of it is assumed to be undefined. Yeah, I think the key point here, correct, is basically the application doesn't interact with the file system directly. It interacts with some library.", "metadataJson": "{\"start\":3128,\"end\":3163}"}, {"text": "And the library basically if you write a pen record, the library sticks an id in it. And so, and also you use the library to read, you know, these records. And so if you see a record with the same id, you know, you skip the second one because you know it's clearly the same one. And you know, they have a double extra finger there for checksums to make sure the record didn't get garbled. And we basically detect the changes in the bytes.", "metadataJson": "{\"start\":3163,\"end\":3189}"}, {"text": "But the id basically helps them to decide or allows the library to decide. Well this is the same record. I'm not going to give it to the application or the application doesn't need to process it. Okay, my question is, instead of rewriting to every replica, wouldn't it be better to remember which replica is failing and to stop until it can be returned to that one? Yeah, so there's a bunch of different designs possible.", "metadataJson": "{\"start\":3189,\"end\":3216}"}, {"text": "Let's return to that later.", "metadataJson": "{\"start\":3217,\"end\":3218}"}, {"text": "And I think one reason that they do this this way is like if there's a temporary failure, like a network disconnection or whatever, you know, the least the right will succeed and they will continue. And there doesn't have to be any reconfiguration, there has to be nothing. You know, the right can just keep going. Right. And so the right doesn't have to fail.", "metadataJson": "{\"start\":3221,\"end\":3240}"}, {"text": "Okay, just a quick question. In general, all of these servers are trusted, right? There's absolutely. This is actually an important point. This is not like your Linux file system where there's permissions and access control rights and all that kind of stuff.", "metadataJson": "{\"start\":3242,\"end\":3261}"}, {"text": "These servers are completely trusted. The clients are trusted, the master is trusted, the software written by Google is trusted. The whole thing is trusted. This is a completely internal file system. In fact, it's sort of cool.", "metadataJson": "{\"start\":3262,\"end\":3275}"}, {"text": "It's a little bit maybe surprising that we even know about this file system in such a detail because it's only used inside of Google. And one of the cool things is that in that period of time and still they do, they wrote up the papers describing actually how these systems work. And there's one reason we know about it and that's extremely cool that they did that.", "metadataJson": "{\"start\":3275,\"end\":3296}"}, {"text": "Okay, so okay, so we now understand how read works. We understand that write works. You know, there are some sort of interesting behaviors. I want to talk a little bit more about consistency. Correct.", "metadataJson": "{\"start\":3299,\"end\":3311}"}, {"text": "And that really comes down to, you know, what does a read observe after you did an append and the homework question really got after this. And so what I would like to do now is I take a quick breakout like five minutes, and so you can discuss the answer to this question and then come back and talk a little bit more in detail about consistency. Okay, so I'm going to make lily. Okay, everybody back, everybody can hear me. Just double checking.", "metadataJson": "{\"start\":3311,\"end\":3348}"}, {"text": "Yeah. Hey professor question, can you go back to this slide with the, when we talked about the right ci care. So you mentioned the master response to the client with the version number. Yep. And if that is the case, then isn't it possible?", "metadataJson": "{\"start\":3348,\"end\":3365}"}, {"text": "Is it even possible? Would it even have to read like a steel data because the client has a version number and then the chunk service would have the version number so they can just compare those. If they don't match the chunks errors can just say I have a steadily rat so you should not read this. Okay, yeah, let's go for the scenario in a little bit more detail. Let me actually get rid of this window.", "metadataJson": "{\"start\":3366,\"end\":3389}"}, {"text": "Okay, let's talk about, so I think the scenario that we're talking about that leads into a problematic situation as follows. We have a primary, we have a secondary, two secondaries, secondary one, secondary two. We have a client on this site, we have a primary decline reach. You know, it's back like a version number, say ten later on another primary will.", "metadataJson": "{\"start\":3395,\"end\":3428}"}, {"text": "Okay, so s two, it has got some servers then at some point. So this information is cached on the site. Maybe one of the secondaries like s two crashes or at least appears to be disconnected from the network. So what the master will do is increment the version numbers, go to eleven Massachusetts 1111. Then, you know, another client may come around and start writing.", "metadataJson": "{\"start\":3436,\"end\":3468}"}, {"text": "So it will write in a new value to s one and s two for the file. So the chunk has now been updated. So let's say the chunk was original ten, same as the version number. And now it's eleven. There's eleven here.", "metadataJson": "{\"start\":3469,\"end\":3481}"}, {"text": "But you know, there's the case that, you know, even though the master coon and the primary secondary coon can talk to secondary too. But the second client, the first client can still talk to the secondary and it will read their version numbers match. Right. They're both ten and it will read, it will send ten back. So here we have a case where a write has completed as acknowledged has to be okay.", "metadataJson": "{\"start\":3482,\"end\":3507}"}, {"text": "And nevertheless there's a client that actually will read a stale value back. So why doesn't that eleven go back to the client, the first client? The reason is because the first client caches it for a longer period of time. They don't actually have anything in the protocol that actually does that.", "metadataJson": "{\"start\":3507,\"end\":3526}"}, {"text": "So does the version increments when the system tries to push an update to s two and it's not able to, or version numbers only incrementally. The version numbers maintained by the master may only increment when you select a new primary, not when you do. There's also a serial number that they talk about, but that's different from the version number. That's just to order, you know the rights. Okay, how does a primary know which secondaries it has to check with before making, before completing a write successfully?", "metadataJson": "{\"start\":3529,\"end\":3570}"}, {"text": "The primary, the master tells us. The master tells the primary secondaries you need to update.", "metadataJson": "{\"start\":3571,\"end\":3577}"}, {"text": "So when the master basically issues the lease to the primary, and if one of the secondaries is down at that moment, does the master consider this a failure or does it just update the version number for the servers that are alive and it just forgets about the other one because it's going to have an outdated version number anyway. Yeah, the paper is a little bit vague on exactly how the recovery part or the reconfiguration stuff works, but I imagine that basically the primary actually does heartbeats with p one, s one and s two. At some point it decides s two is dead and at that point it will point and the lease of the primary maybe runs out and then it will create a new primary and a new s one and another s, you know, to actually hold or maybe just s one because there's no, you know, no additional chunks over and that forms the new replica group for that chunk. Also, the least doesn't run out yet. Basically.", "metadataJson": "{\"start\":3579,\"end\":3641}"}, {"text": "Well, the primary can't point. Okay, so here's like some interesting cases, let's say. So you guys are doing exactly the thing I want based on this paper, which is you really start thinking about all the problematic cases. And this is exactly how you think about consistency. When you start thinking about consistency, you need to sort of consider all possible failures and argue whether those failures lead to inconsistencies.", "metadataJson": "{\"start\":3641,\"end\":3664}"}, {"text": "So the one thing, let's talk about this one case where we got a master, we got a primary, and let's say the primary and the master get disconnected. Let me draw the picture slightly differently. The master in the middle. And we got a server. We got a server here.", "metadataJson": "{\"start\":3665,\"end\":3686}"}, {"text": "S one, s two. And let's say s two is the primary.", "metadataJson": "{\"start\":3687,\"end\":3690}"}, {"text": "And so, you know, whatever it may talk to some other servers out there and perhaps even as one is the, is one of the secondaries for this primary. So let's say there's a network petition. So the master sends messages, you know, heartbeat messages, doesn't get a response.", "metadataJson": "{\"start\":3692,\"end\":3710}"}, {"text": "When can the master point a new primary?", "metadataJson": "{\"start\":3713,\"end\":3715}"}, {"text": "When the lease is over for us to.", "metadataJson": "{\"start\":3719,\"end\":3721}"}, {"text": "Yeah, correct. Because the primary has to wait with a master has to wait until the lease has expired. Because if the lease was not expired then maybe we have two primaries at the same time. Right. P one and p two staying at the same time.", "metadataJson": "{\"start\":3723,\"end\":3742}"}, {"text": "And would that be bad?", "metadataJson": "{\"start\":3742,\"end\":3743}"}, {"text": "Yeah. Then I think, wait, clients wouldn't know where to send to and the master wouldn't know which one is primary. Right. Well, presumably some clients might be still talking to this primary, correct? Yeah, some other clients might be talking to this primary in their primary for the same chunk.", "metadataJson": "{\"start\":3747,\"end\":3764}"}, {"text": "So I think you get very bizarre orderings. Right? Where like some rights will get lost, you know, you know, like be a mess. It would be not a principle to, you know, argument where like one, you know, all rights happen in order, you know, one at a time.", "metadataJson": "{\"start\":3767,\"end\":3781}"}, {"text": "So this is a bad situation. And so the situation is voided like the split brain syndrome, this is sometimes called the split brain syndrome where you end up with a system where you have two masters and this is a problem here, is avoided because of like the lease. The master will not appoint any other primary until the least of the first primary absolutely has expired. And it knows even if the primary is up but not reachable to it, but maybe reachable to other clients that prime. We won't accept any write messages anymore because at least has expired.", "metadataJson": "{\"start\":3783,\"end\":3814}"}, {"text": "Does that make sense?", "metadataJson": "{\"start\":3820,\"end\":3821}"}, {"text": "Okay, let me say one more thing before wrapping up. And I apologize. This is partly because, you know, I had some technical problems, but I wanted to make one more point and came up in the discussion too, in the breakout room, which is, you know, how can you do better how to get strong consistency or maybe just stronger. They got pretty strong consistency but not, you know, not with some little issues. So there's a bunch of different ways you could do it.", "metadataJson": "{\"start\":3823,\"end\":3857}"}, {"text": "And in fact, you know, the, what we're going to be seeing, I think one issue that shows up here all the time is you could, instead of updating the primary and then reporting, making writes visible incrementally is probably not a good idea. What you want to do is update all secondary primaries or none, but not as in this particular design where somebody get updated and some may not be updated and that's actually visible to the cloud. So there are a bunch of techniques or protocol changes that you could do that will make this better. And in fact we'll see in labs two and three, you will build systems that actually have these stronger properties and the deal with these scenarios that currently you hear lead to inconsistency. In fact, if you look at Google itself, and we'll read some of these papers later, Google built additional storage systems, other storage systems that have stronger consistency and basically tailored also to a different application domain.", "metadataJson": "{\"start\":3858,\"end\":3929}"}, {"text": "So for example, like in like halfway the term, we read this paper called Spanner, you know, that actually has a much stronger story for consistency and even has support from transactions. But as like the application domain is quite different, you know, like you can sort of see here that GFS is really tailored, you know, to sort of the reduce running Mapreduce jobs. Okay. So I hope this is a useful introduction, sort of consistency. And we're starting thinking about these kinds of problems because they will be recurring set of problems that will show up in the rest of the term.", "metadataJson": "{\"start\":3929,\"end\":3962}"}, {"text": "And I apologize for running over a little bit late. Thank you. Hanging around. So if people want to ask additional questions, you know, feel free to ask them. And if you have to run to another class, you know, please run to another class.", "metadataJson": "{\"start\":3963,\"end\":3976}"}]}