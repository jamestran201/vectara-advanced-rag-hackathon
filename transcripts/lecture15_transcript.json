{"documentId": "lecture15", "section": [{"text": "Okay, good afternoon or good evening, or good night or good morning. Whatever time zone you're in and watching this lecture, just double checking on the sound. People can hear me.", "metadataJson": "{\"start\":4,\"end\":19}"}, {"text": "Yes, yes. Super. Okay, I'm going to talk about two things today. I want to finish off our discussion of farm, and then I'll talk about Spark. And just to remind everybody else, or everybody where we were, farm, we talked about executing a transaction without any failures.", "metadataJson": "{\"start\":25,\"end\":45}"}, {"text": "And we looked at one example of whether it actually transaction provided external consistency or strict serializability. And so I want to talk about one more example to look about, to talk about serializability and then talk a little bit about file tolerance. But before doing so, it's probably helpful to remind ourselves exactly how transaction works if there are no failures. And so in farm, an application goes sort of for two phases, the execution phase, where it fetches objects from different shards, and then the commit phase. And so if we look back at this picture here, you know, there's an execution phase.", "metadataJson": "{\"start\":45,\"end\":89}"}, {"text": "Three objects are being, three objects are being read, you know, one from each different chart. You know, we have shard one, shark two, and shard three. And every shard has one backup, so the system can tolerate one failure. So the objects are red, the two objects are being modified, as we see in a second. And then once the three objects are read and two are modified, the application might decide to commit.", "metadataJson": "{\"start\":90,\"end\":121}"}, {"text": "And then, so the whole commit phase goes into action. This whole story from steps one through five. In step one, the transaction takes out locks on the objects that have been written. So we see here there have been two objects that have been written to by the transaction. And so we're propagating what's called lock records to every primary.", "metadataJson": "{\"start\":121,\"end\":147}"}, {"text": "So every primary is going to have a lock record for this transaction and for the objects involved in that transaction. And the lock record contains an object id which identifies the object uniquely, a version number. So at the time that the disaction reads an object, it gets back a version number. In fact, it gets back this 64 bit number, which at the top is a lock bit and the rest is the version number and then a new value.", "metadataJson": "{\"start\":148,\"end\":184}"}, {"text": "So, you know, primary one and primary two is going to have, primary one is going to have a lock record for object one, primary two is going to have a lock object for primary, for object two. Then there's a separate phase for the validation of the read only operation. So, operations or objects that are only be read but not modified. And as we can see in those, in this validation step, the dotted lines represent one sided rdmas, and as we talked about on Tuesday, those basically involve no server involvement. The sender can just read an object out of the memory of the server without actually having to interrupt the computation that's actually running on the server.", "metadataJson": "{\"start\":188,\"end\":236}"}, {"text": "So they tend to be very fast, unlike these operations that we saw here, which are actually write rdmas.", "metadataJson": "{\"start\":236,\"end\":243}"}, {"text": "They're cool too, in the sense that you get to write to the appends to this log record without actually interrupting the server. But the server actually has to process these log records. And the server, in this case, once it actually processes one of those lock records, tries to take out the lock. If it succeeds trying to take the lock because the version number hasn't changed, or the lock bit, or no other transaction is taking the lock, then it will reply back using another one, right RDMA and saying okay, so we see that on the acquiring the lock requires action on the server, but the one sided RDMA for reads validation doesn't require any action on the server. Then there's the if everything turns out to be okay, so the transaction is basically able to obtain all those write locks and validate the read operations.", "metadataJson": "{\"start\":247,\"end\":303}"}, {"text": "It actually makes a decision to commit, and to commit it first communicates to every backup the change. So it writes a commit backup record and appends that to every to the backups of the objects that have been modified. And again, you know, it's basically copy of the Bach record. You know, the oid goes in there, the version number goes in there, and the new value, once you know, all the backups, you know, have reported that they actually have a copy of the object. Now we're basically in pretty good shape, correct, because the primary has a copy, the backup has a copy, except the primary doesn't really know if the transaction has been committed yet or not.", "metadataJson": "{\"start\":304,\"end\":359}"}, {"text": "And so a final step is that actually the coordinator or detection coordinator writes out commit record and commit primary record, informing the primaries that actually the transaction actually has committed. And as soon as it gets an acknowledgement from one of the nics indicated by the dotted lines, it actually reports back to the application saying that the transaction commit.", "metadataJson": "{\"start\":359,\"end\":394}"}, {"text": "So what we want to do now is look at two cases. One, we want to talk a little bit more about decentralizability, just to see if the concurrency control worked out. And then we want to talk a little bit about fault tolerance and see if the fault hollow story works out. So let me first start with serializability, or actually maybe before doing that, let me just ask if there's any questions so far whether everybody sort of has swapped, successfully swapped farm back into your memories.", "metadataJson": "{\"start\":397,\"end\":428}"}, {"text": "What does the rectangle mean in this picture? So what's the rectangle? What does it signify along the edge? Oh, it's an object. Oh, it's an object.", "metadataJson": "{\"start\":432,\"end\":442}"}, {"text": "Okay, sounds good. Okay, any other questions?", "metadataJson": "{\"start\":442,\"end\":448}"}, {"text": "Okay, good. Let's, so let's look at the correctness from the concurrency perspective. So the correctness correct is strict serializability.", "metadataJson": "{\"start\":451,\"end\":463}"}, {"text": "And on Tuesday, we looked at a transaction that didn't really involve a write validation because there was no object that was read but not written. And so I want to look at another transaction this time around, where actually the validation phase plays a role. And so I'm going to look at two transactions. And this is sort of, this example is a classic example to test whether a protocol provides utilizability or not. Of course, it's not going to be approved, but it's sort of one of these key examples that generally is very helpful to see, to understand how the protocol works.", "metadataJson": "{\"start\":469,\"end\":504}"}, {"text": "So the protocol, the transactions are if x is zero, then we're going to set y to one. So if object x is zero, we'll set object y to one. Intersection two is sort of the opposite. It looks if y is zero and if y is zero, we'll set x to one. And the reason that these sort of, this is a good test for serializability is that either transaction one should go after t two correct or t two should go after t one.", "metadataJson": "{\"start\":504,\"end\":533}"}, {"text": "And depending on which order you run, either y is one or x is one. But you never should have, you never should have the outcome that x is one. Whoops. X is one and y is one. And that should not be allowed because definitely violates your liability.", "metadataJson": "{\"start\":534,\"end\":555}"}, {"text": "Does that make sense?", "metadataJson": "{\"start\":556,\"end\":557}"}, {"text": "Okay, so what we want to do is, you know, test whether firearm actually is successful in, we want to understand where fire max. Indeed, it is not possible that x is one and y is one get produced. So let's look at the timeline. Let's say here we have t one. Here we have t two timeline.", "metadataJson": "{\"start\":562,\"end\":584}"}, {"text": "So they both, you know, in their sort of the preparation or the execution phase, they both read these objects. So let's say they run truly concurrently. We do a read of x, get at version number zero. We do a read of y, version number zero. Same here.", "metadataJson": "{\"start\":586,\"end\":600}"}, {"text": "And of course, t one will update y, t two will update X. And they basically, at some point both start to commit phase.", "metadataJson": "{\"start\":604,\"end\":614}"}, {"text": "And, you know, let's, so let's say the t one starts first, and basically it grabs the, you know, it needs to lock on y since it's going to be writing y. So let's say x successfully graph the log and y. And so that actually will set, you know, the log bit in the version number of the Y object. And then, you know, let's say, you know, actually since it's going to read as has red x, but it's not, it has red x, but it's not modifying x, it's going to be validation of x. X was read at version number zero, the validation files.", "metadataJson": "{\"start\":619,\"end\":664}"}, {"text": "Nothing extra has changed. Yes, with x the version number is still zero. There we run executed in this order. Things are fine. And at some point this transaction might commit.", "metadataJson": "{\"start\":664,\"end\":678}"}, {"text": "Let's look at t two. And let's say t two runs after the validation of x. So it grabs the log and then it has read y. So it needs to do a validation of y. So it's going to do a validation of y.", "metadataJson": "{\"start\":679,\"end\":701}"}, {"text": "And the question is, is that validation could succeed or not?", "metadataJson": "{\"start\":702,\"end\":707}"}, {"text": "No, because the previous, the other operation has a lock and modified the value so it's not the same that was originally read. Yeah. So the version numbers might mean they're still the same, but the y t one excel has set the lock bit right for the object of y. And so at this point this validation will fail because it's not the same, or the lock bit has been set. And so t two transaction, the t two transaction will abort.", "metadataJson": "{\"start\":711,\"end\":738}"}, {"text": "Okay. Does that make sense? So we see at least in this particular example, that, you know, the, it is the case that t one and t two both don't commit, which would result in this incorrect outcome.", "metadataJson": "{\"start\":739,\"end\":752}"}, {"text": "Any questions about this?", "metadataJson": "{\"start\":754,\"end\":756}"}, {"text": "I had a question. Yeah, yeah. So like for, for the transactions. So these are like, these have to be like update transaction, like write, correct. Like if you read.", "metadataJson": "{\"start\":759,\"end\":774}"}, {"text": "Yeah, go ahead. I was just wondering like if they were like read operations, you could do that like lock free. Right. And as long as the. Yeah, so this is actually good.", "metadataJson": "{\"start\":777,\"end\":787}"}, {"text": "Let's go back to this picture here. Right. Then let's look at the, let's say there were no writes involved in this transaction at all. For example, the two objects that are being read, correct. Are stored at p one and p two.", "metadataJson": "{\"start\":787,\"end\":799}"}, {"text": "Let's assume that these guys are actually not involved. So those objects were not written so that the only operation that's happening is a read of that object. And if you see the protocol is carefully designed so that if you only do reads, you only do one sided rdmas. Correct. Here in the execution phase and one RGCA to do the validation.", "metadataJson": "{\"start\":800,\"end\":822}"}, {"text": "And no locks are taken out, no writes are being done, no records are being appended. That's the only thing that happens. And so this is one of the cool features about farm, is that these transactions that only do reads can be executed without only one sided rdmas and only with no rights to any logs or grabbing any logs. And so that's why, one reason that they get extremely high performance. And this is also the reason why, for example, the lock phase, the lock step and the validation steps are two separate things.", "metadataJson": "{\"start\":823,\"end\":863}"}, {"text": "Because in for read only transactions, there's no lock step.", "metadataJson": "{\"start\":863,\"end\":868}"}, {"text": "Okay, actually question on that. For read only transactions, why do we need the second? Why do we need the validation phase? Because aren't you like reading a value and then you're immediately validating right after it, like the version, it seems like there could be another transaction that has been, has modified the object yet. So even transaction rank or started and committed before.", "metadataJson": "{\"start\":871,\"end\":896}"}, {"text": "Okay. A concurrent transaction that actually writes might modify the object and that would be bad. Right. So the transaction that writes, then the transaction that actually follows it should observe that last write. But if they occur at the same time, then we can reorder them either way.", "metadataJson": "{\"start\":897,\"end\":922}"}, {"text": "Yeah, if they occur exactly at the same time, we can order reorder them. Yeah.", "metadataJson": "{\"start\":922,\"end\":928}"}, {"text": "So it still seems to me like the second validation, because the first time you read it, the second time you just immediately ping and save the version is the same. It still seems to me like the second validation is like almost unnecessary. You might be right. I haven't found very hard about this, that if there's, if there are transactions or only read only transactions, you know, then the validation is not really not necessary. I haven't very carefully thought about like when there's a mix of transactions where there's a case where you need the validation.", "metadataJson": "{\"start\":932,\"end\":964}"}, {"text": "Yeah. Wouldn't that be the case where like you have, like if you read a value, like you expect to read like two values atomically, you read a value after you read a value, like some transaction modifies the other value. Yeah, that possibility, I'm not quite sure, actually. In that case, if that's a problem, like for example, in this case, this t one and t two cases are really crucial that y actually does the validation. Correct.", "metadataJson": "{\"start\":964,\"end\":999}"}, {"text": "Even though actually that transaction t one only reach y.", "metadataJson": "{\"start\":1001,\"end\":1007}"}, {"text": "Correct. What if transaction two was just x equals one instead of like without the if statement blind write?", "metadataJson": "{\"start\":1009,\"end\":1020}"}, {"text": "If it's just a blind write, yeah, it's just x equals one and then it executes after the validation of t one. No, that's fine. Correct.", "metadataJson": "{\"start\":1022,\"end\":1032}"}, {"text": "But, but we didn't, I'm not sure what the question is. So if t two is just like, you know, write x equals one within the validation step, like after, and it executes after the validation step of t one wouldn't then t one think that x equals zero, but then x becomes one before the commit.", "metadataJson": "{\"start\":1039,\"end\":1064}"}, {"text": "Okay, so again, let me, maybe we can make a, hold it up because I gotta redraw the whole picture and figure out exactly what the scenarios you're talking about. Okay, so maybe we can go back to this at the end.", "metadataJson": "{\"start\":1067,\"end\":1078}"}, {"text": "Sorry, I had a question. What is the use case for a only transaction?", "metadataJson": "{\"start\":1082,\"end\":1086}"}, {"text": "If you think back at Spanner and this paper, like, there's often the case in these workloads, the TPCC workload and the TATP workload, where there's a transaction that only does read, for example, I've confused the balances of a set of accounts. Nothing is being written, but a lot of things, a lot of accounts are being read.", "metadataJson": "{\"start\":1090,\"end\":1111}"}, {"text": "Thank you.", "metadataJson": "{\"start\":1113,\"end\":1114}"}, {"text": "Okay, so we see here that there's actually, the validation phase is crucial correct for this t one and two tk transaction. And furthermore, that actually figures out, work out correctly. We get strict serializability. Of course this is not approved, but it gives you this example that sort of tried to get at the tricky case. Actually your farm seems to work out correctly.", "metadataJson": "{\"start\":1118,\"end\":1139}"}, {"text": "Okay, so that's on the concurrency control. Then the second part we want to talk a little bit about is fault tolerance. And just going to want to get the, not going to go a great amount of depth here. Just want to talk about the key challenge and see, build some intuition why we might actually be hopeful that farm actually addresses that key challenge. And so the key challenge, the key challenge is that the transaction order crashes after telling the application.", "metadataJson": "{\"start\":1140,\"end\":1186}"}, {"text": "And then it has to be the case that the transaction persists because we have informed the application that the transaction is committed. So we can actually lose any rights that the transaction has done. And so we can look at this picture again and see if we're going to be hopeful that this is the case.", "metadataJson": "{\"start\":1193,\"end\":1215}"}, {"text": "So, you know, there's a couple things to observe. After the lock phase, it is the case that after the lock phase, it is the case that the two primaries, p one and p two, have a lock record which describes the update. We don't really know if the transaction, you know, that record doesn't say whether the record actually whether the transaction is committed, but we have information about the transaction. Then after this step, the commit backup step. Now we know that, you know, backup b one and b two have the commit record, and then before the transaction, the transaction coordinator reports to the application that it has been successful.", "metadataJson": "{\"start\":1222,\"end\":1282}"}, {"text": "We know that one of the primaries, let's say p one, also has the commit record.", "metadataJson": "{\"start\":1282,\"end\":1287}"}, {"text": "So let's assume there's a crash, like right here. So at that particular, just after that commit point and the system crashes. And what we want to convince ourselves of is that if there's one failure for each chart, things actually work out. Correct. And so the worst case basically is that b two fails.", "metadataJson": "{\"start\":1294,\"end\":1316}"}, {"text": "So here, where is b two? Here's b two. B two fails, and so we lose that commit record that is actually there. The primary might actually have not a commit record yet because it crashed after we received the acknowledgement of one primary. So let's say the p one must actually have to commit record.", "metadataJson": "{\"start\":1318,\"end\":1339}"}, {"text": "So in this case, p one will have to commit record.", "metadataJson": "{\"start\":1340,\"end\":1342}"}, {"text": "And of course the backup has a commit record for b one. This is enough information to convince during recovery that actually the transaction has committed. Because we have a commit record which assess what the tid is that's committed, and we actually have all the information at the backups, namely the commit records which describe the write transformations on the backups during recovery, we actually have enough information to decide that the transaction actually has committed.", "metadataJson": "{\"start\":1345,\"end\":1379}"}, {"text": "And so that's sufficient. Of course, there's a complex protocol that actually needs to go into action and sort of look at all the pieces that are left behind by the transaction. But there are enough pieces left behind for the transaction, for the coordinate, for the sort of new coordinator, the recovery process to decide that action. This transaction indeed has committed and should be persistent. Okay, good.", "metadataJson": "{\"start\":1381,\"end\":1410}"}, {"text": "So let me sort of summarize farm before we jump into the discussion of spark.", "metadataJson": "{\"start\":1411,\"end\":1417}"}, {"text": "So, you know, top level, you know, what is sort of cool about farm is fast. It can execute many, many, many transactions per second. There are some restrictions, of course, on farm, you can't do it all the time.", "metadataJson": "{\"start\":1421,\"end\":1439}"}, {"text": "So first of all, it seems few conflicts. So it uses this optimistic concurrency control scheme. And the reason that it uses this optimistic currency control screen, because it doesn't want to take out logs, because it wants to do this one sided RMB without actually any server involvement. So using optimistic recurrence control scheme, that means that if you want to get good performance and avoid transaction boards, that the workload better actually have few conflicts. And we're seeing evaluation that there are sort of the two common benchmarks that are being used in the transaction literature to measure systems for those two benchmarks clearly is doing extremely well.", "metadataJson": "{\"start\":1442,\"end\":1485}"}, {"text": "Doesn't just mean there are not that many conflicts. The second assumption it makes is that the data must fit in memory.", "metadataJson": "{\"start\":1485,\"end\":1493}"}, {"text": "So it either means that if you have a really, really big database, you have to buy more machines, or if really the data is too big and you don't want to buy more machines, then basically you can't use farm and you have to sort of go back to a more traditional database that actually has persistent storage and so that you can read and write records to a much larger storage device.", "metadataJson": "{\"start\":1498,\"end\":1522}"}, {"text": "Replication is only within the data center.", "metadataJson": "{\"start\":1524,\"end\":1531}"}, {"text": "So in that way, in that way respects, it's quite different from spanner. The whole goal there was to do synchronous transactions across asynchronous replication across data centers to support applications that need to be able to survive or continue. While some data centers are down, in the case of far, this is not the case. It's not targeted to those kinds of applications. And then the final point is it requires pretty fancy or exotic hardware.", "metadataJson": "{\"start\":1536,\"end\":1574}"}, {"text": "In particular, it has two things. This Ups distributed ups to survive complete data center outages, and more importantly, uses this RDMA nics to get actually really high performance.", "metadataJson": "{\"start\":1574,\"end\":1590}"}, {"text": "Okay, that's sort of all I wanted to say about farm, unless there are any further questions.", "metadataJson": "{\"start\":1594,\"end\":1602}"}, {"text": "Okay, so that basically ends our set of papers or the sort of transaction side of 824. So we've sort of done three lectures on transactions, and that's basically, this is the end of talking about transactions that will show up in other papers, but we're not going to talk about them in any more detail. And in fact, we're basically sort of done with talking about sort of the most challenging part in distributed systems, namely building fault hollow storage systems. And we're now seeing a broad spectrum of different designs, including designs that actually support this very powerful programming abstraction of transactions. So in the next, this lecture, or the remaining of this lecture and subsequent lectures, we're going to sort of look at different topics that are unrelated to storage systems.", "metadataJson": "{\"start\":1615,\"end\":1670}"}, {"text": "And so the first topic that we're going to be talking about is sparse.", "metadataJson": "{\"start\":1671,\"end\":1676}"}]}