{"documentId": "lecture14", "section": [{"text": "Good afternoon, good evening, good night, good morning, wherever you are, or whenever you're watching this. So today I want to talk about Spanner. This is a paper from 2012, but that the spanner system is in active use and also still continuously under development. So it's a real system. The main topic, and spanner is really interesting, is it supports wide area transactions.", "metadataJson": "{\"start\":0,\"end\":31}"}, {"text": "This is a really powerful programming model. So even though the data were like maybe the data sharded across multiple servers, and the servers being in different data centers in the different pieces on the planet, you can just run transactions. And they have asset management semantics, they're atomic with respect to failure, that all the writes happen, or none of them, and they provide serializability. And so that's an incredibly powerful programming abstraction. And of course, the challenge is to implement it efficiently.", "metadataJson": "{\"start\":39,\"end\":74}"}, {"text": "And one big challenge is just like a law of physics, the speed of light. Sending a packet from one end in one part of the US to the other part of the US, or to another continent, that takes a lot of time. You know, we're talking about tens of milliseconds. And so what we're going to see in this paper is that read write transactions are actually, are indeed quite expensive, but they work very hard to make read only transactions very inexpensive. And so the read write transactions are implemented using two phase commit, as we talked about last week, two phase locking.", "metadataJson": "{\"start\":74,\"end\":117}"}, {"text": "And one of the things that is interesting about it is that the participants in this protocol are just basically patching groups.", "metadataJson": "{\"start\":117,\"end\":125}"}, {"text": "The read only transactions can execute at any data center, and they are going to run very fast. In fact, if you look at the tables six in the back of the paper, you'll see that read only transactions are like ten times as fast as read write transactions. And there are two key ideas there that we'll talk about in this lecture. One is snapshot isolation, which actually is a standard database idea, but I use it here to make the reads fast. And in particular, you know, to make it actually work well in a distributed wide area setting, they rely on synchronized clocks, or those clocks are not perfectly synchronized, and so their transaction scheme must deal with a little bit of slop or drift or error margin in exactly knowing what the true time is.", "metadataJson": "{\"start\":132,\"end\":188}"}, {"text": "As I said, it's widely used both internally in Google, but also as a Google customer, you can use it. Spanner is basically a cloud service that you can use if you're a Google customer. If you use Gmail, probably your email or parts of the email system actually go through Spanner maybe before diving into more detail, I want to make one logistic comment related to Spenner. As you may already have seen, we made some adjustment for the long weekend, long upcoming weekend. And first of all, lab four a is not as heavy duty as the other labs, so hopefully it should take less time.", "metadataJson": "{\"start\":189,\"end\":238}"}, {"text": "We also canceled next week's lecture so that you can actually use that as time to work on four a. And we made the deadline for four a more flexible. So next Friday is not convenient for you. You can actually choose to, you have another extra late hours to submit it later, and hopefully that will allow you to enjoy the long weekends and perhaps get some sleep and maybe do something else than debugging your labs. The second point I want to make is direct related to spanner.", "metadataJson": "{\"start\":238,\"end\":272}"}, {"text": "Some of you noticed this and came through in the questions. This paper is quite complicated, and there's many reasons why it's complicated, but one reason is that actually there's a lot of things going on. It's a powerful system, has a lot of different components to it, and the interaction between the different components is important. And so there's just a lot, a lot of material in this paper. And so what I will try to do in this lecture is try to make that more clear by focusing on a couple aspects of the paper.", "metadataJson": "{\"start\":273,\"end\":305}"}, {"text": "And I'm not going to do a full treatment of the paper, but focus, I think, what are the most important ideas and why we're reading it in 824, it took me actually, a little bit of time, a few years, to figure out, actually how to present this paper or explain it in a way that I think is maybe easier to understand if you're in the context of 684. Okay. But ask questions as always.", "metadataJson": "{\"start\":305,\"end\":334}"}, {"text": "Okay, so let's dive in and talk a little bit of the high level organization and more from the point of view, from the way I want to talk about this, about spannery in this lecture. And so there's multiple data centers and there's for continuance or simplicity. You know, just think about there are three data centers, a, B and C, and they can be anywhere in the world.", "metadataJson": "{\"start\":336,\"end\":370}"}, {"text": "And the goal is that basically data will be like, we have made data like a chart that contains some database rows or some key value pairs. So we have a shard, maybe it has the keys a to m. And the basic idea is actually, you know, we're going to replicate that shard across data centers.", "metadataJson": "{\"start\":373,\"end\":395}"}, {"text": "And with the goal that, like, even if a complete data center goes down, then, you know, we can kind of proceed. And the way we're going to arrange that we can be able to proceed is that, you know, basically these sharks, they're going to form these replicas that are sitting in different data center. We're going to form one Pax's group.", "metadataJson": "{\"start\":397,\"end\":416}"}, {"text": "So if you're thinking about this or you're trying to think about this in terms of lab three, then you can think about that. We have a key value store where the key value servers are spread around different data centers and the keys are updated by submitting raft writes through the raft log and then the individual keyvs update their state. You can think about lab free being instead of running free KV servers on your machine, you're going to run one KV serve in different data centers.", "metadataJson": "{\"start\":423,\"end\":459}"}, {"text": "Okay, so then there's going to be a Paxos group per chart. So there, you know, there might be other sharks that have all other parts of the value space or database rows. So let's say, you know, we have only two shards, you know, for this particular database, you know, shards containing a to m and then the shard containing n to z and they form their own axis group.", "metadataJson": "{\"start\":462,\"end\":494}"}, {"text": "And the reason we want to get a multiple chart is to get parallelism so that we can, you know, if the transactions involve different charts, you know, disjointed set of charts, you know, they can do. These transactions can actually proceed completely in parallel. So as I mentioned earlier, we have a paxis group per shark for replication. But Pacsus actually provides us an additional sort of benefit. The communication cost from a to b or from a to c might be very expensive.", "metadataJson": "{\"start\":496,\"end\":544}"}, {"text": "And since packages allows the proceeds or raft allows us to proceed with just a majority, the slowest machine might actually have not that much impact. And so we can sort of easily tolerate either slow machines or actually one data center being down. So the majority rule helps us in two ways. We get data center fault tolerance and sort of slowness.", "metadataJson": "{\"start\":545,\"end\":577}"}, {"text": "A final goal, as we'll see in a little bit more detail, is that a client of spanner, so let's say here's some server user. Spanner would like it to be the case that the server can actually use a closed replica. The replicas typically are placed close to the clients that actually use them. In fact, we'll see that read only transactions can be executed basically by the local replica and without any communication to the other data centers. Now when I talk about client here, this is typically the backend service of some Google service.", "metadataJson": "{\"start\":581,\"end\":630}"}, {"text": "So for example, this might be the Gmail server that's sitting also in some data center or maybe in the same data center. And talks to the replicas in that particular data centers, and of course, outside our real clients, like users that read and write email. Okay. Any sort of questions about this high level organization.", "metadataJson": "{\"start\":630,\"end\":653}"}, {"text": "Okay, so let me sort of lay out the challenges that I want to focus on in this lecture. And so there are going to be sort of three main challenges.", "metadataJson": "{\"start\":661,\"end\":675}"}, {"text": "One is the way is that, like I said, we want attribute read only transactions without actually having to communicate with any other server. But we got to make sure that the read sees the latest, right.", "metadataJson": "{\"start\":677,\"end\":690}"}, {"text": "And this is sort of a classic challenge that we've seen before again, in zookeeper. Zookeeper sort of avoid doesn't really address the challenge direct head on and just like weakens the consistency. But here in this design, we'd like to arrange it in a way that actually we still keep linearizability. In fact, the spanner shoot for a strictly stronger property than linearizability. Second thing is when you want to support, Spanner wants to support transactions across shards.", "metadataJson": "{\"start\":707,\"end\":736}"}, {"text": "So even transactions like we do, a bank transfer and one account is in one chart, the other account, the destination count, is another shard. We want to arrange that it still can be executed like a transaction and have asset semantics. Finally, the transactions, both the read only ones and read write ones, must be serializable. In fact, a little bit stronger than serializable.", "metadataJson": "{\"start\":741,\"end\":768}"}, {"text": "And we'll see that for this. Basically for the read write transactions, we're going to use two phase locking, two phase commit, and basically the two protocols that we talked about in the last lecture. What I'd like to do first is talk about read write transactions and then talk in more detail how reader only transactions are executed so that they can run very efficiently.", "metadataJson": "{\"start\":773,\"end\":804}"}, {"text": "Okay, so read write transactions are basically two phase locking and two phase commit. So hopefully this is going to be easy to understand.", "metadataJson": "{\"start\":807,\"end\":817}"}, {"text": "And then they're going to follow these sort of complex timing diagrams that we looked at last week. And so the way it's set up, and I'm going to simplify a bit, but the way it's sort of set up in Spanner is we have the client, and the client is sort of in charge of really running the transaction, uses a transaction manager or transaction library that actually runs on the client machine and is in charge of basically orchestrating the. And again, the client here is not the user web browser for Gmail, but it's basically the servers or the server on the Gmail server in the data center that is the client or spanner. Let's make the picture reasonably simple. So we're going to have two shards instead of three or five.", "metadataJson": "{\"start\":822,\"end\":874}"}, {"text": "So we have shard a and we have shard b. And let's assume we're going to just execute the transfer transaction, the same one as before, where we're going to deduct some money from one account and add to another account. And initially, I'm going to talk about this without the timestamps.", "metadataJson": "{\"start\":874,\"end\":891}"}, {"text": "In some ways, one reason to do so is that actually for the read write transaction, timestamps are not very important. The timestamps actually are mostly there for read only transactions, and they need a little bit tweaking to the read write transactions to support the read only transactions. And therefore the timestamps drift into read write transactions, too. But in essence, the read write transactions are just basically straight two phase locking and two phase commit. So the client's going to read these accounts.", "metadataJson": "{\"start\":897,\"end\":928}"}, {"text": "So for example, let's assume that read X. So it's going to do the transfer transaction from moving money from adding one to X and subtracting, subtracting $1 from X and adding a dollar to Y. So it's going to read X. And let's assume that X sits in one chart. So this is going to be a cross chart transaction.", "metadataJson": "{\"start\":928,\"end\":951}"}, {"text": "And here's the read of Y. And there's going to be, when the client executes these or issues these read operations, they go to the shards, and the shards actually keep an eye lock table. And so they record basically this transaction, you know, so let's say this is transaction whatever, TID, and they're going to record that the X is owned, you know, it's the lock table. X is owned by the client, you know, and here in this case, Y is owned by the client. And that's sort of the very standard, you know, we've seen before.", "metadataJson": "{\"start\":952,\"end\":991}"}, {"text": "Now, the thing that is slightly different here, and I'm not fully drawing it out here, is that when we're talking into one shard a Shard A is really one of these package groups. And so it has free, in this case, a group of free peers. And so SA is really a replicated surface consisting of multiple peers. When executing a read only transaction, we're going to be talking to the leader of that peer. So if you think in terms of raft style, you can just think about it.", "metadataJson": "{\"start\":992,\"end\":1026}"}, {"text": "The read request goes to the leader of the Paxos group. And another. So now, every time I sort of draw this one single arrow here for SA or for SB, you know, it's a much more complicated story, particularly when writes get involved, because writes actually are going to go through the Paxos or raft group.", "metadataJson": "{\"start\":1026,\"end\":1050}"}, {"text": "The read only or the lock table is actually not replicated, it is just stored at the leader of the faxer's group. And if the leader goes down during the transaction, then basically the transaction has to be restarted or the transaction will be awarded because the locking information gets lost. The reason that these lock table is not, is not replicated is to just make read operations fast. Okay, so once, you know, the client actually has gotten the value of an x and y and has taken out the walks in sort of two phase locking style, the, you know, it's going to subtract one from x, add one to y and then basically going to submit a transaction. So basically all the writes are done locally at the client.", "metadataJson": "{\"start\":1053,\"end\":1110}"}, {"text": "Again, sort of, this is the Gmail server. And when everything is, when the client is done with the transaction, it submits transaction to spanner and it submits it to a transaction coordinator.", "metadataJson": "{\"start\":1110,\"end\":1126}"}, {"text": "So some, you know, set of servers or some machine is being picked as the transaction coordinator. And again, the transaction coordinator is a taxes group. So it's multiple peers in different data centers. And one reason, Greg, that we want this to be a taxis group is so that as we've seen before, in two phase protocol, over the two phase commit, that if the coordinator fails, it might actually block the participants. If the participants have pre prepared and agree to go along with the transaction, but then the coordinator fails, these participants have to hold on to their locks and have to wait until the coordinator comes back.", "metadataJson": "{\"start\":1128,\"end\":1170}"}, {"text": "By replicating the coordinator, we're using Paxos, we make the coordinator highly available and so basically avoid that particular sort of disaster scenario. Okay, so the transaction coordinator is then basically in charge of running the two phase commit protocol. And so it will send the updates for x and y to the leader of shard a, x and y to the leader of shard b, they grab the logs. In this case they already hold the locks or promote the locks to read, write logs and basically prepare the transaction. So they don't really execute it yet, but just make the typical using right ahead logging, prepare the changes.", "metadataJson": "{\"start\":1170,\"end\":1223}"}, {"text": "And if everything is okay, then they basically commit to the transaction by entering this prepared state.", "metadataJson": "{\"start\":1223,\"end\":1232}"}, {"text": "And this is sort of a big moment because at this point the transaction is or the peers or the participants are committing to this transaction. And we know from last lecture, the participants do actually have to record some state so that if they fail and they come back up, they can recover and pick up from where they left off. So at the prepared state, basically this results into a pax's right, you know, recording the state of the transaction and the two pc state, the locks that the participant is holding, et cetera, et cetera. And so this packs, right. So the leader of this particular shard is going to push packs right across the different peers in the group to ensure that that state is replicated and fault tolerant.", "metadataJson": "{\"start\":1235,\"end\":1292}"}, {"text": "And so once the participants have prepared and agreed to prepare, then they send back, okay, okay, so this is very similar to our two phase protocol that we talked before. And at this point, the coordinator can commit.", "metadataJson": "{\"start\":1294,\"end\":1312}"}, {"text": "And of course, at the point of the commit, the coordinator needs to record that it actually made the commit decision, because when the participants may come back later and want to know, find out about it, and we might have failures. So again, the Paxis state or the pc, the two phase commit state, is written using paxos and replicated using Paxos. And for this whole presentation, you can just think about Paxos as being complete substitute or equivalent to raft.", "metadataJson": "{\"start\":1315,\"end\":1345}"}, {"text": "Spanner predates raft, but conceptually, and through first order for this paper, they're basically the same.", "metadataJson": "{\"start\":1347,\"end\":1355}"}, {"text": "So at this point, the transaction commits, and this is really the commit point. Once the transaction coordinator has written down, the transaction is committed, that is the actual commit point, and then it informs the participants that has happened, and you know, they're going to respond back with, okay, great, transaction is committed, and the transaction coordinator can clean up its state. And at some point later, the shards can also clean up their state. And at the point of commit, the participants at least release their locks.", "metadataJson": "{\"start\":1358,\"end\":1395}"}, {"text": "Okay, so that is sort of the basic story for read write transactions. Any questions about this? I think the simple way to think about it is this is two phase commit. Two phase locking. With the main difference between what we talked about last week and this week is that the participant, the transaction coordinator, the participants are all patches groups.", "metadataJson": "{\"start\":1405,\"end\":1430}"}, {"text": "They're replicated, they're highly available. And some of the problems that we talked about, the two phase commit last time are less relevant here because the participants are much more highly available. So each shard is replicating the lock table or no? Yeah, well, it's not replicating the lock table. It's replicating the lock that's holding when it does the PPE.", "metadataJson": "{\"start\":1430,\"end\":1456}"}, {"text": "So only the lock when it's. That it's holding during the prepare? Yeah. The state that it needs to do, the two phase commit.", "metadataJson": "{\"start\":1456,\"end\":1463}"}, {"text": "So when they like the current locks for some transactions and hasn't reached the prepared stage, it will. They just be lost. They'll be lost and then the transaction will abort. The participant would just not participate and tell the coordinator, hey, I lost my locks. Can't do it.", "metadataJson": "{\"start\":1466,\"end\":1485}"}, {"text": "Thank you. Okay, so now the rest. Basically, this lecture is about the read only interjections. And so these interactions, they only do reads, no writes, and they are common. And so one of the goal is to actually make them very fast.", "metadataJson": "{\"start\":1487,\"end\":1514}"}, {"text": "And the way they're going to, the way they achieve, you know, the high performance is they're going to arrange that the reads spanner makes them high performance because reads are only from local shards and they have no locks. And no locks is good because that means that read write transactions can't block and read transactions. Or another way of saying is that read only transactions can't block read write transactions. And no two phase commit. And so that means also no wide area communication necessary.", "metadataJson": "{\"start\":1518,\"end\":1559}"}, {"text": "And so that the reads can definitely execute from a local replica. And you also see, of course, this is the read from a local chart. The real challenge here is how to still get consistency or serializability. But it's important to point out that's for no moment assume that we know how to do this. But if we could then really follow local charts, holding no logs, not doing two phase commit, means that, like, all the communication is local within that, within that particular data center.", "metadataJson": "{\"start\":1559,\"end\":1595}"}, {"text": "And it can be very fast. And if you look at the end in tables three and six in the paper, you basically see that they basically read only transactions are ten times faster than read writing transactions. Read write transactions are in the order of hundreds of milliseconds, which sort of makes sense because they have to communicate long distance. But the read only transactions are in the order of five to ten milliseconds.", "metadataJson": "{\"start\":1595,\"end\":1630}"}, {"text": "Okay? So the key challenge, of course, is always going to be with executing from local replica how to get correctness. So let me talk a little bit about it, because it's slightly different than what we seen before. So, correctness means two things here. One, the transactions are serializable, so they must execute in some serial order.", "metadataJson": "{\"start\":1632,\"end\":1655}"}, {"text": "So if we think about it like, we have a read write transaction, and we have another read write transaction, and we have a read only transaction. Now, the read only transaction has to sort of fit between the two read write transactions. And read only transactions should not observe some part of the read write transaction. So the read write transaction does multiple writes like the first one. Then the read only transaction, you know, sees all those writes or none of them.", "metadataJson": "{\"start\":1655,\"end\":1682}"}, {"text": "Similarly, for the one that actually is passed. Okay, so the second. So this is a standard thing that we talked about last week's serializability. And then when they go for something stronger, what they call external consistency.", "metadataJson": "{\"start\":1683,\"end\":1697}"}, {"text": "And external consistency means that if a transaction two starts after transaction one has committed, then, you know, t two must see t one's, right?", "metadataJson": "{\"start\":1704,\"end\":1723}"}, {"text": "So we'll go back into this previous picture like, so if this read only transaction, this one started after read the first transaction committed, then this read only transaction must see the reads and the rights of that transaction, t one. And so the idea, if you think about this, basically, this sort of, sort of external consistency means like serializability plus this real time requirement. And in fact, you know, it is very similar to linearizability.", "metadataJson": "{\"start\":1728,\"end\":1760}"}, {"text": "And except, you know, the one way to contrast external consistency for transactions with linearizability is that external consistency is really transaction level property. And the way we've been talking about linearizability so far, it always has been sort of on individual regional rights and to the first order, I think you can sort of think about it exactly in the same way. And like linearizability, you know, external consistent users, pleasant for programmers and is very strong. Consistency require property. Okay, any questions about the correctness definition there or the correctness goal that the spanner has?", "metadataJson": "{\"start\":1769,\"end\":1812}"}, {"text": "Okay, let's then talk about how they actually achieve this correctness for read only transactions. And let me start out by explaining a back plan that actually doesn't work. And then we're going to talk about a better plan. So the back plan is we're going to read an arrange that we read always the latest committed value.", "metadataJson": "{\"start\":1817,\"end\":1840}"}, {"text": "That seems about right, because we have to arrange that if t two starts, after t one committed, we see it, you have to see it, right? So why not read the latest committed value and then we should be, maybe we were good. So here, the problem case, of course, doesn't actually work. So t one name it. T one does the write of x does the right of y as the transfer commits.", "metadataJson": "{\"start\":1843,\"end\":1873}"}, {"text": "Then, you know, we have another t transaction. We have transaction t three where t three is the read only action. It actually does a read of x. So this is sort of real time. And then after t two, then after t one or t three starts, t two runs, it runs correctly.", "metadataJson": "{\"start\":1873,\"end\":1892}"}, {"text": "Reverse the right of x, the right of y, it commits, and then t four u got a little bit delayed. And then actually it does the second out of read and read y sort of time, you know, as usual, going that way.", "metadataJson": "{\"start\":1892,\"end\":1906}"}, {"text": "And you know, you're following the reads the latest committed value. Then, you know, what is this read going to return? It's going to return the value from this transaction, t one. And this read is going to return the value from y from that direction because, you know, that's the line is a committed value. And that would be wrong, right?", "metadataJson": "{\"start\":1910,\"end\":1931}"}, {"text": "Because now we're actually in a situation where basically, you know, t freeze, observes, writes, you know, from different transactions and not get a consistent picture. So this rule is not good enough. And to avoid this problem and not use this bad plan, Spenner uses a different plan. And that plan is called snapshot isolation.", "metadataJson": "{\"start\":1939,\"end\":1965}"}, {"text": "This is actually just standard database idea and mostly sort of in the local databases and not actually across the wide area. And so we'll talk about that in sort of the white area aspect in a Second. But just let me first explain what snapstroke isolation is. And so what Snapstrack isolation does is we're going to assign a timestamp to a transaction.", "metadataJson": "{\"start\":1973,\"end\":2004}"}, {"text": "And, you know, there's two different points where we're going to assign these Timestamps. For read write transactions, it's going to be, you know, the commit, start of the commit, and for read only transactions, it's just going to be the start of the transaction.", "metadataJson": "{\"start\":2006,\"end\":2026}"}, {"text": "And then we're going to execute all operations of the transaction in timestamp order.", "metadataJson": "{\"start\":2028,\"end\":2039}"}, {"text": "I'll explain in a second what I mean with that and be able to execute all the operations in timestamp order. Now, each replica doesn't store one value for a particular key, but replica actually stores multiple values for a key, namely with their timestamp.", "metadataJson": "{\"start\":2042,\"end\":2061}"}, {"text": "So, for example, at a particular replica, we can ask them, please give me the value of x at time ten, or give me the value of timestamp x at 20. And so sometimes this is called multi versioned databases or multi version storage. Basically, for every update, you basically keep a version of the data item so that you can go back in time. And so this fixes, you know, the problem that I sort of showed in the first case, because basically what's going to happen, let's look at the free transaction again. So we got t one, we got t two, we got t three, t one.", "metadataJson": "{\"start\":2074,\"end\":2113}"}, {"text": "There's right of one. You know, there's a right of right of x, write of y, there's commit. And let's say the commit actually happens at, you know, you know, ten. So this transaction basically runs at timestamp ten. Then, you know, at some point we're going to get our read of x.", "metadataJson": "{\"start\":2113,\"end\":2133}"}, {"text": "I'll talk about that in a second. And then here we have this other transaction. We get the right of y, the right of sort of x, right, of Y. The commit, let's say this transaction commits at timestamp 20. So this run basically at that timestamp.", "metadataJson": "{\"start\":2133,\"end\":2153}"}, {"text": "And then we have the read of x and we have the reader y. Now, when the read of x happens, it's going to be assigned a time step, the starting time of the time of the transaction. So the starting times, let's say that the starting time of this transaction is 15. So t two runs at 15. And so when the read of x is executed, you know, it needs basically to read the latest value of x before 15.", "metadataJson": "{\"start\":2153,\"end\":2185}"}, {"text": "And we do that, the latest committed value for timestamp 15 before timestamp 15 is going to be the values from this transaction. So read x reads the value of transaction one. But of course, read y also will execute at the timestamp of the start of the transaction. So it's going to also be y at the time 15. And there's going to be only one value for that y 15.", "metadataJson": "{\"start\":2185,\"end\":2214}"}, {"text": "Namely that is the one produced by the transaction t one. And so read one, the read y will also read from td one. And so we avoid this problem that we had before where we reading from different transactions. And so this gives us the linearizability or the serializability that we'll be looking for because all the transactions are executed in a global timestamp order.", "metadataJson": "{\"start\":2214,\"end\":2239}"}, {"text": "Does that make sense? So what you can think about is that every replica basically keeps a table of values and timestamps. And so we're going to have like x at value nine at ten and x at value eight at 20. And so when the read comes in at a particular replica, the read for 15 comes in, I could just pick out the latest write preceding its type stamp.", "metadataJson": "{\"start\":2242,\"end\":2274}"}, {"text": "So I have a question. So when we do the read x, let's say. So let's just focus on read x. So X itself, it exists on some shard which is replicated on a Paxos group, which, let's say there are like three servers that replicate x. And when you read from x, because we want to read only transactions to be very fast, we just read from the local replica, which may not necessarily be the leader.", "metadataJson": "{\"start\":2276,\"end\":2301}"}, {"text": "So, like, how do we guarantee that we don't read a stale. How do we make a stale read? Brilliant question. And that's exactly the topic I wanted to talk about next, because the problem is, as you pointed out, there's a challenge that maybe the replica hasn't seen.", "metadataJson": "{\"start\":2302,\"end\":2321}"}, {"text": "Hasn't seen the right to x right at timestamp, whatever, ten. Right.", "metadataJson": "{\"start\":2325,\"end\":2332}"}, {"text": "And so the way this problem is solved in spanner, the solution they call something that's called save time.", "metadataJson": "{\"start\":2335,\"end\":2343}"}, {"text": "So, the way this is resolved is that basically, Paxus, or raft, sends all writes also in timestamp order.", "metadataJson": "{\"start\":2350,\"end\":2361}"}, {"text": "So there's not a. You know, you can think about, like, the total order is not a counter, as you should, for example, have done maybe in lab three, but it actually is literally a timestamp. And since the timestamps also form global order, you know, that global order of timestamps, you know, sufficient sort of, to order all the writes. And then there's a rule. There's an additional rule for a read before you can do a read.", "metadataJson": "{\"start\":2365,\"end\":2391}"}, {"text": "So before read of x at, you know, timestamp 15, the replica has to wait for. Right. That is, with timestamp bigger than 15, this Caesar write with a timestamp bigger than 15, it knows that there's certainly no writes anymore before 15. And so, therefore, it's safe to execute the read at timestamp 15 and know what value actually needs to be returned. And so, for services that are.", "metadataJson": "{\"start\":2392,\"end\":2428}"}, {"text": "So this means that the read actually may have to get delayed a little bit until the next write. Now, of course, for busy services, these writes will come along all the time. And so the wait is probably nonexistent or almost non existent. Okay, but this is the rule that needs to be followed to make sure that, indeed, this problem of replica actually have not seen the right yet. Returning the wrong value, the rule is slightly more complicated.", "metadataJson": "{\"start\":2428,\"end\":2454}"}, {"text": "You also have to wait. Also wait for transactions that have prepared but not committed.", "metadataJson": "{\"start\":2455,\"end\":2460}"}, {"text": "For example, transaction might have been prepared basically at, you know, say, a timestamp 14, but it hasn't maybe committed it right yet, you know, to the key value store. And so we got to make sure that, you know, any transaction that was prepared before our read, timestamp that actually commits before we actually return the value of the read. Okay, does this make sense? Would this also be the case for different shards, or do we consider different shards just separately?", "metadataJson": "{\"start\":2479,\"end\":2513}"}, {"text": "The reachers hit a local shard, the local replica. And so I'm not 100% sure what the question you ask me. I think that. I think the question I'm asking is the correctness guarantees. Do they apply across shards?", "metadataJson": "{\"start\":2516,\"end\":2535}"}, {"text": "Yes, they apply to the level of transactions. Right. So, if a read only reads a local replica, we still have to make sure that the transactions are externally consistent, and by following these rules, we achieve that goal. Okay. Makes sense.", "metadataJson": "{\"start\":2536,\"end\":2553}"}, {"text": "Thank you.", "metadataJson": "{\"start\":2553,\"end\":2553}"}, {"text": "Okay, now, you know, we're sort of getting to sort of the core part of the spanner paper, which is really, you know, to be, we want to reason about time, like timestamps in this case. And, you know, the clocks, you know, of the different servers must be, the clocks must be good, it must be perfect. People must, different participants must agree on the timestamp order. And if a transaction takes a particular timestamp, that timestamp must be the same timestamp everywhere in the system.", "metadataJson": "{\"start\":2557,\"end\":2593}"}, {"text": "The way I've described it in this previous slide slide is that, you know, whatever some participant picks, the retransaction assigns a timestamp to it, like 15, which will maybe go back a little bit, you know, here. You know, we're just assigning timestamps to these transactions, and it better be the case that actually t one and t two and t three agree on these timestamps and that they're comparable. And so, as we'll see in a second, this only matters really for read only transactions.", "metadataJson": "{\"start\":2595,\"end\":2630}"}, {"text": "And we didn't consider sort of the two cases, like, so what happens here? So the question that we want to ask is what happens if, like one replica or like one server just asks the wrong type and so it doesn't agree with the time at the other surface? What kind of problems could it introduce? So let's first think about the case. What, the timestamp is too large.", "metadataJson": "{\"start\":2645,\"end\":2666}"}, {"text": "So, for example, let's go back to our version here with this case. So let's say the read only transaction starts reading and it reads actually instead of, you know, 15, maybe whatever the value of returns actually is, 25.", "metadataJson": "{\"start\":2673,\"end\":2693}"}, {"text": "And what would I do? Let me make it a little simpler, you know, 18 to get less confusion. You know, what would, what is the outcome of actually having a timestamp that is off but off in the direction of being too large?", "metadataJson": "{\"start\":2695,\"end\":2712}"}, {"text": "If it's still less than 25, doesn't it still read? Or than 20, doesn't it still read from the first one? But if it's greater, it'll read from the second one. Yeah, which either one is fine. Correct.", "metadataJson": "{\"start\":2715,\"end\":2727}"}, {"text": "The real key issue here is that before reading, remember, you have to wait until you see your. Right, right. So if you're. So what happens if your timestamp is too large or off on the too large side? You have to wait for.", "metadataJson": "{\"start\":2728,\"end\":2744}"}, {"text": "Exactly. You have to wait a little bit longer, maybe, but nothing goes wrong.", "metadataJson": "{\"start\":2744,\"end\":2749}"}, {"text": "So now the other question is, what if the timestamp is too small?", "metadataJson": "{\"start\":2755,\"end\":2760}"}, {"text": "So, for example, the read, when the t three, the machine that executes t three, asks for the time instead of time ten, it actually gets back, say nine. And this is sort of a variation of the lecture question. So basically like t three runs at nine. And maybe this is a good time for actually to take a breakout room and you can think and argue what, you know, what is the outcome, what is the potential outcomes, or what could go wrong if actually the timestamp that got assigned at t three is nine instead of ten.", "metadataJson": "{\"start\":2763,\"end\":2800}"}, {"text": "So maybe we can do a breakout here, Lily, would have been possible.", "metadataJson": "{\"start\":2803,\"end\":2806}"}, {"text": "Ok, I think Lilly is back.", "metadataJson": "{\"start\":2809,\"end\":2810}"}, {"text": "Hold on a second here.", "metadataJson": "{\"start\":2819,\"end\":2820}"}, {"text": "Okay, anybody else getting the noise?", "metadataJson": "{\"start\":2838,\"end\":2841}"}, {"text": "Okay, it okay, everybody backing. Everybody can hear me again? Yep, we're all good.", "metadataJson": "{\"start\":2845,\"end\":3232}"}, {"text": "Assume we're all good. Yes, yes, we're good. We're all good. Okay. Okay, so back, you know, sort of this question here we're investigating, we're relying on scheme where the clocks of different machines are perfectly synchronized.", "metadataJson": "{\"start\":3237,\"end\":3256}"}, {"text": "As I mentioned, matters only for read only transactions, because read write transactions grab logs and use two phase locking to get a total order. So they will execute in some serializable, external, consistent order. Where we see that for read only transactions, the timestamps crucial, at least we think it's crucial for if they're too large, you know, that maybe just affect performance negatively. But the question is what happens if the timestamp is too small? And so the particular question we're asking is like what would happen if in the scenario that we looked at here at a second where we have transactions, t one, actually, I wrote this wrong.", "metadataJson": "{\"start\":3257,\"end\":3302}"}, {"text": "T two and t three, where t three is doing the read and this read instead of actually happening, maybe at 15, it actually happens because the clock gets free wrong. It actually happens at nine. And how bad would that be?", "metadataJson": "{\"start\":3302,\"end\":3322}"}, {"text": "Break our serializability. Anybody? Okay, okay, sorry, I didn't hear you because I actually had some problem. I said I can hear you, I think. Can you repeat the answer?", "metadataJson": "{\"start\":3327,\"end\":3342}"}, {"text": "Sorry. I said it would break serializability. Yeah. Why? Because then your read would happen, which you assume to be happening after your latest write would then happen before, which wouldn't yield the right.", "metadataJson": "{\"start\":3342,\"end\":3357}"}, {"text": "Yeah, so just make this story complete. T three really is executing at time 15, which is after t two. T one committed. So t three must see t one's right. But if the clock is wrong and timestamp nine got assigned to t three, then t three will actually read the value of x before transaction t one executed.", "metadataJson": "{\"start\":3357,\"end\":3385}"}, {"text": "And that'd be wrong. That will break external consistency.", "metadataJson": "{\"start\":3385,\"end\":3390}"}, {"text": "Okay, everybody clear on that?", "metadataJson": "{\"start\":3392,\"end\":3395}"}, {"text": "Okay, so clearly we'd like to avoid this.", "metadataJson": "{\"start\":3410,\"end\":3413}"}, {"text": "And so the central problem that analyzed this is how do we get clocks. How do we keep clocks synchronized? Oh, wait, I just have one more question about. So this scenario basically can happen because it's always the coordinator for read write transaction that assigns the time step. So even if the read is local and it happens on, so it's possible for the machine that was running behind to have a version that's in the future of its local clock.", "metadataJson": "{\"start\":3416,\"end\":3450}"}, {"text": "Yeah. Or in the past. In the future. Yeah. Okay.", "metadataJson": "{\"start\":3450,\"end\":3455}"}, {"text": "And that can happen because it's not that machine who decided the timestamp. No. Okay, so forget about which machine. It is. Like t three, uh, is going to decide on the timestamp for its transaction.", "metadataJson": "{\"start\":3455,\"end\":3467}"}, {"text": "Right. If we go back here to this picture. So here's we got t three. Uh, so t three starts, and t three starts in principle, in absolute time, in true time, after ten. Right?", "metadataJson": "{\"start\":3467,\"end\":3483}"}, {"text": "Because t three starts after t one, as we can see from this picture. Like, so this particular read x, you know, so we sign a timestamp to a read only transaction. We start to sign the timestamp for the read only transaction at the point of the start of the transaction. So the read of the x is the start of the transaction for t three. T three basically needs to get the timestamp.", "metadataJson": "{\"start\":3483,\"end\":3504}"}, {"text": "So it asks, please give me the current value of its clock. And, you know, if, you know, the clock is reliable and correct, you know, we'll give back some time after ten for sure. Right. Because t three started after ten. But let's, you know, we're hypothesizing that the clock of t three is not, we are synchronized and the clock actually returned nine.", "metadataJson": "{\"start\":3504,\"end\":3528}"}, {"text": "And so now transaction three will execute at timestamp nine, and that will cause us to read the value from before t one, and that will break external consistency. Okay, but if the replica on which we execute t three had the, it may have the version ten of variable x, right? It might, it might, but, you know, it's executing it at time nine, and so it will get the value from before ten. Right. This is version memory?", "metadataJson": "{\"start\":3529,\"end\":3572}"}, {"text": "Yeah, yeah, yeah, yeah. Just making sure. This is a very good question. So I think these are very important points, crystal clear what's going on here. Okay, so I think we're now hopefully all on the same page.", "metadataJson": "{\"start\":3572,\"end\":3586}"}, {"text": "You know, it's very important that these clocks during these different machines are pretty perfectly synchronized. And of course, you know, it's not possible to get perfect clock synchronization. And so a couple of difficulties.", "metadataJson": "{\"start\":3586,\"end\":3598}"}, {"text": "It's difficult to get clock synchronization because clocks naturally drift.", "metadataJson": "{\"start\":3601,\"end\":3606}"}, {"text": "And so, you know, when you think it's, you know, whatever, 10:00 p.m. And plus one microseconds. You know, my machine might think it's, you know, 10:00 p.m. And two plus two microseconds. And that just basically because, you know, they're sort of, you know, they're in your machine or in the servers that are oscillators, you know, that sort of keep track of time.", "metadataJson": "{\"start\":3609,\"end\":3628}"}, {"text": "They are supposed to run at a particular frequency, but the frequency, you know, is not perfect. And better clocks, atom clocks have better oscillators that are much more precise. And that's the kind of clocks that spanner is sort of relying on that are pretty high precision, but they need to sort of synchronize once in a while with common global time. So to avoid the problem of drift, they have pretty precise clocks. They use atomic clocks, which are more precise than, like, the clocks in your computer, and then they synchronize with global time.", "metadataJson": "{\"start\":3628,\"end\":3671}"}, {"text": "So to make sure that, like, all clocks agree on global time. And then they sort of keep ticking, ticking, ticking, and then resynchronize periodically synchronized clocks and with global time. And they use GPF, the global position system that broadcast time, as the way to synchronize these different atom clocks and then keep them running in sync. And so it looks like the paper doesn't really say too much about actually how the true time system works, but it looks like they have maybe a few or one atomic clock per data center. The servers synchronize without time server to regularly synchronize their local clocks with that time master and the different time masters in the different data centers.", "metadataJson": "{\"start\":3673,\"end\":3724}"}, {"text": "You know, synchronize. Synchronize themselves through the GPS system. But as a result, the clocks are actually on. The different servers are pretty close in terms of they talk about the epsilon and what the error rate is. And so it seems like the epsilon for their clocks.", "metadataJson": "{\"start\":3724,\"end\":3744}"}, {"text": "If you look at one of the tables in the end of the paper, is in the order of a few microseconds to a few milliseconds.", "metadataJson": "{\"start\":3745,\"end\":3752}"}, {"text": "So when a machine reach, asks the operating system, please give me what the current time is. The current time gets returned maybe a few microseconds off from true time or even a few milliseconds off from true time.", "metadataJson": "{\"start\":3755,\"end\":3770}"}, {"text": "Okay, so this is a little bit of a bubble. Yeah, go ahead. Before I. Yeah. So, like the paper, I mean, as you mentioned, maybe did not really go in depth into that, but just like, the process of synchronizing the clocks or even measuring how different they are.", "metadataJson": "{\"start\":3773,\"end\":3789}"}, {"text": "I mean, shouldn't we also account for the time of the message travel? I mean, yeah, yeah, I think that's what they mean. Sorry, I didn't say much about this, but this is what they mean. So they sort of keep a running estimate about what they think. You know, the.", "metadataJson": "{\"start\":3789,\"end\":3802}"}, {"text": "For example, to synchronize with the time master, the, presumably the time library on the local machine keeps track of the start to make an estimate of what is the average delay or the normal delay for sending or receiving a message to the time master, and basically uses that to correct for any small mistakes. Then the protocol clearly have support for outliers, and so we'll ignore outliers. Maybe something bad happened in the network, and therefore your timestamp got delayed a lot, and so you should not include those. And I guess there's a third problem, which is that sometimes these oscillators go kaput. They just are not correct anymore.", "metadataJson": "{\"start\":3802,\"end\":3845}"}, {"text": "And so they may return incorrect values. And so, again, they don't talk about in great amount of detail, but it seems, yes, they do use similar techniques like NTP to deal with those kinds of problems. Thank you.", "metadataJson": "{\"start\":3845,\"end\":3860}"}, {"text": "So if you're interested in this kind of stuff, there's this protocol called NTP that actually your computer undoubtedly uses to actually synchronize its clock with global time. And NTP has all these kind of mechanisms built into it. But NTP doesn't have the same precision or the same small margins that actually true time has. And so I think in NTP, you know, you should be thinking that these error rates are in the order of, you know, milliseconds to ten milliseconds. And basically that has to do mostly with the round trip time.", "metadataJson": "{\"start\":3863,\"end\":3898}"}, {"text": "Any further questions about this?", "metadataJson": "{\"start\":3902,\"end\":3904}"}, {"text": "Okay, so clocks are not perfectly synchronized. We know that basically there's a margin of error. And what basically the true time does is actually giving, when true time gives an answer, gives you best estimate or guess of what the current absolute time or true time is, plus what the machine thinks is the margin of error.", "metadataJson": "{\"start\":3907,\"end\":3929}"}, {"text": "And so the solution, or to solve, to deal with this clock drift, is to not use time stamps, true time, or just, just pure timestamps, but basically timestamps are intervals.", "metadataJson": "{\"start\":3934,\"end\":3950}"}, {"text": "And so every value returned from now, like from the current time, basically has earliest and latest.", "metadataJson": "{\"start\":3960,\"end\":3972}"}, {"text": "And so, for example, we asked for the current time, and it is true time is 10:00 a.m. Or 02:00 p.m. It might return an interval, saying, well, the earliest it could be is 01:00 p.m. 159 in 59 seconds and whatever, 20 microseconds and latest might be 02:00 p.m. And plus two microseconds.", "metadataJson": "{\"start\":3975,\"end\":4004}"}, {"text": "Some machines might be more than a couple microseconds and some machines might be even a millisecond. And in case, in some cases the margin, sometimes in the order of ten milliseconds or multiple milliseconds, but it gives them an interval and it's guaranteed that the true time is within that interval. So is the interval epsilon or two? Epsilon.", "metadataJson": "{\"start\":4005,\"end\":4027}"}, {"text": "The paper doesn't really talk about it in that sense. It is just an estimate about what the margin of error is.", "metadataJson": "{\"start\":4031,\"end\":4037}"}, {"text": "And I think if you look at the details of the protocol, often that margin of error occur boils down to two epsilon.", "metadataJson": "{\"start\":4042,\"end\":4048}"}, {"text": "Okay, now to deal. So now we need to adjust our protocols, right? Because our protocols have some rules about setting the start time for a timestamp or setting the start time for a transaction using a timestamp and a rule for, you know, reach busting to wait until they see the next right. And we'll see that basically there's a couple rules that need to be changed to deal with intervals as opposed to with a true time. So first of all, the start rule is different.", "metadataJson": "{\"start\":4052,\"end\":4091}"}, {"text": "So the start rule is the current time. You know, we ask the computer, please tell me what the current time is, gets an interval back, and then the start time that we pick is the latest.", "metadataJson": "{\"start\":4095,\"end\":4109}"}, {"text": "And so that just means, correct, that the whatever timestamp the start rule actually assigns is guaranteed to be after true time. So we know that true time is in the past. And for read only transactions this is assigned to the start of the transaction.", "metadataJson": "{\"start\":4112,\"end\":4130}"}, {"text": "And for read writes and says before it actually is at the point that the commit starts. So that part doesn't change. The only part that really changes is that you get the end point of the interval. And the reason you get the end point of the interval is so that at least true time has passed, has passed. Then there's a second rule that we didn't have before, which is the commit wait rule.", "metadataJson": "{\"start\":4133,\"end\":4160}"}, {"text": "And we're going to delay a transaction. So if the transaction got some timestamp at the commit time, the start of the commit, and then we get to the end of the commit, then we're going to delay that commit delay commit until the timestamp that was assigned at the starting of the commitment is past. Hold on, let me run my notes around here. Hold on 1 second.", "metadataJson": "{\"start\":4161,\"end\":4193}"}, {"text": "We're going to delay until the timestamp is before now earliest.", "metadataJson": "{\"start\":4211,\"end\":4220}"}, {"text": "So we know that it actually is definitely before true time.", "metadataJson": "{\"start\":4226,\"end\":4229}"}, {"text": "Okay, so, so that's sort of the modifications to the protocol. And let's see sort of how that works with a simple example to get a little more feel for it.", "metadataJson": "{\"start\":4232,\"end\":4242}"}, {"text": "So an example I'm going to use is slightly simpler than the previous one. I'm just going to focus on the transaction that just writes to x and we're going to still have free transactions. So here's t one. And t one does write to x and then commits. And you just, you know, let's, you know, we don't really care too much about these transactions.", "metadataJson": "{\"start\":4248,\"end\":4269}"}, {"text": "So let's see that it commits at one. So two time at one. So now we're going to run transaction 22 and it's going to write. So the transaction two, two, of course runs after t one. And it writes, you know, say x two.", "metadataJson": "{\"start\":4269,\"end\":4288}"}, {"text": "So here writes x one. It writes two to x. It starts to prepare, which is the beginning of the commit. And so at the beginning of the commit it's going to ask for a time and so it's going to get an interval back. And we know that the true time that's going to get back is some will lie in that interval, but somewhere in that interval.", "metadataJson": "{\"start\":4289,\"end\":4314}"}, {"text": "And so the interval, in fact, might start well before true time. Let's say the intervals start to even have one, which overlaps with transaction t one. And maybe the latest value of true time is of the intervals ten. So that's what it gets back. And what we're going to pick as the time stamp is we're going to pick this value ten and we want to pick the latest value because we want to make absolutely sure that if there's a transaction that started before the true time, that we pick a time that is definitely after the true time.", "metadataJson": "{\"start\":4314,\"end\":4354}"}, {"text": "So we're going to pick ten, which is definitely beyond one. And so we'll never get confused about this previous transaction. Then at some point. So this transaction picks timestamp ten and that ten might be a little bit further in the future of true time. Right.", "metadataJson": "{\"start\":4354,\"end\":4371}"}, {"text": "So the transaction does the prepare all whatever necessary work, you know, the two phase commit and then actually hits the real commit point. And there may have to wait, right. Because as the commit rule, we have to wait a little while until we actually sure that ten exactly in the past. And so this basically at the commit time, what, you know, the transaction coordinator will do is look at the commit time and just keep reading its local clock and will keep reading its local clock until it gets an interval back where the earliest time is past ten. So maybe at some point of reading, boom, boom, boom.", "metadataJson": "{\"start\":4371,\"end\":4413}"}, {"text": "Reading, reading, it gets an extra interval back and that interval. And if the interval starts at, say at nine or seven, it keeps reading until it actually gets a value that's bigger than ten. And then we know for sure that the true time has passed. And so it's safe actually to commit the transaction. And so any transaction that now runs after t three will have, must have run after two time ten.", "metadataJson": "{\"start\":4413,\"end\":4443}"}, {"text": "So let's say transaction three starts at some point and transaction free starts after t two. So we have to now make absolutely sure that we read, you know, x is two. If it reads. So this is going to read x. And so, you know, it will ask for the current time it's going to get.", "metadataJson": "{\"start\":4445,\"end\":4471}"}, {"text": "Maybe an interval back. Interval might overlap a little bit with t two, but, you know, the true time, you know, maybe for this clock is pretty precise. And the true time, the interval gets back is from ten to twelve. So true time is somewhere between ten and twelve. We know it basically has to be past, you know, ten because, you know, we're going to t three reads by definition, pat, you know, after t two, and that's going to work out because basically t three is going to pick its timestamp, the end of the interval, you know, by the latest rule.", "metadataJson": "{\"start\":4471,\"end\":4510}"}, {"text": "So t three is actually going to run a timestamp, you know, true time twelve, if you will. And that will guarantee, correct, that, you know, this interval, we know is for sure past the true time ten. And so when t three reads, it is going to read x is two because it will observe, you know, the value of transaction two because, you know, the, it's reading well beyond true time. And this must cause a little bit of a delay. But hopefully, if the clocks are pretty precise, as we talked a little bit earlier, that delay is actually going to be small.", "metadataJson": "{\"start\":4510,\"end\":4547}"}, {"text": "Does this make sense?", "metadataJson": "{\"start\":4551,\"end\":4552}"}, {"text": "I'm sorry, I had a question. So if t two is saying that it's going to start a transaction at time ten, then when we read something in the same transaction, do we also need to make sure that ten is outside, like before the now interval? The question is, let's say t two does more than rights and does some recent rights. T two should observe its own rights, correct? Is that a question you're asking?", "metadataJson": "{\"start\":4557,\"end\":4591}"}, {"text": "Um, teach you, like when, when we, when research, for example, was we also gonna read why? Yeah. And so let's make this, add this to it.", "metadataJson": "{\"start\":4592,\"end\":4602}"}, {"text": "Yeah. Or, yeah, I thought after that, but that's oh, that's fine. Okay. Either way, I don't really care. Before they prepare, they actually read.", "metadataJson": "{\"start\":4606,\"end\":4613}"}, {"text": "Why? Let me.", "metadataJson": "{\"start\":4613,\"end\":4615}"}, {"text": "Yep.", "metadataJson": "{\"start\":4619,\"end\":4620}"}, {"text": "And then, oh, okay. Maybe, maybe there's no maybe. The read writings actually don't really matter that much. Okay. Because if you go back at the very beginning at this picture, you know, the, if a read write transaction does reads correct, the reads will go to the shard masters or the shard leaders that will technically take read logs out.", "metadataJson": "{\"start\":4623,\"end\":4653}"}, {"text": "And the reads are basically, the client executes all the operations locally. And so it gets read values, and it will read the most recent read, the value that it got after it locked, you know, the locked variable. So basically, in read write transactions, the locks really do all the global ordering. There's the two phase locking and shears global ordering. And so really, the interaction that is interesting is the interaction between the read only transaction, which is t three, and the read write transactions.", "metadataJson": "{\"start\":4654,\"end\":4692}"}, {"text": "When does t two actually commit? And we're reading it before it commits. So the protocol in the read write transaction is we're reading right early on, we first do all the work, and then we go to the commit phase. So here is like, here's the commit point, or here's where the client says, like, please commit this transaction and sends it to the transaction coordinator. And then the transaction coordinator runs the two phase commit protocol.", "metadataJson": "{\"start\":4694,\"end\":4721}"}, {"text": "But all the shards leaders have locks the values that actually, that transaction is actually using.", "metadataJson": "{\"start\":4722,\"end\":4730}"}, {"text": "Okay. All right, so let's try to summarize a little bit.", "metadataJson": "{\"start\":4750,\"end\":4755}"}, {"text": "So, read write transactions are basically globally ordered or serializable external consistency or serializability plus external consistency, due to the fact that they basically do two phase commit plus two phase locking. The read only transactions are the ones that are sort of special because they only contact a local replica. And the reason they actually see, you know, the correct value is because of snapshot isolation.", "metadataJson": "{\"start\":4763,\"end\":4795}"}, {"text": "Each data item is actually versioned and stamped with the timestamp in which actually was modified. And so, you know, you can read in the past using snapshot relation, and then, you know, to ensure that. So this snapshot isolation really gives us extra serializability.", "metadataJson": "{\"start\":4799,\"end\":4820}"}, {"text": "But, you know, Spanner actually shoots for something stronger. Namely, it actually shoots for this external consistency property, which is like linearizability. It has sort of this real time component to it.", "metadataJson": "{\"start\":4825,\"end\":4836}"}, {"text": "And to actually ensure that we get actually external consistency, we execute the read only operations in timestamp order.", "metadataJson": "{\"start\":4838,\"end\":4847}"}, {"text": "And because timestamp order requires on perfectly synchronized clocks, the spanner relaxes the rules a little bit by actually using time intervals.", "metadataJson": "{\"start\":4852,\"end\":4868}"}, {"text": "And the whole goal correct and by using these set of techniques, it turns out that read only transactions are very fast.", "metadataJson": "{\"start\":4874,\"end\":4882}"}, {"text": "So read write transactions are basically, you know, not actually particularly fast. You know, if you look at the table six, it's, you know, 100 milliseconds for a read write transaction. That means, basically, you can only do ten transactions per second, right. Which is really not much. But the read only transactions are fast.", "metadataJson": "{\"start\":4884,\"end\":4901}"}, {"text": "Now, this is the case, though, like, you know, you know, although the read write transactions are maybe not that fast, then they are very powerful. You're basically doing transactional operations across multiple shards that are sitting in different data centers in different parts of the world. And that is, for programmers, an incredibly convenient and powerful tool that you get asset semantics across shards that are replicated all over the world.", "metadataJson": "{\"start\":4902,\"end\":4932}"}, {"text": "So this is all I wanted to say, actually, about Spanner. I hope this actually was helpful and that you're not maybe less confused and not more confused because of this lecture. But if you have still questions, feel free to hang around, and I'll be happy to discuss this or any other aspect of spanner. And good luck finishing free b. And I hope you get time to enjoy the long weekend.", "metadataJson": "{\"start\":4935,\"end\":4963}"}, {"text": "Thank you.", "metadataJson": "{\"start\":4967,\"end\":4968}"}, {"text": "So I have a question, maybe early in the slides. So, when you have the diagram for the two phase commit for the read write transaction. So before we communicate with the. With the. With the TC.", "metadataJson": "{\"start\":4971,\"end\":4987}"}, {"text": "The TC stands for the coordinator. Yeah. Transaction coordinator. Yeah. Okay, so before we communicate with the transaction coordinator, when we read x and read y, initially we don't do any communication with the TC.", "metadataJson": "{\"start\":4987,\"end\":5000}"}, {"text": "That's correct. So my question is, what is the nature of these reads? Do we treat them as read only? So we read from the replica that is closest to us, or do we actually do, like, a majority type of communication to do these things? No, we go actually to the.", "metadataJson": "{\"start\":5000,\"end\":5015}"}, {"text": "It's going to the participant leader. Sorry. Goes to the Paxos leader. I see. So it goes to the.", "metadataJson": "{\"start\":5015,\"end\":5021}"}, {"text": "Oh, okay. So it goes like in lab three. Sorry. So it's basically. So the read x here basically was the same thing as in lab three.", "metadataJson": "{\"start\":5021,\"end\":5031}"}, {"text": "So it goes through the leader and it gets replicated through the whole. No, it actually doesn't. There's no. I believe there's no actually read going inside, running through Paxos. But, you know, it goes through the transaction reader, the Pax's leader.", "metadataJson": "{\"start\":5031,\"end\":5046}"}, {"text": "And the PAx's leader, you know, of course, knows what the last right was. But what if the, for example, we have some failure and the leader thinks that it's the leader, but it's in some partition. And it actually has outdated information.", "metadataJson": "{\"start\":5046,\"end\":5057}"}, {"text": "I don't know exactly. You know how they deal with this case? I think. I think their leases make that impossible. Yeah, so they have the.", "metadataJson": "{\"start\":5059,\"end\":5069}"}, {"text": "So every leader is leader for a period of time. And you become during that period of time, no other leader can be leader. Okay, that's it. Awesome. Thank you.", "metadataJson": "{\"start\":5069,\"end\":5081}"}, {"text": "Thank you, Romania. Hey, I had a question with the last diagram with the time interval. Yep. Yeah. I was wondering if, like if the read for t three, or like, let's say like the right in t two, the time intervals from like one to twelve.", "metadataJson": "{\"start\":5082,\"end\":5101}"}, {"text": "Also such that the write happens at twelve for t two and t three reads also happens at twelve. What happens in that scenario? Okay, you gotta walk me through one more. Okay. Yeah.", "metadataJson": "{\"start\":5101,\"end\":5114}"}, {"text": "So for t in t two, the. Right, right now it's from one to ten, right? Yeah, maybe if that's like twelve. So that it's like the same as the transition three. Okay, twelve, not 212.", "metadataJson": "{\"start\":5114,\"end\":5127}"}, {"text": "Sorry, I was meaning both.", "metadataJson": "{\"start\":5127,\"end\":5129}"}, {"text": "So if the. So the p. Correct, section two would pick twelve as the start time of the read write transaction. It would wait correct until twelve shows up in the interval before it commits. So true time is somewhere between one and twelve.", "metadataJson": "{\"start\":5132,\"end\":5158}"}, {"text": "It will wait until at least he's 13. So true time is now beyond twelve for sure. And now we know that t three starts after t two. Correct. Right.", "metadataJson": "{\"start\":5158,\"end\":5171}"}, {"text": "So it could never got this interfall back because the true time is already definitely past 13 or, you know, past twelve. So when this guy now reach its clock, we know that it's going to be some interval. You know, maybe it still includes ten, but you know, it will include 14 or 13 and we'll pick the latest value, right? And so let's say it picks 14 and so it will do a timestamp actually at 14.", "metadataJson": "{\"start\":5171,\"end\":5203}"}, {"text": "Gotcha. So basically, like, that scenario wouldn't have happened because the time interval thing would have guaranteed the earliest and latest. Yes, and it is the case that t three definitely started after t two by definition. That's the way we set up the example. And then also, then if instead the read was happening around the same time as commit, like not necessarily strictly after, as in this case.", "metadataJson": "{\"start\":5206,\"end\":5231}"}, {"text": "Like what is there any guaranteed on that? Like, does it have to remember? Great question. So remember, the definition of linearizability is correct. If t two and t three basically run roughly concurrent, like really run truly concurrent, then it doesn't matter.", "metadataJson": "{\"start\":5231,\"end\":5247}"}, {"text": "T three can go forward to t two or after. Okay, but then what about serializability aspect? Because if they execute again, like it might not necessarily be in the same order. Right. The war executes some total order and t three either goes before t two in total order or after t two.", "metadataJson": "{\"start\":5247,\"end\":5269}"}, {"text": "But either order is fine. Both are allowed by serializability or linearizability because the execution is truly concurrent. Gotcha. Gotcha. The t three starts before t two.", "metadataJson": "{\"start\":5269,\"end\":5280}"}, {"text": "Committed. Gotcha. And then for the t three, like if the commit and read happens at the same time, the t what t three is actually going to read is just going to depend on if the replica it's reading from has that commit. All right. Gotcha.", "metadataJson": "{\"start\":5281,\"end\":5295}"}, {"text": "Thank you so much. Hey, welcome. Good question. I had a question about so like if it's what it is in this picture and they when it tries to commit twelve is not there, it's just going to retry and wait again. Yeah.", "metadataJson": "{\"start\":5295,\"end\":5313}"}, {"text": "Okay. It's just keep reading the clock until it gets an interval back where the earliest is passed. It's timestamp. Okay. And I also had just a clarification just to make sure.", "metadataJson": "{\"start\":5313,\"end\":5329}"}, {"text": "The guarantee that it provides is that if it assigns in timestamp x and by the time where x is like before the now interval, this machine or this boxes group will have seen everything or this machine will have seen everything that has happened at like before x. Is that right? Yeah, I think more or less, yes. I'm not 100% sure what you're asking, but you know what? We get an interval back.", "metadataJson": "{\"start\":5329,\"end\":5366}"}, {"text": "What we know is that the true time is somewhere in this interval. So here's true time.", "metadataJson": "{\"start\":5366,\"end\":5371}"}, {"text": "And so when we start to prepare, you know, we know that, you know, true time is not past twelve, but it's somewhere between one and twelve. And so when we do a commit, you know, we've got to make sure that the commit actually happens really after true time. And so we're going to wait a little bit. So we know we got pick twelve and we're going to wait until actually our clock gives us an interfall where true time is definitely past twelve.", "metadataJson": "{\"start\":5374,\"end\":5400}"}, {"text": "I think what I was asking was more if you pick twelve and then your interval is returned to be 13 to 20, do you know that anything that with time stamp less than or equal to twelve that the changes for those transactions that you're going to be able to see them? Yes. Okay. Okay. That makes sense.", "metadataJson": "{\"start\":5403,\"end\":5428}"}, {"text": "Thank you. Yeah.", "metadataJson": "{\"start\":5429,\"end\":5430}"}, {"text": "So I don't totally understand what the point of commit wait actually is because it seems like commit wait will happen after you've already selected the timestamp for the transaction that you're interested in and you'll simply delay the actual, like, absolute time at which the thing will actually get committed. Just to make sure that I know. External. No, but at the commit time, you really start updating the database. Aha.", "metadataJson": "{\"start\":5432,\"end\":5457}"}, {"text": "And so. And at commit time, you have to commit, you return, like in t three didn't run until after the c. Right? That was our definition. Like t three started after t two committed.", "metadataJson": "{\"start\":5457,\"end\":5470}"}, {"text": "Right, right. But like, what would happen if you didn't, like, what do you lose if you don't have. Then t three would run concurrently with t two.", "metadataJson": "{\"start\":5470,\"end\":5478}"}, {"text": "Okay, hold on. I see. I'm not sure which scenario you want to explore, but if we allow t three to start before c, then we have a completely different story. Correct. Because then there's nothing to discuss because t three runs coherently with t two and it observed two t or not.", "metadataJson": "{\"start\":5481,\"end\":5498}"}, {"text": "And both are correct. Okay, I see. And so the timestamp for C, we got to wait until we pass true time of, you know, the studying of the prepared number that we get back. So we know that the true time really has passed t three. Could you never pick a time, a true time that actually would be before t two committed on.", "metadataJson": "{\"start\":5498,\"end\":5529}"}, {"text": "I see, I see. Okay. I guess it seemed like since everything was already versioned, that if you sort of modify the data, if you sort of physically modified the database early, like that would be okay because everything has a time step attached to it, so no one would. From far away, it seemed like no one would do the wrong thing. I guess like looking more at what they actually define as extra as their external consistency.", "metadataJson": "{\"start\":5531,\"end\":5557}"}, {"text": "I don't really understand how it is similar to linearizability because it just says that if a transaction actually commits before another one starts, then they'll have, then the first one will have a smaller timestamp than the latter one. Is it also sort of like implicit in that, that transactions execute in the order of their timestamps? For sure. Maybe my shorthand notation was not so brilliant, but, yeah, absolutely. The real requirement is that if a transaction starts, if t two starts after t one committed, you must observe all the rights from t one.", "metadataJson": "{\"start\":5557,\"end\":5594}"}, {"text": "Right, right, I see. And I guess the. I guess the time knowing that the timestamp of t two is larger than t one will tell you that you're going to have observed everything of t one because. I see. That makes sense.", "metadataJson": "{\"start\":5595,\"end\":5609}"}, {"text": "Yeah. Yep. Thanks. You're welcome.", "metadataJson": "{\"start\":5609,\"end\":5612}"}, {"text": "Any other questions? If there's still anybody there, I had a question about part four 2.3. The schema. Change transactions. Yeah, yeah, yeah.", "metadataJson": "{\"start\":5620,\"end\":5635}"}, {"text": "Okay, good. Yeah, I didn't talk about it at all. I was just curious about like, because essentially they talk about like predicting the time of the commitment. Well, let me tell you the way I think about it. And then you can see me, you can tell me where, you know, where your confusion is.", "metadataJson": "{\"start\":5635,\"end\":5653}"}, {"text": "So schema change, correct. That means basically adding column to a table or deleted column of a table, something along those lines, so really changes the layout of the database. And so schema changes are generally expensive and the way they make sure that they're atomic, they run them in far in the future. So they run them with a timestamp well beyond the current time. And so that gives, that transaction can just like do its stuff.", "metadataJson": "{\"start\":5653,\"end\":5683}"}, {"text": "Right. Because every other transaction is reading and writing using this version memory. And you know, they're creating a version memory that's way farther in the future, so it can't affect any of the current transactions running. Now, it could be the case that this takes so long that by the time they actually want to commit the schema migration part, there are actually transactions that are starting to encourage on the time that migration transaction started. Those are transactions time just marches on.", "metadataJson": "{\"start\":5683,\"end\":5721}"}, {"text": "They're doing. And then the rule is that basically any read write transaction or read only, any transaction basically has to stop until basically the transfer, the migration transaction has completed. Because the migration transaction is time stamping the new values with its timestamp. So when there's a request essentially for this migration, um, it chooses a commit time far into the future. And then, you know, you hope that basically the, that transaction also commits, you know, not, yeah.", "metadataJson": "{\"start\":5721,\"end\":5762}"}, {"text": "By then. Yeah. So, and any reads that come at a time up until then are served from the current. Yeah. Local replica, no problem at all.", "metadataJson": "{\"start\":5762,\"end\":5772}"}, {"text": "Correct. Because they could have even seen it. Right. Okay. And I guess it's a pretty cool trick, right?", "metadataJson": "{\"start\":5772,\"end\":5780}"}, {"text": "If you have version memory, you can schedule things in the future. Yeah, it's a cool trick. I was just wondering how they actually like figure out that point in the future.", "metadataJson": "{\"start\":5780,\"end\":5790}"}, {"text": "Yeah, it's probably some heuristic or something. Okay. So, but, but essentially, so if you, you don't choose a time that's far enough in the future, then you run the risk of. Okay, so then you have like incorrect results. Okay.", "metadataJson": "{\"start\":5792,\"end\":5812}"}, {"text": "To avoid that risk of incorrect results, which is actually blocked. Yeah, I see. Okay, awesome. Thanks so much. You're welcome.", "metadataJson": "{\"start\":5813,\"end\":5823}"}, {"text": "All right, bye.", "metadataJson": "{\"start\":5823,\"end\":5824}"}]}