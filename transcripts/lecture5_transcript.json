{"documentId": "lecture5", "section": [{"text": "So the topic for today is raft replication protocol that we'll discuss in quite a bit of detail. In fact, we're going to spend multiple lectures on it, one because it's sort of one of the core elements of examples of distributed replication protocol. So today we've been mostly focusing on, on the material that is necessary for labs two a and two b. So the election of the leader as well as pushing the logs around. And then next week we're focusing more on two c and 2d, namely the snapshots and the walk compaction, and gives us opportunity to discuss any other aspect of raft.", "metadataJson": "{\"start\":0,\"end\":53}"}, {"text": "In fact, we'll talk about rap one more time in the week after. We'll have a q and a lecture on two a and two b, and so talk about the actual solutions to the labs, two a and to b. That's the plan. And so I'm going to just dive in with the starting point for today. The place probably to start is to observe that in some of the previous case studies of replicated systems, we've seen sort of a pattern.", "metadataJson": "{\"start\":53,\"end\":89}"}, {"text": "If you look at GFS, Mapreduce, virtual FMFT, they all have a single point of failure. So even though they're replicated systems and they do replication for fault tolerance, all of them actually had a single point of failure. In the case of Mapreduce, it was the coordinator, in the case of GFS, the master that hands out the leases, for example, and in the VM FTE cases, the storage server, or really the testing set server.", "metadataJson": "{\"start\":92,\"end\":133}"}, {"text": "And the reason why, as we discussed in the previous lecture, the reasons that actually there are single machines instead of replicated machines is to avoid the split brain syndrome.", "metadataJson": "{\"start\":135,\"end\":148}"}, {"text": "And for many systems, I mean, one way is sort of a bummer, correct? We're building a high performance, we're building very fault tolerance systems. And on all these designs, actually, we do actually still have sort of a single point of failure, even though replication is used for many other aspects of these system designs and for many of the systems that we talked so far about to avoid this split brain syndrome and actually introduce a single point of failure or maintain a single point of failure, that's perfectly acceptable in many cases, because in many of these cases, like if the storage server or the master goes down, you know, hopefully it will, it's only a single machine, so the chance that it's going down is smaller than, you know, any out of the chunk servers. And, you know, probably there's somebody ready, you know, to be beat up. You know, in the case that the master goes down, comes back up and can really make sure there's only one master online.", "metadataJson": "{\"start\":156,\"end\":211}"}, {"text": "So, you know, many, many circumstances is perfectly fine. You know, it will lead maybe to a very short downtime. But you know, in some systems, you know, it's really nice, wouldn't really nice is actually if this, we didn't even have to have two single points of failures. Basically we could reduce downtime even further and increase uptime. And that's really where the protocols, the style of protocols that raft come into play.", "metadataJson": "{\"start\":212,\"end\":242}"}, {"text": "But just before diving into or talking about the key ideas in the protocol, let's remind ourselves why this single point of failure leads or why actually replicating these sort of crucial single point of failures like the test and set server, can lead to this split brain problem. You might think, why didn't you replicate the single point of failure too? So let's try to do that and then we'll quickly see what the problem is. So let's replicate the testing set server.", "metadataJson": "{\"start\":243,\"end\":276}"}, {"text": "And so let's do a simple, straightforward case. We'll just have one tested set server s one. And here we have s two as the other replicated test set server and just remind. Correct. Like test and set takes an argument new and it returns the old value.", "metadataJson": "{\"start\":288,\"end\":310}"}, {"text": "And the goal is that if two clients at the same time call test and set, one wins, gets eventually falls back as the old value and the other one will lose because it will get true back as the old value. So let's see how this might work play out. So we have a client what calls destined set and communicates with the first server, what communicates with the second server, and we don't really know actually, it doesn't get the response for the second server. So there's sort of two cases now where if s two doesn't respond, there's two possible reasons why that could be the case. So one case could be s two didn't respond because s two failed.", "metadataJson": "{\"start\":312,\"end\":362}"}, {"text": "And in some cases, in that scenario, really what we'd like to be doing is that at that point, basically c one just declares victory, you know, since, you know, nobody else can actually see, observe, you know, the value at s two, you know, we should be done. We could proceed. But the problem is that there's the second case. And the second case is that there's a network petition.", "metadataJson": "{\"start\":366,\"end\":389}"}, {"text": "There's a network partition between c one and s two. So in that case, really c one cannot proceed because there could be another client, c two, and actually communicating with s two, and it would be terrible. Correct. If c one proceeds in this case because it might then update s one, get basically the false value, think it succeeds at the same time s two communicates with s two and also succeeds during the test and set. And so now we're violating our contract of the test and set.", "metadataJson": "{\"start\":391,\"end\":428}"}, {"text": "So this is this issue of this split brain. Like when there's a network petition, we can end up in a situation where both servers are up and running and serve a different subset of the clients and thereby violating the contract of the specification of the system that we're building. And the real challenging part here is that c one just cannot tell the difference between these two situations. It doesn't know where it actually is. S two is not reachable because it failed or because of the network partition.", "metadataJson": "{\"start\":429,\"end\":462}"}, {"text": "And so that's sort of the conundrum that's behind all this. These previous systems and why they sort of used a single, why they don't replicate these crucial surfaces that need to be up to avoid the split brain syndrome. So what can be done about this? And really the key problem here is what can we do about network partitions? How could we handle them?", "metadataJson": "{\"start\":463,\"end\":494}"}, {"text": "And so there's a key idea that sits in raft and in many other protocols that underlies basically the solution or why those protocols actually might succeed. And really what it is, in sort of a nutshell, it is this majority rule.", "metadataJson": "{\"start\":494,\"end\":514}"}, {"text": "And just to give the example in the context of the test and sets in your server, instead of actually running the server with two replicas, which I conveniently did, I'm going to run it with three. So I'm going to have s one, s two and s three. And the rule now is going to be that like a server can, a client can consider an operation succeeded if it at least can update a majority of the servers. So in this case we can send messages, s one for the doing test and set. We send a message to s two for doing testing set.", "metadataJson": "{\"start\":517,\"end\":550}"}, {"text": "Then we get back positive responses from both of them, like a false in both case. Then we'll return false to actually the application and the application considered the test and set succeeded. And of course we do it also to the third one. But the third one made actually, you know, the, the message actually might not arrive or the server might be down. We don't really know.", "metadataJson": "{\"start\":550,\"end\":569}"}, {"text": "But actually we don't really care because consider the following situation scenario where we have the second client also trying to do in test and set. Well, to be able to succeed, it has to talk to a majority of the servers. So whatever majority is going to talk to, it is going to include either s one or s two for s one and s two. Both of these operations actually have succeeded by SC one. So s two or c two will always observe the result of C one's operation, because there's sort of an overlap.", "metadataJson": "{\"start\":569,\"end\":605}"}, {"text": "And we'll see basically in raft, this is exactly the same thing, sort of roughly what's going on, right? Like when a leader accepts or enters an operation in the log of the majority of the followers, then it means that a subsequent leader that's going to come on in the next term will also try to acquire a majority to get voted as the leader. And as part of the voting, you know, there's going to be one server or one follower that actually has seen the last operation performed by the last leader. And so that is going to basically be the building stone on which we can build these fault tolerant services that can handle network partitions in failures of servers while still achieving strong consistency. Another way to think about this majority business is that if the network petitions, there can be only one partition that has a majority, there can be no other partition that has a minority, and so only the partition that actually has the majority can actually proceed.", "metadataJson": "{\"start\":622,\"end\":693}"}, {"text": "Another way of saying that, another implication of that is it could also be the case that there are multiple partitions and there's no majority anywhere. And in that particular case, the system can just not proceed, and clients talking to it basically have to wait until the network is healed enough that at least that there's going to be at least one partition with a majority of services.", "metadataJson": "{\"start\":695,\"end\":720}"}, {"text": "Another sort of quick observation here is that of course you may want to get this particular scheme that I just described with free service only tolerates one server going down, correct? If two servers going down, nobody can actually obtain a majority, and so therefore nobody can actually, or no client can actually get the operation through. Clearly, it's easy to extend this idea by what typically is called like doing two f plus one replication. So if you want to tolerate f faults instead of one, you need two plus f one for the server, so that at least you have always a minority, the majority if f fails. And so in the case of here, f is one.", "metadataJson": "{\"start\":724,\"end\":763}"}, {"text": "And so we're running with three servers. You want f to be two, you have five servers, et cetera, et cetera. You want f to be free, you have seven servers. Okay. One other thing that may be important to point out, and I came up a lot in the questions, what is actually the majority?", "metadataJson": "{\"start\":763,\"end\":780}"}, {"text": "And the majority is the majority of all the servers that are there, both the up ones and the down ones. And so when you take a majority, you don't take the majority of the two. If there are only two alive, you take a majority of all the servers in the system.", "metadataJson": "{\"start\":781,\"end\":795}"}, {"text": "Any questions about this sort of key idea of can there be an even number of servers? Like if you have four servers with the majority, then be three? Yeah, if we'll see that in a couple cases. Correct in graph that, you know, if, you know, the number of servers is reduced from seven to six, and because one is down and you still want to proceed, you need a majority, which you still need four servers to actually proceed. But if you can get the four servers, then you can keep them going.", "metadataJson": "{\"start\":800,\"end\":829}"}, {"text": "Okay, any other questions? So I also have a question about the majority. So does the majority consider the server itself? So suppose in raft does like the server itself considering this. Yeah, yeah, it's part of it.", "metadataJson": "{\"start\":834,\"end\":849}"}, {"text": "So, like, you know, often, as we see in raft, correct, that the leader vote immediately for itself or the candidate votes for itself. And the leader, when it appends, you know, to its own log, it counts that as one. So it's part of it.", "metadataJson": "{\"start\":849,\"end\":862}"}, {"text": "Okay, so there's quite a number of protocols using this idea, and sometimes this idea is referred to as quorums or quorum protocols for the obvious reason. And there were sort of two the state of the art for a long time until like in the early nineties or late eighties, basically there's no protocol. So basically we were always in a situation of like this single point of failure. And then in early nineties, there were two protocols that came, were invented roughly at the same time. One is called paxis, which is mentioned in the paper quite a bit.", "metadataJson": "{\"start\":868,\"end\":915}"}, {"text": "And the other one is called viewstand replication, sometimes called VR. And these were invented around 1990. They didn't actually get much attention at that point in time because people didn't really have pressing needs to build these sort of completely automatic fault tolerance systems. But that changed in the last 15 years. So in the last 15 years, there's much use of these protocols.", "metadataJson": "{\"start\":915,\"end\":947}"}, {"text": "And in fact, sort of interesting. Observe. Correct. That is in the last 15 years, and that's basically sort of 15 years after these protocols were invented. So basically they sat on the table or sat on the shelf for 15 years until people actually had a real use case for that.", "metadataJson": "{\"start\":956,\"end\":975}"}, {"text": "The protocol that we're going to be mostly implementing in the lab and we're going to discuss in the lecture and which is the topic of the paper is, you know, falls in this, you know, lineage of protocol, and it's called draft. And it was, I think it was came about or it was written up in 2014, the papers from 2014. And this is one of the more sort of complete descriptions. You know, it touches on a lot of different aspects. If you're going to build a complete replicated state machine and explains those clearly, that's one of the reasons why we're using it for the labs and for this particular lecture.", "metadataJson": "{\"start\":977,\"end\":1012}"}, {"text": "Any questions about the history here around protocols before I dive into Raft?", "metadataJson": "{\"start\":1015,\"end\":1024}"}, {"text": "Okay, so before actually diving into RAF, let me first sort of talk a little bit about how one would use Raff to build a replicated state machine, because that's, in the end, our end goal, and that will help reasoning about what raft actually should be doing for us. And so the basic way you would use raft, and we'll actually do this in lab three, is, let's say you have a server and the raft is basically nothing else than a library. And so in our setting, it's going to be code package and you can write a replicated state machine by basically importing that package. So here's we got raft at the bottom and you're build a server using raft. And so for example, in lab three, we're going to build a key value server using raft.", "metadataJson": "{\"start\":1031,\"end\":1093}"}, {"text": "And clients talk to the key value servers and submit, put and get operations.", "metadataJson": "{\"start\":1094,\"end\":1106}"}, {"text": "The key value servers, when it receives one of these put and get operations, it basically hands it off to raft. It basically puts it to the raft and actually sticks it in. What we'll see in much more detail, it'll put it in a log. Actually draw this slightly differently. So pens it to the log, actually the end of the log, and then raft internally is going to talk to other servers to basically replicate that log.", "metadataJson": "{\"start\":1109,\"end\":1142}"}, {"text": "And so here are some other servers. They're basically structured exactly in the same way. They have a raft library component and a key value server, like in many other replicate state machines that we've seen. Identical key value server. And we're going to play the same game as any of the other previous replicated state machines we were seeing.", "metadataJson": "{\"start\":1142,\"end\":1165}"}, {"text": "We're going to basically feeding operations to the key value server in the same order everywhere. And as a result, again, the key replicas all apply the operations in exactly the same order. We should say the same key value store is being built. So we have a key value store here. Whatever key 20 has some value.", "metadataJson": "{\"start\":1165,\"end\":1186}"}, {"text": "We're applying the updates to this table exactly in the same order. And all the replicas things should be good. We're going to have identical replicas. So the way raft is structured is that one of the replicas is the leader. So let me for convenience, you know, that's going to be the first one on this site and it just actually declines to talk to the leader.", "metadataJson": "{\"start\":1186,\"end\":1207}"}, {"text": "And then what happens next is that the, so the client talks to one of the leader, the leader, KV server. KV server appends a log entry to the raft log of the leader. And then, you know, basically communicates, you know, with the other raft libraries. And those raft libraries do exactly the same thing. Basically they append, you know, the operation, you know, to the log, to their logs and send the response back, you know, to the leader.", "metadataJson": "{\"start\":1208,\"end\":1244}"}, {"text": "And you know, of course you know, all this state historic, the log and some other stages stored on disk so that if any one of them fails they at least have the last part of their log still available and can build from there. So once basically, and we'll talk a little bit more in detail, but once the entry or the log entry is replicated on free servers, then they can actually be delivered in order to the key value server. So every committed operation or every log has an index or every operation has an index zero or actually 1234 whatever in order and they're going to be delivered in order to the key value server. So once an operation committed, it's going to be delivered to the key value server. The key value server performs the operation, maybe updates the key value table.", "metadataJson": "{\"start\":1246,\"end\":1299}"}, {"text": "And then since actually the leader alone sends a response back to the client. So that's roughly the sequence of events that happen. So client sends a request to the leader leader inserted in Raft. Raft chitchats with the other servers. Once it's replicated on enough machines then it's considered to be committed and we'll talk a little bit more precisely what committed means.", "metadataJson": "{\"start\":1300,\"end\":1325}"}, {"text": "And then uncommitted operation is delivered to the key value servers and then they execute the operation and respond to the client or the leader response to the client. So what happens on the failure?", "metadataJson": "{\"start\":1325,\"end\":1339}"}, {"text": "On a failure, a newer leader is elected and it will take over the role of the old leader. And so for example, like in this particular picture, it might be the case that this leader crashes and this becomes the new leader in the next term as we see exactly what that means. And then the clients basically foil over, they will see they don't get a response. So they will time out and they fail over to the second leader, the new leader, and basically retry their operation. And so they insert that new leader will take their operation, insert it back into raft.", "metadataJson": "{\"start\":1344,\"end\":1383}"}, {"text": "The same things happens as before, and maybe this time we get lucky and we execute the operation without any failures. Now, if you think a little bit carefully about this, that does mean that a client operation might end up twice in the log, right? Because it actually may be the case that the first time it succeeded, but just didn't get the response, so it doesn't know that it succeeded. Will retry this new leader will put the log, put the new retry operation also in the log, and it will pop out at some point at the key value servers, and that will actually be a duplicate. And so as you will see later in the lab three, actually, it turns out that you need to do duplication detect duplicates, but that's mostly an issue for lab three.", "metadataJson": "{\"start\":1383,\"end\":1442}"}, {"text": "So it's not going to be the main issue for now. But it's just important to know that sort of, this is the sort of general style in which one would actually use RAv to build a replicated state machine.", "metadataJson": "{\"start\":1442,\"end\":1454}"}, {"text": "Any questions about sort of the how to use raft for replicated state machines? We have a question in the chat. Yeah. What is a typical number of clients contacting the leader?", "metadataJson": "{\"start\":1458,\"end\":1472}"}, {"text": "Well, there could be many. There's no real limit on it. What you will see, it could be the case that a single machine can, I mean, maybe that's implicitly what the question is. It could be the case that a single leader can just not tolerate, is not capable of handling that many clients. And so what happens then is that basically the servers get sharded.", "metadataJson": "{\"start\":1475,\"end\":1497}"}, {"text": "You shard the key value servers in multiple raft groups. So for example, shard one would be one raft instance, shard two would be a raft instance. Chart. Free will be a raft instance, and the clients talk to the appropriate shard to apply their operations, and in that way can scale the server to many clients. And in fact that is what lab four does.", "metadataJson": "{\"start\":1497,\"end\":1520}"}, {"text": "How does the client know how to communicate with the new leader? After the old leader fails, the client basically has a list of all the servers that are in the system. So in this case we're running with free servers. Everybody agrees there are only free servers in the world. The clients know about the free servers and they try one randomly.", "metadataJson": "{\"start\":1522,\"end\":1540}"}, {"text": "And if that is not the client, it will re redirect. If that's not the leader, it will redirect the client to the appropriate one, the actual leader. Yeah, the answer is quite do we assume the servers are geographically close to each other, or could they be far apart? They could in principle be far apart. There's no real restriction on that site.", "metadataJson": "{\"start\":1541,\"end\":1565}"}, {"text": "The only issue of course, is if the servers are very spread around the world, the delays to actually get a lock record appended will take a little while and so the delays will be long.", "metadataJson": "{\"start\":1566,\"end\":1578}"}, {"text": "What is the log entry being executed in the KV storage once it's committed by raft. So once Raft has decided that enough replicas have received it, and it's no way possible that one has to back out of that operation, then it's handed to the key value server. So the leader will first like execute this command and then the leader will tell the followers that this command is committed. And then the follow. The leader knows, as soon as the leader knows that the operation is committed, Excel can hand it off to its key value server.", "metadataJson": "{\"start\":1582,\"end\":1619}"}, {"text": "Let me, let's talk about it right now and I'll go in more detail as we go. So like a bit more of an overview of the protocol. So previous board was sort of an overview of how you use raft to build and replicate state machine. Now let's look at the raft itself a little bit in more detail. So here are some timelines.", "metadataJson": "{\"start\":1619,\"end\":1639}"}, {"text": "We got a leader, we got two followers. So we're running the three decline talks to the leader. The leader has a log of all the putting get operations. When it gets a new one, it pins it to the end of the log. Then it actually sends it, you know, sends the log entries to the new log entries to the followers.", "metadataJson": "{\"start\":1639,\"end\":1669}"}, {"text": "And let's say that follower two, here's follower one, here's follower two. Follower one actually receives this log entry. Everything is okay. It pencil to the end of the log. It sends basically an act back saying yes, I appended it.", "metadataJson": "{\"start\":1675,\"end\":1689}"}, {"text": "At this point in the raft view of the world, two servers actually have the log entry. And so raft actually the leader can actually commit the log entry. And so at this point the leader can actually hand off that request that is just in the receive. They can actually hand it off to the KV server. And the way actually the lab does this or the lab infrastructure does this, is that basically we have a channel, a go channel, one single go channel where basically the raft go routines and the raft go routine that actually decides that a particular operation is committed.", "metadataJson": "{\"start\":1689,\"end\":1733}"}, {"text": "It just sends that operation on the apply channel so that the key value servers can actually apply it some point later. So notice at this point the leader basically has committed the operation. And it's perfectly safe for the leader to commit this operation because a majority of the servers, actually the majority of the peers actually have received the operation. Correct. So even if there was a failure and we elect a new leader, that's the case that that leader, or one of the servers that is remaining, actually has the last operation that was appended to the log by the previous leader.", "metadataJson": "{\"start\":1733,\"end\":1773}"}, {"text": "So everything is good. Of course, the last guy, the first follower, will also at some point respond and said yes. So this point, basically from the point of view of the leader, the operation is committed, except the followers don't really know it yet, because the followers, they only know that they got it and that the leader probably has it, but they don't really know. And so what actually happens is that the subsequent operation, so when another client request comes in, the leader will append another log entry and basically sends a new operation or a new append entry to the followers. And that append entry does basically two things.", "metadataJson": "{\"start\":1773,\"end\":1817}"}, {"text": "It provides the new log entry for the new operation, but it also confirms that all the preceding operations tells us which operations actually have committed so far. And so when f one and f two receive this operation, this RPC, they know that the operation has been committed by the leader, and therefore at that point they can also deliver it through their key value instance and know that this operation is committed.", "metadataJson": "{\"start\":1817,\"end\":1846}"}, {"text": "I had a quick question. So when the leader sends an operation over and a server replies, does that mean. That means it's in the log, right? Yes. But what happens if you get a majority, like a slight majority, and then one of those logs, like one of those servers crash?", "metadataJson": "{\"start\":1851,\"end\":1874}"}, {"text": "Like are logs, do logs have to be in storage, like in disk? Yep. So that's a very good question. Correct. So the every change to the log, if you look at the raft paper in detail here, some state must be stable, and the log is one of those pieces of information that must be stable, as well as the term number and a bunch of other things.", "metadataJson": "{\"start\":1874,\"end\":1896}"}, {"text": "And so if a server crashes, no big, no big deal, really, because the majority deal of the servers does have that entry on their disk, and when they come back up, they'll find it.", "metadataJson": "{\"start\":1897,\"end\":1909}"}, {"text": "I have a question. Yeah. So what happens if, for example, follower one replies with yes, and the leader commits just before it sends the next command to the remaining followers to also commit, it crashes. Now the remaining followers don't know that they need to commit because the leader has crashed. So wouldn't this cause problems?", "metadataJson": "{\"start\":1913,\"end\":1938}"}, {"text": "They will commit it. Right? Because one follower will have it in his log, and as we'll see in the leader election rules, it will become the leader and it will propagate that append entry to the other servers and then they will apply it to. Okay, I see. Okay, thanks.", "metadataJson": "{\"start\":1938,\"end\":1955}"}, {"text": "Okay, so that brings me maybe this is a good sort of segue to the next topic on why logs.", "metadataJson": "{\"start\":1957,\"end\":1962}"}, {"text": "Last few questions all about that. Yeah, actually as a follow up to the last question. So the server crashing after it has consensus, crashing right before committing. It could delay committing. What do you mean with committing?", "metadataJson": "{\"start\":1964,\"end\":1984}"}, {"text": "So after it has consensus on a change, right after it has a majority, the paper said you sweat. You essentially tell other ones, like all the servers, you tell them, okay, this is ready for or sorry, it's committed. It's ready for like execution right into the state machine. So if it crashes before it tells other servers that it's ready for execution, it could delay execution. Right.", "metadataJson": "{\"start\":1984,\"end\":2017}"}, {"text": "It could delay execution. Okay, so first of all, if the leader fails at that point, nothing happens, correct. Like at least the clients can't proceed with any other, more operations. And so basically if you want to wait to think about it at that point, sort of raft reconfigures itself, elects a new leader and that's going to be, you know, f one or f two. You know, one of the two is going to be the new leader and the other one is going to be the follower.", "metadataJson": "{\"start\":2018,\"end\":2039}"}, {"text": "Right. And one of the two actually has received, you know, the append entry that the leader might already have committed. In fact its KV might already have executed. No problem. Correct.", "metadataJson": "{\"start\":2039,\"end\":2050}"}, {"text": "Because one of them has it. That one will become the leader as we'll see later with the leader election rules. And that one will, you know, submit it to its kv server and we'll replicate it to the other follower and then it will apply to its kv server. So in the end we all will apply all the operations in the same order. I agree.", "metadataJson": "{\"start\":2050,\"end\":2070}"}, {"text": "Eventually it'll happen. But it could be delayed, right? Yeah, could be delayed. Absolutely.", "metadataJson": "{\"start\":2070,\"end\":2074}"}, {"text": "The new ladder sets a complete log to all the followers.", "metadataJson": "{\"start\":2079,\"end\":2081}"}, {"text": "It will, the way it works is actually it will try to send the end of his log. If one of the followers doesn't recognize the end of the log it will back off and send more and more earlier entries. We'll talk about it later. But in the end it could be the case that the leader will replay its complete log to one of the followers if one of the followers has missed all the log entries, again coming back to logs. So one reason you might wonder why you have logs at all is because like the KV server has a database too, has a table with all its information.", "metadataJson": "{\"start\":2084,\"end\":2121}"}, {"text": "And so why do we need actually sort of this information twice? Once in the logs and once in the KV table? And so a couple of reasons for that. One, a couple that already came up in the questions. Basically one for retransmission.", "metadataJson": "{\"start\":2121,\"end\":2137}"}, {"text": "So when the leader, you know, sends one of these append entries, you know, to one of the followers that message might get lost. And so the leader must be able to retransmit it. So it needs to keep a record of all the log entries that are sort of in flight. Second reason, it's probably the prime reason first is like we need order. Every append operation or every command must be delivered in the same order at the same.", "metadataJson": "{\"start\":2144,\"end\":2172}"}, {"text": "And all the replicas and the log is a very convenient way for us to actually maintain that order. So the second reason, retransmission. The third reason is we need persistence.", "metadataJson": "{\"start\":2172,\"end\":2183}"}, {"text": "One of the followers might crash where actually all of them might crash. And then they come up again. And we still need to be in a position that we can basically retransmit log entries to bring everybody up to date. And so the log must also be persistent. And finally, we need sort of space for tentative operations or tentatively committed or tentative commands.", "metadataJson": "{\"start\":2185,\"end\":2209}"}, {"text": "So as we noted earlier or that came up in those earlier questions, when the leader sends an operation to a follower, the follower actually doesn't really know at that point whether that operation will be committed. So it just must stick around for a little while until it actually learns whether that operation is committed. And so we need some space for these tendon if operations. And the log is a convenient place to actually do that.", "metadataJson": "{\"start\":2211,\"end\":2231}"}, {"text": "In the end, you know what will happen. Correct. Is that the logs are identical on all servers.", "metadataJson": "{\"start\":2234,\"end\":2244}"}, {"text": "You know, there may be out of sync during periods of time. One might have more entries than other logs. But like if we sort of keep running the system and then stop the clients, then at some point in time all the logs will be completely identical. And that means that basically those, since they all have the same order, all the operations, all the operations will be fed, you know, to decline the KV servers in the same order. And they KV servers will end up in the same state.", "metadataJson": "{\"start\":2248,\"end\":2275}"}, {"text": "Okay.", "metadataJson": "{\"start\":2277,\"end\":2278}"}, {"text": "Okay, let me talk a little bit about an individual log entry.", "metadataJson": "{\"start\":2283,\"end\":2286}"}, {"text": "So we. Whatever. I'm going to draw many pictures like this where, you know, there's a bunch of log entries, you know, starting zero, one two, blah blah blah blah. And if we look inside of one of these log entries, you know, there's going to be a command which we're mostly going to sort of ignore. We're not going to really think about much about it at all.", "metadataJson": "{\"start\":2297,\"end\":2316}"}, {"text": "That's the thing that actually being delivered to the application. So that's, for example, the put in order to get operation with its arguments. And then the second thing that's in there is a term, and this is basically the leader's terminal. So the term in which this particular command actually was appended to the log. And one way you can think about this is that basically the term identifies uniquely the leader that appended the operation to the log.", "metadataJson": "{\"start\":2316,\"end\":2349}"}, {"text": "Because during every term there's only one single leader. And so the term id really basically implicitly signals who the leader was that actually appended to the, to that log entry. The number that goes along here. So if this is n or maybe like, let me use I here, you know, that's the log index.", "metadataJson": "{\"start\":2349,\"end\":2374}"}, {"text": "So the combination of a log index plus a term number basically uniquely identifies the content of that particular entry. There could be no two log entries that have the same index, the same term, and have different commands because only a single leader could have been in charge in that particular term and that leader commits intense operations.", "metadataJson": "{\"start\":2378,\"end\":2400}"}, {"text": "Will uncommitted log entries be overwritten? Yeah, they might get overwritten. We'll talk about it later, but that's certainly possible.", "metadataJson": "{\"start\":2404,\"end\":2410}"}, {"text": "Okay, so if we look at this picture sort of, there are two things that this part of the, I guess, an answer to this question. There are two things that need to happen. We need to elect leaders for a particular term, and we need to actually ensure that the sure logs become identical.", "metadataJson": "{\"start\":2413,\"end\":2436}"}, {"text": "We have failures and we have leader changes. And so we're going to talk about both of these topics, and we're going to start with leader election and then talk about making launch identical a little bit later. Okay. Election is really the topic of lab two a and the story is in some ways straightforward. We have, say here we are a system of three entries.", "metadataJson": "{\"start\":2439,\"end\":2473}"}, {"text": "We have a leader in, you know, term ten. We have a follower in term ten. So we're sort of stable situation. Another follower in term ten. And let's say the leader crashes or, you know, gets partitioned, you know, from network.", "metadataJson": "{\"start\":2473,\"end\":2488}"}, {"text": "So, you know, can't talk anymore. What will happen is that the followers will start an election. And the reason they start an election is because they're missing heartbeats from the leader.", "metadataJson": "{\"start\":2488,\"end\":2500}"}, {"text": "The job of the leader is to periodically, in a sort of fixed interval, send a pent entry to the followers. Normally that might happen because a lot of clients are active and so the followers continuously get append entries. But if the leader doesn't receive any commands from the clients, then it actually supposed to send in a heartbeat, periodically telling the, basically to inform the followers that actually you're still the leader. And the heartbeat basically takes the form of a normal append entry, except no new log entries. And so the leader just tells it later.", "metadataJson": "{\"start\":2507,\"end\":2549}"}, {"text": "Just tells an heartbeat. Like, my log is this long and this is my last entry. And if, you know, those match and then everything is good. So if the leader fails, then after a couple of heartbeats and we'll talk a little bit more in detail about this, there will be an election timeout. So the followers have a timer running and they reset that timer every time they get a heartbeat or an append entry.", "metadataJson": "{\"start\":2549,\"end\":2581}"}, {"text": "But if they don't receive any heartbeats or append entries, then at some point in time after this election timeout, the timer goes off. And at that point, a follower starts an election. And just let's assume that the first follower reaches that point first and what it will do, it will increase its term number. So it will set the term number to eleven. It will talk to itself, if you will, and be part of the, and will vote for itself.", "metadataJson": "{\"start\":2582,\"end\":2612}"}, {"text": "And then, you know, they'll contact, you know, the other follower, and we'll also try to contact the leader, presumably the leader, you know, let's say the leader is down, so the leader doesn't respond, but the second follower does respond. And so at that point it gets two votes, you know, one from itself and one from the follower. And at that point, actually it becomes the new leader. It becomes the leader for term eleven and then it, you know, start again. Basically, clients fall over to that leader and, you know, the things you just proceeded before.", "metadataJson": "{\"start\":2612,\"end\":2641}"}, {"text": "Okay, now maybe there's a couple problems that you might worry about, but one of them could be the following situation. It turns out there was a network petition between leader ten where the leader for term ten and the followers for term ten. And so at some point that network partition heals. And so maybe your client requests came in still to the leader for term ten. And now, of course, now it looks like, oops, we might have actually two leaders.", "metadataJson": "{\"start\":2643,\"end\":2675}"}, {"text": "And we're back into the split syndrome problem. That turns out not to be the case because when the leader tries to actually send append entries to the followers, to followers that are now in term eleven, they will just reject those append entries. And they will tell the old leader that, too bad he is not the leader anymore. In fact, they will send a message back saying, no, I cannot do the append. And here is my current term number eleven.", "metadataJson": "{\"start\":2676,\"end\":2711}"}, {"text": "The leader receives that, sees that the term number eleven is bigger than this term number ten and basically steps down as leader and becomes a follow up. And they kick off another election or to basically become part of a term that has all three of them. But basically, there's no chance of a split brain problem because you just can't actually get any operation through. So no split brain.", "metadataJson": "{\"start\":2712,\"end\":2737}"}, {"text": "And the reason that we avoid the split brain is because of this majority rule as well as these term numbers.", "metadataJson": "{\"start\":2742,\"end\":2747}"}, {"text": "Okay. That's, of course not. The only problem that we might have. Another challenge is we might end up with split vote.", "metadataJson": "{\"start\":2751,\"end\":2765}"}, {"text": "So, like, with this particular picture, you know, we had our leader, maybe he was leader in ten, his partition of the network. Here are two followers in ten.", "metadataJson": "{\"start\":2769,\"end\":2780}"}, {"text": "And if we're not careful, maybe they will actually start elections very close to each other. And so f ten votes for itself, the first leader follower follows itself. The second leader, the second follower vote for itself. Then they sent vote requests each other. And the rule is that you can vote for one vote per term.", "metadataJson": "{\"start\":2782,\"end\":2811}"}, {"text": "So when, you know, the first follower has voted for itself, it has voted for itself. And so when it receives a second vote request, it actually cannot vote for that vote request because it already voted for itself. And so at this point, we're going to have a split vote. Like, this guy is going to have one vote and this guy is going to have one vote. They won't do anything.", "metadataJson": "{\"start\":2814,\"end\":2832}"}, {"text": "But at some point, again later, there will be a timeout, and then this process starts again. And of course, you know, as part of that process, you know, this is election eleven. Election eleven resulted in twelve. It will actually go to election number 1212. And, you know, start.", "metadataJson": "{\"start\":2833,\"end\":2850}"}, {"text": "Try to try to do this again. Now, if you're not careful, correct. This could just happen over, over and over endlessly. You know, basically every time, you know, the two followers roughly at the same time start this election, then, you know, we're not going to make forward progress. And so we need to avoid this problem is that the election timers are randomized.", "metadataJson": "{\"start\":2850,\"end\":2872}"}, {"text": "And so when the followers, you know, set their election timer in the paper, they talk about, like, picking a value between 150 milliseconds and 300 milliseconds, a random number in that interval. And every time, you know, these followers basically reset their election timeout, they pick a new number, a random number in that interval, and only when that timer then goes off, they run the election. And if basically, you know, the. If this interfall is wide enough, it's unlikely that, like, the first person or the first follower that actually gets the first follower whose election timer runs out, you know, the interval is wide enough that, you know, there's a good chance that it actually will succeed. You know, completing a complete election before the second timer goes off for another follower.", "metadataJson": "{\"start\":2888,\"end\":2944}"}, {"text": "And so this avoids, you know, this sort of endless split vote. You know, we might get unlucky and we get maybe one or two split votes. But like, over time, you know, that has to be the case that we in the end will succeed.", "metadataJson": "{\"start\":2945,\"end\":2956}"}, {"text": "There are a couple sort of, and this is important maybe for the lab, too, there's sort of a couple pressure on these election timeouts.", "metadataJson": "{\"start\":2960,\"end\":2968}"}, {"text": "You don't want to pick an election time out that is too short because if it's too short for shorter than a heartbeat, you might lose one message and immediately you start running an election and nothing bad happens because we'll elect a new leader, you will go through a new term and all that kind of stuff. But basically during that election, the system is actually not usable. The clients are actually blocked. So you don't want to unnecessarily cause elections. And so one of the things you want to do is basically probably take a value that's at least bigger than a few timeouts, than a few heartbeats.", "metadataJson": "{\"start\":2974,\"end\":3017}"}, {"text": "We're maybe in a data center. We'll take a few milliseconds sort of to do an RPC, and we may want to wait at least maybe three, four RPC routing time so that we get a chance to retry an RPC without actually having the election timer go off and so we can recover from temporary network failures. Then we want to presumably add some random value to that random value to avoid the split votes. And so one hand, you know, we want to make the random value as big as possible, right? Because we make it big, then there are very little chance that actually we run into a split vote problem.", "metadataJson": "{\"start\":3023,\"end\":3065}"}, {"text": "But the other hand, if we do that, then there's a chance that, you know, the system may be down for a longer period of time. We might pick a big value for the election timeout. And that means downtime from the point of view of the clients. So we want to keep this, you know, short enough that the downtime is short.", "metadataJson": "{\"start\":3065,\"end\":3088}"}, {"text": "In the paper, in Lindsay, in eval section, does quite a bit of work on elevating, you know, what are some reasonable values for their particular setting? And then that's how they got to the 150 to the 300 milliseconds in the lab. We're going to be reasonable, generous, you know, basically if you sort of recover within a second, you're more or less going to be good with our test cases.", "metadataJson": "{\"start\":3095,\"end\":3117}"}, {"text": "Okay, I want to make one more point about the, about the elections. So, you know, another scenario that is sort of important to consider is, you know, we have a follower, we have a leader. The leader goes down. So there was ten, this was ten follower ten.", "metadataJson": "{\"start\":3122,\"end\":3142}"}, {"text": "And we already talked a little bit about it when, let's say this guy goes first, it votes for itself.", "metadataJson": "{\"start\":3145,\"end\":3151}"}, {"text": "And, you know, we, the protocol records, you know, unstable storage. Why, who it voted for. So the record, you know, records on the. So this follower one, this follower two, it will record. It voted for interim eleven, you know, for follower one, why is it recorded on stable storage and why does the follower need to remember that, that it voted in term eleven for itself?", "metadataJson": "{\"start\":3154,\"end\":3187}"}, {"text": "This way it doesn't vote twice. If it fails. Yeah, it might crash right, like right here and come back up. And it should remember that. It might have, let's say it was actually, there's a third one guy in here, you know, follower or whatever, free.", "metadataJson": "{\"start\":3194,\"end\":3213}"}, {"text": "And it already voted for follower three, then it can actually not change in mind. Correct. And so because in the end, we, at the end of the term, we need to have to be in a position that per term there's only one leader and never, never two. And so to ensure that, you know, every follower must remember for which candidate it voted and never change its mind.", "metadataJson": "{\"start\":3214,\"end\":3239}"}, {"text": "Okay. I guess there's a whole discussion in the chat about the timeout numbers and in relation to the lab so people can look there if they are confused about this or otherwise. We can talk about elections a little bit more because this is about the last thing I want to say about elections. Any questions about the lectures?", "metadataJson": "{\"start\":3247,\"end\":3270}"}, {"text": "I have a bit more general question. So in like, the figure two of the lab, it says that for each server, you're storing like the current term and then who it voted for. Yep. But not the, like, current state of the server. Like if it's candidate or follower or leader.", "metadataJson": "{\"start\":3276,\"end\":3296}"}, {"text": "So I'm wondering how they, like, is that implied or is there another way to figure that out? I guess when you come back up, you come back up as a follower and, you know, you start an election and that will, in the end, at the end of election, you know what you are? You're either a follower or the leader.", "metadataJson": "{\"start\":3297,\"end\":3320}"}, {"text": "I see. Okay, thank you. That makes sense. One warning about figure two, both positive. And then there's maybe negative statement anywhere in the figure two when it says, like, you know, you should do this, you should really do it.", "metadataJson": "{\"start\":3324,\"end\":3340}"}, {"text": "So you cannot admit any detail that actually is in figure two. If you do that, then undoubtedly you're going to fail some of the test cases. Unfortunately, figure two is not complete. And so you still will have to do some thinking. So particularly like figure two doesn't really say that much about like how the replies are handled of the vote RPC and the append RPC.", "metadataJson": "{\"start\":3340,\"end\":3360}"}, {"text": "And so you will have to do some thinking sort of to fill in the missing details. And so there's two points here, like figure two. Anything that's in there you better deal with it. But there still might be things missing that you will have to resolve for yourself.", "metadataJson": "{\"start\":3360,\"end\":3376}"}, {"text": "You'll be looking at figure two a lot.", "metadataJson": "{\"start\":3380,\"end\":3382}"}, {"text": "So from the description of the election process there seems to be like even for lab two a, which deals only with election, there seems to be some state that we need to store on persistence. Yeah. So like for example, like who you voted for and the term number. Yeah, but when you look at the code of lab two, all of the code that has to do with saving the persistence, it says above at lab two c. So is it that it's not that crucial maybe to care about persistent storage or should we absolutely start implementing persistent storage?", "metadataJson": "{\"start\":3386,\"end\":3418}"}, {"text": "Yeah, I think in the first couple of tests we don't actually crash machines and so therefore it's not important. And until you see, we're definitely crashing machines. Okay, awesome, thank you.", "metadataJson": "{\"start\":3418,\"end\":3429}"}, {"text": "Okay, how are we doing? Everybody on, on board?", "metadataJson": "{\"start\":3433,\"end\":3439}"}, {"text": "Okay, here we go, next step. Okay, so one important thing to realize, and you know, this came up earlier in the questions and I want to hit that now topic hard, is that logs may diverge, verge and it can be quite dramatic, but just let's get into it for just a basic understanding and then discuss it in more detail. So let's do a couple simple examples. So I'm going to write things using the following notation. We're going to have three servers as before, but I'm going to cut all the timeline business and just like draw the logs.", "metadataJson": "{\"start\":3445,\"end\":3491}"}, {"text": "And you know, we have indexes. So let's say these are indexes. You know, there's preceding part two, we're talking about ten 1112. And so you know, there's a term entry in each one of them. So let's say all three servers committed or pended their operation in term three to index ten.", "metadataJson": "{\"start\":3495,\"end\":3519}"}, {"text": "And the same thing happened with index eleven. So one way to think, one way you could get this correct is that server one would be the leader. Server one appends it to its own log, then replicates it to s two and s three. Same thing happens for index eleven, then index twelve. Maybe they're also successful.", "metadataJson": "{\"start\":3521,\"end\":3544}"}, {"text": "It tends one operation in that index for each term. So that's term free, but then it crashes. Now we're in a situation where one server actually has an extra log entry in its log and the other ones don't. Or maybe one of them has a two, but they're definitely not identical. That's a simple case, correct.", "metadataJson": "{\"start\":3544,\"end\":3568}"}, {"text": "Nothing really particularly going on, but there's much more interesting cases going on. So another case could be one server. Again, s one. Here we go. Same scenarios.", "metadataJson": "{\"start\":3568,\"end\":3580}"}, {"text": "S two, s three. And let's say we have the following logs.", "metadataJson": "{\"start\":3580,\"end\":3586}"}, {"text": "So this is the situation we have. In index ten, we have entries at all servers for term free. In index eleven, the first server has no entry, the other two have an entry for term free. And then last one, number twelve, we have entries in index four at servers two and three with term numbers four and five. So the first question that we need to ask ourselves, is this possible?", "metadataJson": "{\"start\":3594,\"end\":3624}"}, {"text": "Can raft end up producing logs in this way? In the same log entry, we have two different term numbers in same log index. Yeah, that's possible.", "metadataJson": "{\"start\":3626,\"end\":3639}"}, {"text": "So basically it looks like. So like what is the scenario that produced? Yeah, so it looks like server two or server three was the leader for term three, and then got some logs out to server one and then shared another log with only one of the two servers, after which point it went down. And then s two got elected as leader for term two, which is still possible because its log is like up to date, as. As up to date as the others.", "metadataJson": "{\"start\":3641,\"end\":3678}"}, {"text": "And then say that again just to make sure. So s two got elected for term four, right? Yes. Okay. Okay.", "metadataJson": "{\"start\":3679,\"end\":3685}"}, {"text": "Not term two. Okay, so s two gets elected for term four. Yeah, yeah, yeah. And then using s one as a backup, basically as the follower.", "metadataJson": "{\"start\":3685,\"end\":3694}"}, {"text": "Yes, I think. Yep. And then s three gets elected for term five before s two can put anything in the logs. Yeah. Another way to saying it, like maybe s two crashed right away after got elected to turn four.", "metadataJson": "{\"start\":3696,\"end\":3710}"}, {"text": "And so then there's a timeout. Then let's say s three was petitioned, but now it's back and that apple going, it will become, it can come into term five, correct? Yeah. Okay, good. So that's possible.", "metadataJson": "{\"start\":3710,\"end\":3727}"}, {"text": "So it turns out there are quite a bit of wild variations possible. So if we like, look at the.", "metadataJson": "{\"start\":3728,\"end\":3733}"}, {"text": "So here's the figure six. Figure seven. Sorry, from the homework and oops.", "metadataJson": "{\"start\":3736,\"end\":3745}"}, {"text": "And the homework asked basically questions about the form, like, well, what happens? Like this guy in the figure six itself, in figure seven, whatever it is, I think it's figure seven.", "metadataJson": "{\"start\":3747,\"end\":3759}"}, {"text": "The scenarios being discussed is this guy becomes the top one, becomes new leader, and in the homework, we ask the question, what happens if this leader just goes away? And what are the possible outcomes? You know, and the possible outcomes, like, are, sorry, for every log index, the question is, which one gets rejected, which one will be accepted for sure, and which ones depends.", "metadataJson": "{\"start\":3761,\"end\":3793}"}, {"text": "And I think this is an important thing exercise to do, because once we understand what all log entries are, possible outcomes, in other words, who will really firm up, like, our understanding about how raft actually is supposed to be operating? What I'd like to do is do a quick breakout room, and I'd like you to identify two possible outcomes that not guaranteed in the corresponding scenario. So let's take a quick breakout room here, session here, and let's try to figure out the answer to the homework question. And five minutes, really. Were you able to Johns?", "metadataJson": "{\"start\":3796,\"end\":3842}"}, {"text": "Do you want me to do. Yeah. Oh, yeah. Yep. Let me actually make you participants host.", "metadataJson": "{\"start\":3842,\"end\":3859}"}, {"text": "Okay, I'm going to your host it.", "metadataJson": "{\"start\":3859,\"end\":3884}"}, {"text": "Okay. Can everybody hear me again?", "metadataJson": "{\"start\":4330,\"end\":4332}"}, {"text": "Can I get a positive signal? Yes. Okay, super. Okay, so I hope everybody had a good time discussing this. And so let's.", "metadataJson": "{\"start\":4335,\"end\":4348}"}, {"text": "So this is really this question about log divergence. And, you know, we see here, correct. In this figure, there are pretty wild variations in what can happen, and these are all possible, as the caption of the figure explains, they're all possible scenarios. So the question in the homework, and just to quickly review that, I'm sure most of you already, I don't think we can see your screen. Oh, how do you do that?", "metadataJson": "{\"start\":4348,\"end\":4377}"}, {"text": "That's because I forgot to click share. Yes. Okay. Let me fix that. I appreciate that.", "metadataJson": "{\"start\":4378,\"end\":4385}"}, {"text": "Okay. Ready to see my screen? Yeah. Okay. Thank you.", "metadataJson": "{\"start\":4394,\"end\":4400}"}, {"text": "Sorry for that. Okay, so going back to this caption of figure seven, figure seven explains, like, these all possible situations, and, you know, what we want to figure out before talking about the details about how the logs get repaired, you know, what actually is possible? So assume that actually, this top guy did not get elected leader in the caption of the figure, but, like, basically, what. What outcomes are possible? And they're going to be what I mean with that is, like, for example, we look at f, you know, it has a two index of an entry from term two in index four.", "metadataJson": "{\"start\":4400,\"end\":4438}"}, {"text": "And the question that we want to ask and understand is that clearly this is possible because this could have happened. What will happen next? Is it possible that this entry will survive? So, when we reconfigure, like, the leader, basically, you know, Lars gettin being, you know, put together in or synchronized or, you know, the leaders force the logs to be identical, is there a scenario in which actually the log entry, the entry from term two will survive?", "metadataJson": "{\"start\":4439,\"end\":4466}"}, {"text": "No. No. Correct. This will definitely be rejected. Correct.", "metadataJson": "{\"start\":4469,\"end\":4474}"}, {"text": "And why is that the case? There's entries from term six that are already committed. And so f will never be elected. Yeah, f will never be elected. And so somebody else will be elected and nobody has a two in term, two in index four.", "metadataJson": "{\"start\":4474,\"end\":4490}"}, {"text": "So it will be overwritten. Which what value will be overwritten?", "metadataJson": "{\"start\":4491,\"end\":4494}"}, {"text": "There will be a four over written. Yeah. This will become a four. Right. So when later on when we talk about sort of lock, you know, synchronization or forging logs on followers, it has to be the case that this two turns into four at f.", "metadataJson": "{\"start\":4498,\"end\":4513}"}, {"text": "Good. So this also answers sort of a second question. Like the entry from the index four from term four is definitely going to be accepted. Okay, then maybe more interesting is this question about depends. So we know at least one entry term that definitely got to reject it.", "metadataJson": "{\"start\":4514,\"end\":4537}"}, {"text": "We know one term that's definitely going to accept it. And you know, are there any sort of terms and indexes initiates that may be possibly accepted? The 7th maybe? Yeah, the sevenths. So when would the sevenths not be accepted?", "metadataJson": "{\"start\":4537,\"end\":4554}"}, {"text": "Okay, so the easy scenario is the 7th will be accepted. Correct. Because D gets exacted leader and then it will force it log on everybody else. So that's the case where seven gets accepted. When does seven get rejected?", "metadataJson": "{\"start\":4555,\"end\":4567}"}, {"text": "If C gets selected leader and then D is down and it is overridden. Yes. D goes down, C becomes leader and then C's log entries are being pushed on everybody else entries. And then whenever D. And so we're, then we're going to be turn seven.", "metadataJson": "{\"start\":4568,\"end\":4587}"}, {"text": "Correct. And at some point that will be longer and every D comes back up, you know, its entries are going to be overwritten. Right.", "metadataJson": "{\"start\":4587,\"end\":4593}"}, {"text": "So seven is definitely possible but not guaranteed. In the other case, I had a question. Eight could become leader too, right?", "metadataJson": "{\"start\":4596,\"end\":4605}"}, {"text": "A. Yeah, if, yeah, a could get leader if C and D are down. C and D don't even have to be down for it to be elected leader. Right. It just has to get, if it's the first one, the longest log.", "metadataJson": "{\"start\":4607,\"end\":4624}"}, {"text": "Correct. If the term, if the two terms are equal at the end, then you pick the longest log. But I think if a is the first one to call for an election, potentially bf, if C and D are down, a could win the election. But if they're alive, can't it also still win the election? Ah, so let's say C and D.", "metadataJson": "{\"start\":4624,\"end\":4652}"}, {"text": "Okay, let's d is down. Let's do that for sure. Correct. Because we know that D will win the election, period.", "metadataJson": "{\"start\":4652,\"end\":4657}"}, {"text": "Does it have to, like we know, we know, we know it won't get a vote from D, right? For sure. But, but what does it have to win? Well, what do we think?", "metadataJson": "{\"start\":4659,\"end\":4672}"}, {"text": "What if a received the vote from b, e and f first and then received the vote from D?", "metadataJson": "{\"start\":4675,\"end\":4681}"}, {"text": "We can get a majority. Correct. Without actually de participating.", "metadataJson": "{\"start\":4685,\"end\":4688}"}, {"text": "Even if they participated in the election, it's possible for a to get the majority votes, right?", "metadataJson": "{\"start\":4698,\"end\":4704}"}, {"text": "Yes, I think we just answered that.", "metadataJson": "{\"start\":4708,\"end\":4711}"}, {"text": "Isn't, isn't there a mechanism though where if a candidate sees a message from a, another server with a higher turn number, like stands down?", "metadataJson": "{\"start\":4714,\"end\":4725}"}, {"text": "Yes, from a higher term number. But a has to, if D is down, it doesn't have to be down, right? You just have to get them first. Yes, yeah, exactly.", "metadataJson": "{\"start\":4728,\"end\":4742}"}, {"text": "But it will step down when it gets to vote from seven or d. Wait, are you sure about that? Because I don't think so.", "metadataJson": "{\"start\":4746,\"end\":4754}"}, {"text": "Well, what, okay, so if D at some point will depends, they're going to start racing, correct? The, okay, let me actually, this is a very important topic and so I don't want to do it in 10 seconds. And let me come back to this at the start of the next lecture. Okay, this is a great point to stop.", "metadataJson": "{\"start\":4756,\"end\":4775}"}, {"text": "I think the state machine in figure four clarifies it. Like a candidate goes back to follower if discovers a higher term. So if wasn't down it will go back to follower, it won't try to elect itself. Wait, but I thought that it would only go back to a follower state if it received an append entries from a current leader. Like a request vote is not the same.", "metadataJson": "{\"start\":4779,\"end\":4805}"}, {"text": "So it's worth knowing that there's a difference between the term number that a server's at and the term of the most recent entry in the log. Oh yes, server a can have most recent entry in the log b six via term six, but have its current term number be seven. Its term number can actually be arbitrarily high. Yes, correct. That's correct.", "metadataJson": "{\"start\":4807,\"end\":4832}"}, {"text": "And so a could start term eight and then get elected in term eight and that. And whatever d tells it doesn't matter because if these most recent term is seven, a won't go back to follower because it's a higher term than seven. Ah, I see. So it could be a higher term, arbitrary. Could a go into, um, into term seven, its own term seven?", "metadataJson": "{\"start\":4832,\"end\":4861}"}, {"text": "If d was like partition, like if there was a network petition and it didn't know about D, I confused. Like even if it does know about D. Why does that matter? It shouldn't matter.", "metadataJson": "{\"start\":4861,\"end\":4874}"}, {"text": "If a promotes itself to candidate and then sends out a request, a vote request, then d could come back and say, oh, well, my latest log entry was at this index for term seven, but I don't think that. Wait, actually it doesn't even say that, right? In the response to a request vote, RPC just says, you know, its current term, which would be seven, which matches a's, because that we, that's our hypothesis. And then, yeah, if a started election, it will be turn seven, but a diva will not vote for a. Yes, that's true, but I don't know.", "metadataJson": "{\"start\":4879,\"end\":4929}"}, {"text": "I think it's totally fine if D is alive and gives a response to a's vote request. Like, I don't think that deters a from being a candidate someone just put in the chat. I think the answer to that. So like Grant said that as soon as D rejects the vote, a will revert to being a follower because it will realize that it has a lower term number. But okay, not necessarily, because again, the, remember that the most recent term on their log is not the same thing as the most recent terminal that the server keeps track of.", "metadataJson": "{\"start\":4929,\"end\":4967}"}, {"text": "You can have a higher current term than the most recent thing on the log. And so if a tries to elect itself in term seven, yes, when it tries to contact D, it will give up. But if a tries to elect itself in term eight, which it would, if it's already seen that these has reached term seven, then a can get elected. Wait, but if D is, if we assume that D is in term seven, like if a tries to elect itself for term seven, then we don't actually hit this case, because if the RPC request or response contains a term which is strictly greater than the current term, you give up. But if they're the same, then it doesn't matter, it will just proceed as normal.", "metadataJson": "{\"start\":4967,\"end\":5013}"}, {"text": "That's correct. Yes, that's true. Do you get entries get replaced by whatever a is telling it? So at the end of section, like the election restriction, section five 4.1, they say ref determines which of the two logs is more up to date by comparing the index and term of the last entries in the log. So wouldn't that mean that D has to become the leader if we're comparing?", "metadataJson": "{\"start\":5014,\"end\":5041}"}, {"text": "No, because D just votes. No, but other people might vote yes.", "metadataJson": "{\"start\":5041,\"end\":5046}"}, {"text": "Yeah. So what about, so go ahead. What about the case where e runs for election in term five, for example? Five, yeah, and then it gets a vote from d saying no, it would then update its term right. And stop running.", "metadataJson": "{\"start\":5049,\"end\":5069}"}, {"text": "Okay, hold on, I got this. You're thinking about e starting an election for five. Yeah. And if it gets a response from almost anybody, it's going to see that it's behind in a term. Right.", "metadataJson": "{\"start\":5070,\"end\":5081}"}, {"text": "So it does update its term in that case. Right. Another question, another way to ask this question. Who can become leader? Right.", "metadataJson": "{\"start\":5081,\"end\":5089}"}, {"text": "In this, you know, from these, you know, whenever, from these six, ABC through f, who can become leader?", "metadataJson": "{\"start\":5089,\"end\":5095}"}, {"text": "Can one f become leader? No, all of them are up. Can e become leader also? No, the only ones that can become leader are AC and D. That's correct.", "metadataJson": "{\"start\":5097,\"end\":5111}"}, {"text": "Possible leaders.", "metadataJson": "{\"start\":5114,\"end\":5115}"}, {"text": "But can, can a become a leader if D is alive? Because the cause. The safety section says that the RPC, the request vote RPC implements the restriction that it compares the last log entries and the one with the highest log entry should become the leader. No, I only think it doesn't vote for someone. Exactly.", "metadataJson": "{\"start\":5117,\"end\":5141}"}, {"text": "So like d will never vote for a. Right or for anything else. That's exactly right. I mean, it's also not true that like if you have an entry in a higher term than another node that like you're actually sort of like better. Right.", "metadataJson": "{\"start\":5143,\"end\":5159}"}, {"text": "Because I mean, you can be like becoming leader and trying to append entries into the log, but they like don't go through so well. These life, they wouldn't go through. Okay, these life. Correct. D voted against a becomes leader.", "metadataJson": "{\"start\":5159,\"end\":5175}"}, {"text": "D try a actually tries to. Well, if a won't contact d if actually didn't vote for it and so they would just proceed. So why could d have logs in term seven if it's not a leader in term 7D? Must have been the leader in term seven, correct? Yeah, I agree.", "metadataJson": "{\"start\":5175,\"end\":5202}"}, {"text": "Otherwise we'd have no log entries.", "metadataJson": "{\"start\":5203,\"end\":5205}"}, {"text": "When can you have log duplication? I think I forgot. So in this case was d, was d a leader in term seven who got disconnected and then the new leader got elected, the one at the top? Yeah, I think so, actually. Let me take that back.", "metadataJson": "{\"start\":5214,\"end\":5236}"}, {"text": "I don't really remember exactly the sequence. I didn't really pay attention much to the top guy. But I presume that is the case. It does say that is leader for term eight at the, in like the right of that law.", "metadataJson": "{\"start\":5236,\"end\":5250}"}, {"text": "I think the case, we're looking at, the one where a has a higher term number, then D is a contradiction case in the safety argument they mentioned, like, I think they proved that this is not possible, that what is thought possible, like that a could be elected, be missing an entry and have a higher term than D, which is the case here. I think there's a, like the step proof that this is a contradiction. I don't think so. I think it's totally fine for it to be elective leader. Yeah.", "metadataJson": "{\"start\":5253,\"end\":5284}"}, {"text": "I think the, I think the safety proof says that a future leader cannot have, needs to have all of the logs committed. Committed. And seven is not committed. Exactly. There's a whole bunch of operations here that they're not committed.", "metadataJson": "{\"start\":5284,\"end\":5300}"}, {"text": "Correct. They're just tentative and so anything can happen to them. Nothing wrong will happen in the end. And the reason that the whole committed thing works is because if it's committed, then more than the majority have those entries. And so someone who doesn't have the committed entries could never become elected, right?", "metadataJson": "{\"start\":5301,\"end\":5319}"}, {"text": "That's correct. Yes, that's exactly right.", "metadataJson": "{\"start\":5319,\"end\":5322}"}, {"text": "Yeah. They discussed like. Yeah. Like in terms of only someone who has all the committed entries can get elected. I think it gets more complicated with, you know, within the people who have all the committed entries, which ones can get elected.", "metadataJson": "{\"start\":5329,\"end\":5344}"}, {"text": "That's. Yeah. Some ways it doesn't matter. Yeah.", "metadataJson": "{\"start\":5344,\"end\":5349}"}, {"text": "All the commits after term four are not committed. Then here. Right. So these two fours, those are not the first. The first two fours, those are.", "metadataJson": "{\"start\":5357,\"end\":5370}"}, {"text": "What's the question? They were committed. Yeah, those would be committed, but then everything after wouldn't be. Yeah. Those two fives will be committed.", "metadataJson": "{\"start\":5370,\"end\":5378}"}, {"text": "Two six. And the two six will also be. Oh, we only need exactly half. It's okay. Exactly half is okay.", "metadataJson": "{\"start\":5378,\"end\":5386}"}, {"text": "Okay. No one over half. What, four? Yeah. So the fives and six wouldn't be committed.", "metadataJson": "{\"start\":5386,\"end\":5394}"}, {"text": "Oh, but you have the, oh, you have the other leader, right? Yeah. Yeah. Okay. Oh, let's not forget about those.", "metadataJson": "{\"start\":5395,\"end\":5401}"}, {"text": "Right, I got it. Okay. I'll come back to this at the beginning of next lecture because some, some of you probably had to run to another class. But I'll come back to this. If you have more questions, feel free to stick around.", "metadataJson": "{\"start\":5401,\"end\":5416}"}, {"text": "You know, we'll resume this.", "metadataJson": "{\"start\":5417,\"end\":5419}"}, {"text": "Can you say the log duplication again? When could there be log duplication? What I meant is that the leader at some point forces its log on the followers.", "metadataJson": "{\"start\":5423,\"end\":5437}"}, {"text": "I think. I think you meant it on some of the previous slides. Okay, so maybe. Yeah, maybe again, one way. I can't remember what I said.", "metadataJson": "{\"start\":5441,\"end\":5451}"}, {"text": "Sorry.", "metadataJson": "{\"start\":5451,\"end\":5452}"}, {"text": "It is a little more. Maybe a little more.", "metadataJson": "{\"start\":5455,\"end\":5458}"}, {"text": "Oh, it says they're deleted duplicates. Detect duplicates. Yeah, detect duplicates. Yeah. Yeah.", "metadataJson": "{\"start\":5469,\"end\":5477}"}, {"text": "This is, this is not a raft issue. Correct. This is really a key value service issue where a client request might be reach. A client may. Okay.", "metadataJson": "{\"start\":5477,\"end\":5490}"}, {"text": "A client might not get a response even though that request actually went through raft because what happened is that the leader maybe applied the operation to its state, but before it responded to the client, it crashed. So the client will retry and will send its request to the new leader. The new leader will run it through Raft and it will pop out of raft again. And so it has to be the case that the KV server does duplicate detection. Also, this is like only if the client didn't get the response.", "metadataJson": "{\"start\":5490,\"end\":5521}"}, {"text": "Exactly. Yeah, exactly. Okay. Okay, I see. Thank you.", "metadataJson": "{\"start\":5521,\"end\":5525}"}, {"text": "Yeah. Then there will be no, you will do this in lab three.", "metadataJson": "{\"start\":5525,\"end\":5528}"}, {"text": "I think I also asked this question during the lecture, but I don't think I fully understood the answers. So I'll just repeat the question. Yeah, the question is that. So you said that the way commits work is that once the leader commits, it waits for a new client message and then it just appends that message to the log entry and sends an append entry to the remaining followers with an additional message saying that they should also commit all the previous entries. Correct?", "metadataJson": "{\"start\":5533,\"end\":5561}"}, {"text": "Yeah. There's indirectly the protocol, they're staff. Correct. Right. So my question is that what if the leader commits all the entries and right before it is able to send this message to the remaining followers, the leader crashes.", "metadataJson": "{\"start\":5561,\"end\":5576}"}, {"text": "It cannot commit until it has a majority response from all the majority of the followers. I see. So it first, like even it sticks to it log, but doesn't actually deliver to the KV servers yet. I see. So it waits from a reply of commit from all the remaining followers and then it commits its own.", "metadataJson": "{\"start\":5576,\"end\":5598}"}, {"text": "Yeah. Okay. Okay. So there's basically, there's this variable last applied or commit index. Correct.", "metadataJson": "{\"start\":5599,\"end\":5605}"}, {"text": "That is actually maintaining. It only increases the commit index once it receives a response from the majority of the followers. And all of the followers say that they have committed their own, like they have committed the log entries on their own servers. They will commit their log entries once they know that the leader actually has committed it.", "metadataJson": "{\"start\":5605,\"end\":5625}"}, {"text": "Right. So that's my question. Like, how would they know if the leader is unable to send that message to the remaining folks? Well, so then, you know, we basically, we end in situations that we just saw in this figure seven. There's going to be tentative log entries in their logs.", "metadataJson": "{\"start\":5628,\"end\":5642}"}, {"text": "And depending who becomes new leader and what the log situation is, you know, that operation may get committed or may not get committed. I see. Okay. Yeah. Thank you.", "metadataJson": "{\"start\":5642,\"end\":5655}"}, {"text": "Hi. I had a follow up question on this. So if says the leader pushes a log entry, it gets accepted by majority, but it crashes. Then later on this log entry can be committed. Right.", "metadataJson": "{\"start\":5655,\"end\":5668}"}, {"text": "By some other leader. It can also may not. Yeah, but say if it's get committed, then how does the new leader know who was the client who requested for this log entry? Or how will the client notify it in the.", "metadataJson": "{\"start\":5669,\"end\":5684}"}, {"text": "Okay, so there's a really question about like actually how the KV servers actually stored information with raft. So the scenario is client talk to the leader. The client, the leader actually committed did execute this operation or not? It did not. No, it didn't.", "metadataJson": "{\"start\":5693,\"end\":5711}"}, {"text": "It did not. So then one of the followers later will get this operation, maybe apply. It won't send a response because it doesn't even know about the client. But the client will retry. Right.", "metadataJson": "{\"start\":5712,\"end\":5725}"}, {"text": "Because it actually hasn't gotten response. It will contact a new leader and basically enter the same operation in the raft again. And it will pop out again and then the server will send, as we'll see in the lab three, it will send the last response. So in fact the servers remember the last value that they sent back.", "metadataJson": "{\"start\":5725,\"end\":5744}"}, {"text": "Okay, so if there's a get request, you know, the first get request is executed. There's no response to be sent back. That get request actually. But you will store the response in the, in the KV, the KV server will remember the response. So when it sees the duplicate, then we'll send the response.", "metadataJson": "{\"start\":5747,\"end\":5767}"}, {"text": "So there will be duplicate detection table that includes the response. Okay. Yeah, makes sense. Thanks. You're welcome.", "metadataJson": "{\"start\":5767,\"end\":5777}"}, {"text": "I was a little bit curious. I think I asked roughly this in my question, like, pre lecture question as well, like how raft compares to other like consensus algorithms in terms of optimizations you could do. And as an example, I was thinking the only thing I could think of was batching. And it seemed like raft is perfect for batching because the leader could just put more than one entry on its log, wait a little bit before sending its next entries, then send that offend entries with whatever batched set of operations or whatever it wants the replicas to do. So I was trying to figure out what are the deficiencies of rafters from a performance point of view.", "metadataJson": "{\"start\":5779,\"end\":5817}"}, {"text": "Well, Rev does not do batching. Right. And you know, maybe it could, but I would make the protocol more complicated. And so they decided not to, I guess. Okay.", "metadataJson": "{\"start\":5817,\"end\":5826}"}, {"text": "I guess the way I happen to have written my like raft lab code, like I sort of did batching implicitly because I would just wait a little bit sometimes before sending append entries. So I guess it felt like you don't really need to do anything other than just like maybe wait if you feel like it before. Yeah, you can have multiple. Uh huh. So yeah, I guess like, yeah, I think it all comes down to performance.", "metadataJson": "{\"start\":5826,\"end\":5853}"}, {"text": "So there's a whole bunch of optimizations that graph doesn't do. You know, that some other systems do. Like, you know, for example, you might be able to commute to your operations because it doesn't matter in what order you do them. So there's a slews of optimizations and raptors basically do none of them. Okay, thanks.", "metadataJson": "{\"start\":5853,\"end\":5872}"}, {"text": "And it might be perfectly fine for like the use case, correct? Right.", "metadataJson": "{\"start\":5872,\"end\":5875}"}, {"text": "You mentioned something a couple minutes ago, that log entry could be lost. So is it possible that like a client request could never be executed, but the draft guarantees that all the servers will execute the same set of lock entries in the same sequence. And so like that means draft is not, cannot be usable for like all applications if we can only if we can afford some lost requests. Well, you know, we have to assume that, you know, the responses from the, from the raft service as a whole. Correct.", "metadataJson": "{\"start\":5879,\"end\":5914}"}, {"text": "The KV servers plus raft may get lost anyway because the network gets lost. Network may lost responses. So the client has to be able to repeat. We call it must recent, must retry. I see, I see.", "metadataJson": "{\"start\":5914,\"end\":5929}"}, {"text": "So like when it actually commits a log entry and executes it, it replies to the client that it did actually do that. Yeah, yeah. So this is like we were a little bit earlier, we talked about this. The duplicate detection table. The duplicate detection table has the response that was sent or was constructed in response to executing that operation.", "metadataJson": "{\"start\":5929,\"end\":5947}"}, {"text": "Thank you.", "metadataJson": "{\"start\":5948,\"end\":5949}"}, {"text": "Any further questions?", "metadataJson": "{\"start\":5953,\"end\":5955}"}, {"text": "All good. Thank you so much. Hi, welcome. It's great to have so many questions.", "metadataJson": "{\"start\":5958,\"end\":5962}"}]}