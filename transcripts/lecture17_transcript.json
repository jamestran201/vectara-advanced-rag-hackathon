{"documentId": "lecture17", "section": [{"text": "So I want to talk today about Memcache. This is a paper from Facebook from 2013. Memcache is still widely used, and many websites or big Internet websites, you have ideas or architectures that are similar to it. The paper is an experience paper.", "metadataJson": "{\"start\":0,\"end\":22}"}, {"text": "So the goal of the paper is not so much to introduce new ideas or new concepts or new innovative ways of building systems, but, you know, it's more to report on actually practical experience in trying to build systems that in this particular case can support a billion requests per second or multiple billion requests per seconds. And so there are sort of three lessons, you know, that you can take away from this particular paper.", "metadataJson": "{\"start\":30,\"end\":57}"}, {"text": "One is they get very impressive performance out of building, out of building a system with off the shelf component.", "metadataJson": "{\"start\":62,\"end\":71}"}, {"text": "So the system consists of standard open software packages like MySQL, memcached, and they combine that together to actually build a system or scale that out to a system that can actually support in a billion requests per seconds. As you will see, as we see in this lecture, there's continuously a tension between performance and consistency.", "metadataJson": "{\"start\":76,\"end\":103}"}, {"text": "And as you will see in this paper here, the design is mostly driven by performance, but they want to provide some degree of consistency and that is sort of added to make the system at least be usable for the application that actually Facebook has. In fact, the consistency model is quite different from the consistency models that we've seen before. Most of the system we've talked about so far actually provides either external consistency or linearizability, very, very strong for consistency. And in the case of Facebook, their application don't really need linearizability. If a user is reading news articles and the newsfeed is a couple seconds behind, doesn't really matter.", "metadataJson": "{\"start\":108,\"end\":155}"}, {"text": "And so they have absolutely not the goal of purchasing providing sort of linearizability or strict consistency. So that's an important thing to keep in mind.", "metadataJson": "{\"start\":156,\"end\":166}"}, {"text": "Despite that, they're not shooting for strong consistency. There are some sort of cautionary tales in the paper that adding consistency measures is not easy if you sort of not have prepared for the start.", "metadataJson": "{\"start\":168,\"end\":184}"}, {"text": "But nevertheless, you really can't argue with the success of the system. It's clearly very successful and allows Facebook and websites that follow, or companies that follow similar strategies to actually scale to a large, large number of users.", "metadataJson": "{\"start\":186,\"end\":205}"}, {"text": "So my plan for this lecture basically is first to talk about performance, because really the performance is the driving force behind this design. And then at the end, talk more about consistency. Before jumping in though, let me know if there's any questions.", "metadataJson": "{\"start\":208,\"end\":223}"}, {"text": "Okay, so let me start with sort of a little bit of broader introduction to performance and basically, so talk about like website evolution.", "metadataJson": "{\"start\":230,\"end\":239}"}, {"text": "I'm sure many of you actually have built websites and if you sort of start out and you don't have any users, that's pretty straightforward. You buy a machine or random machine on Amazon or anywhere else and you just basically need three components. You need a web server. So let's say Apache, you need a sort of an application framework to build your website in. Maybe it's PHP, maybe it's Python.", "metadataJson": "{\"start\":245,\"end\":274}"}, {"text": "In the case of Facebook, I think they use PHP. And you need a database to actually store the data of your website. And so for example, you might use whatever MySQL as Facebook is doing. And so clients connect to your website, run whatever application code, whatever application service the website provides, and store and retrieve data using the database. The database provides transactions, it has sql, so it's easy to query over the data in different ways.", "metadataJson": "{\"start\":274,\"end\":314}"}, {"text": "And all the persistent state is short in the database. So you just have to back up the database and you basically have a good sort of fault tolerance plan. And that's sort of the, and for any website that's a small number of users this is completely sufficient. And the way many websites are built, however, when the number of users increases, you probably need to go to a much little bit more sophisticated design. So this is step one in the evolution.", "metadataJson": "{\"start\":314,\"end\":343}"}, {"text": "And in step two sort of try to address the first bottleneck that you will run into when you have a larger number of users. And typically the bottleneck that you will run into is basically the computation of the cycles being used by the application. So if you have thousands of users running at the website at the same time, or 10,000 or whatever number of users it is, and just running the application code on a single cpu or a single computer gets to drive space. When you do the cpu load up to 100% and then you can't support more. Fortunately, this is actually straightforwardly solved because the database actually has all the persistent state.", "metadataJson": "{\"start\":343,\"end\":391}"}, {"text": "So the typical way you solve this is you keep the one machine with the database or keep the machine on the database and then just buy a bunch of different machines for the front ends. And I'm just going to talk about the front end as one thing, which is typically the website Apache plus some application code. If you have more users you'll buy more machines.", "metadataJson": "{\"start\":391,\"end\":413}"}, {"text": "They all connect to the database to get their data. And actually this design works out extremely well because basically the frontends are stateless. All the state again is in the database. And so adding a new server is pretty trivial. All the front end sucks will see the latest writes because all the data is actually stored in the database, so there are no consistency issues.", "metadataJson": "{\"start\":420,\"end\":446}"}, {"text": "In terms of fault tolerance is easy. If one of these machines fails, no problem at all. Maybe the other machines have to take over the load or you have to bring on a new machine of mine. But you don't actually have to do anything in terms of complicated data restoration or restoration, because all the data is actually in the database. And so this is typically the first thing that happens as a website scales.", "metadataJson": "{\"start\":446,\"end\":473}"}, {"text": "Now, of course, if your website scales further and you need to support more than, say, for example, a simple MySQL setup can probably support 100,000 simple read transactions or simple read queries per second, probably thousands of write transactions. And so if the total number requests from your users actually goes over that hundred thousand, then you need a different plan. So then the next plan is typically sharding.", "metadataJson": "{\"start\":475,\"end\":510}"}, {"text": "And so far, this is all pretty standard. And so what you do is actually you take the storage machine and you split it in multiple machines.", "metadataJson": "{\"start\":518,\"end\":527}"}, {"text": "The front ends basically stay the same. We still have our many front end machines, and, you know, here we have our sharded database, and basically, you know, some range of the keys lives on whatever, maybe 1240 live on, you know, shark 142, whatever, 70 live on shark two and 70 to 100, you know, just to make stuff up. Lives in shark free. So basically, you take the tables in the database or the rows in the database and shark them by key. And so when the front end needs to know, of course, which databases there are, and even though it needs to get key 32 will go to shark one.", "metadataJson": "{\"start\":529,\"end\":581}"}, {"text": "If it needs to get to key 50, it will go to shark two.", "metadataJson": "{\"start\":581,\"end\":585}"}, {"text": "And so this gives us database parallelism.", "metadataJson": "{\"start\":587,\"end\":590}"}, {"text": "So, like, if every most requests actually are on two different charts, you know, basically, instead of actually limited by the one machine, we're actually getting the throughput of one machine, say, 100,000 times the number of machines that we have.", "metadataJson": "{\"start\":596,\"end\":612}"}, {"text": "And so that is typically the next step. And of course, this actually has. This step is a little bit more painful than the first step, because now you might actually have cross transactions if you need them, or if you want to avoid them, you got to group the keys that go together on the same machine. Otherwise you need some two phase commit type protocol. If you do transactions across sharks.", "metadataJson": "{\"start\":614,\"end\":643}"}, {"text": "This is the step from the first design two to design three is sort of a significant step.", "metadataJson": "{\"start\":643,\"end\":651}"}, {"text": "Now, if you grow further and further, you might think, well, you know, you could just shard the database further and further to a fewer queues of keys per server, but that actually increases the risk that you actually have to do crush shard transactions. So there's another way of going which is to observe that like, well maybe it's not really important that the database actually supports the reach. We can offload the reach from the database and basically the database only does the writes. Then maybe we can get a big performance gain. So that's basically the next common step that websites take.", "metadataJson": "{\"start\":654,\"end\":697}"}, {"text": "If this klopp is, and you know, it could be in the form of Memcache D or redis, sort of popular, you know, open source packages for caching. And then the basic plan is, you know, sell is roughly as follows. You have a lot of front ends as before, and we have a set of caches on the site. We'll talk a little bit about that in a second, a little bit more. In the case of case, you know, we are here, our caches cache layer, cache one, cache two, cache three, and in the case of Facebook, these are called, each individual server is called a memcached D demon and the whole cluster, or the collection of caches is called Memcache.", "metadataJson": "{\"start\":697,\"end\":759}"}, {"text": "And you know, there's still our database. Shard it across maybe multiple machines. This is sort of their storage layer.", "metadataJson": "{\"start\":763,\"end\":774}"}, {"text": "And sort of the idea is pretty straightforward. If front end needs to want to read a particular key, it first tries to cache and hopefully we'll hit in the cache. And so basically get a quick response back from the cache. If it's not in the cache, then it can retrieve it from the storage system and then install the data into cache and writes basically go straight to the storage server.", "metadataJson": "{\"start\":777,\"end\":811}"}, {"text": "And this sort of design, we'll talk about it much more in the details in a second. But this sort of design where you add a caching layer works extremely well for read heavy workloads. If you think about Facebook, it is going to be a whole lot of users and what they're doing is reading other people's posts, you know, looking at the timelines, you know, maybe watching, looking at pictures, reading the news articles, et cetera, et cetera. So it's a very heavily oriented workload, workload oriented to reads. And you know, in this case the reads are going to be all served from these caches.", "metadataJson": "{\"start\":817,\"end\":859}"}, {"text": "And these caches can be like dirt simple. And think about the caching, the key value server that you built in lab three, the key value server itself is actually nothing more than a hash table. Maybe you want to be a little bit smart about having locks per bucket. And so that you have a bunch of concurrency within the cache server itself or the key value server itself, but it is basically pretty straightforward.", "metadataJson": "{\"start\":859,\"end\":886}"}, {"text": "There are two challenges that come along with this, where the main challenge basically is how to keep the database and the CAASPP persistent.", "metadataJson": "{\"start\":890,\"end\":903}"}, {"text": "That's challenge one. And a lot of the paper is devoted to not talking about that. And the second challenge, which is also the main theme from the paper, is how to make sure that the database doesn't get overloaded.", "metadataJson": "{\"start\":909,\"end\":925}"}, {"text": "And the issue here is that once you scale up, say, to a billion requests per second by using caching, if any of the caches fail, that load will shift from the front ends, perhaps to the database. And of course the database is completely not designed to support that kind of workload and basically will fall over. A key challenge in the whole set of lessons that you learn from this particular paper is the techniques to basically avoid going to the database so that there's no risk that actually you overload the database.", "metadataJson": "{\"start\":935,\"end\":975}"}, {"text": "Okay, any sort of questions so far?", "metadataJson": "{\"start\":979,\"end\":981}"}, {"text": "Let me say a little bit about consistency because that will be, although I'm going to talk mostly about performance, it's going to be important to keep in mind even in this sort of section about consistency, about the performance. I had a quick question. Sorry. It goes back to having the state, like the clients be stateless. So what?", "metadataJson": "{\"start\":987,\"end\":1010}"}, {"text": "Yeah, there we go. On the second part of website evolution, why is it important for the clients to be stateless? That makes the replication easy, right? The clients don't actually, you don't replicate the data, so you don't have to keep the data consistent. All the data lives in one place, mainly at the database server.", "metadataJson": "{\"start\":1010,\"end\":1028}"}, {"text": "Okay. Yeah. So the idea is like any client can fail and it doesn't matter. Yeah, it doesn't matter. Computing.", "metadataJson": "{\"start\":1029,\"end\":1038}"}, {"text": "And you don't have to worry about actually keeping data consistent because data is only in one place. Like a lot of the things that we've been talking about this semester doesn't show up in this particular design.", "metadataJson": "{\"start\":1039,\"end\":1049}"}, {"text": "Okay. Okay, so back to sort of, once you do actually cache data, you do have this consistency issue, right? And so, you know, the interesting question is like, what is database, what is Facebook shooting for? And something typical, it's called, almost like it's called eventual consistency, which is a pretty vague term. Um, but basically, you know, maybe the contrast to this is to say it actually does not shoot for linearizability.", "metadataJson": "{\"start\":1053,\"end\":1082}"}, {"text": "And in fact, you know, what they're sort of shooting for is, you know, they do want, you know, right ordering.", "metadataJson": "{\"start\":1084,\"end\":1090}"}, {"text": "So writes are all applied in some consistent you know, total order so that you don't get weird, you know, going back in time problems. And that is all done basically by the database.", "metadataJson": "{\"start\":1094,\"end\":1104}"}, {"text": "So not really a big concern for the memcache layer itself. In terms of reads, it's okay if reads are behind, and that is really the property of the applications that Facebook wants to support. Again, the data that's in these caches is the data that their users actually consume, web pages, posts, timelines, friend lists and all that kind of stuff or status. None of that actually is really that important for users to see. A very up to date picture legs behind a little bit, one to 2 seconds, no problem at all.", "metadataJson": "{\"start\":1108,\"end\":1160}"}, {"text": "Certainly if it left behind for hundreds of milliseconds, they usually won't even notice. It's not perceptible, so it's okay to behind. Of course you don't want to be behind for hours. The user might actually notice, but for a little while behind, it's actually not a particular big deal. So they don't really shoot for strict linearizability where read observed the last write.", "metadataJson": "{\"start\":1161,\"end\":1185}"}, {"text": "If it's some reason write, that's fine. There's one exception to that, and which is that they do want to arrange that clients read their own writes and meaning that if one client updates a key k and then immediately reads later that key k, it is very desirable that that client actually does observe its own right, because it actually makes it more complicated to. Otherwise running complications would be even more complicated. So this sort of roughly what they're shooting for. And you know, this is quite a bit weaker than some of the models that we have seen before.", "metadataJson": "{\"start\":1186,\"end\":1236}"}, {"text": "And reminding me a little bit from the zookeeper sort of style of contract that it can provide.", "metadataJson": "{\"start\":1236,\"end\":1244}"}, {"text": "Okay, so one other thing I want to say, go back a little bit. So we need to keep the databases and the cache consistent in some manner. And so the basic plan that Facebook follows using the invalidation plan or cache invalidation plan, and we'll see later in the lecture why that is the case. Basically, what happens if the front end does write, it goes actually to the database. Here's MysQl, but they run on a, next to the database in another program, whatever it calls squeal.", "metadataJson": "{\"start\":1248,\"end\":1301}"}, {"text": "And basically it looks at the transaction log. So MySQL maintains a transaction log to implement transactions, and SQL looks like this transaction log sees what things get modified. And basically if there's a key that gets modified, so it sees like k gets modified, it will send an invalidation message to the cache, basically deleting it. Just issues a delete of that key k to the appropriate cache, and that way the data will be removed and then at some point later when a client comes along, does a read, it will get a miss in the, in the cache, reads and retrieves the data from, reads it from there. So here does get, let me call this a get, there's a read, gets the data from the read, and then actually installs it in the cache.", "metadataJson": "{\"start\":1305,\"end\":1366}"}, {"text": "And so one thing you might wonder, why actually does the application itself install the data into the cache so it doesn't put, and this has to do with actually that these caches, what they call are look aside caches.", "metadataJson": "{\"start\":1368,\"end\":1382}"}, {"text": "And the reason they're sort of look aside is that because typically what the application will do with the data that it actually reads from the database is maybe massage it a little bit and they'll do some computation on it. Maybe it'll take the text of the page and actually turn it into an HTML page or htmlify it, and then store the result of that HTML version of the page actually into the cache. Or maybe it reads a bunch of different records, aggregates some data, and puts the aggregated result in the cache. So the application is in control in this design of what to put in the cache. It puts a little bit more burden on the front end or in the application or the client in this case.", "metadataJson": "{\"start\":1384,\"end\":1423}"}, {"text": "But it has an advantage that you can do some pre processing before actually sticking something into cache. And this in contrast, where the cache would be transparent, where the cache would be sitting between the front ends and the storage server, and if you're missing the cache, then the cache would change the data. But of course the cache and the database don't really know what the application exactly wants to store in the cache. And so in the Lucas site design, this, the application is sort of in control of the cache.", "metadataJson": "{\"start\":1424,\"end\":1451}"}, {"text": "So in a little bit more detail we can look at this picture of like how actually reads and writes are implemented. So here's the reads. Oops, sorry.", "metadataJson": "{\"start\":1454,\"end\":1468}"}, {"text": "So this is figure two from the paper.", "metadataJson": "{\"start\":1479,\"end\":1484}"}, {"text": "And so here's our web server, that is our client.", "metadataJson": "{\"start\":1487,\"end\":1489}"}, {"text": "The client retrieves the k from the memcache, as we'll see in a second. Typically it actually will ask for a whole bunch of keys. There's not uncommon that the web server will ask for 20 to hundreds of keys. Presumably you're starting to compute some web page. The web page contains aggregates data from lots of different places, and for every piece of data that needs to be put into that web page, the client issues a get request we have made perhaps with many, many many keys that goes to Memcache.", "metadataJson": "{\"start\":1491,\"end\":1525}"}, {"text": "It gets results back. And when sending that gap to the memcache, it might contact many memcached servers. The results come back to the web server. If anything is missing, it can process the ones that actually returned the positive result. But we get nil back.", "metadataJson": "{\"start\":1525,\"end\":1543}"}, {"text": "Then the client does a select on the database and runs an SQL query that returns some data and the results of that. And the client might do some computation and then actually install the processed values that came back from the select into Memcache. That's the read site. Again here you can sort of see the Lucas site property or aspect of this design where Memcache is not really sitting straight between the web server and the database, but sits on the site and is managed by the client. So here's the right site.", "metadataJson": "{\"start\":1544,\"end\":1585}"}, {"text": "So for example, if the web server or the application needs to whatever, add a post or put a picture in the post or whatever, the server doesn't update, sends basically the update to the database. This is just performed like a normal transaction. And then of course on the database, on the site as we saw before, and we'll do invalidations using the squeal daemon.", "metadataJson": "{\"start\":1588,\"end\":1620}"}, {"text": "But that squeal beam operates asynchronously and so the client, the write doesn't really wait until that invalidation has happened again. Once the update in the transaction, once the update is done in the database, the transaction is completed, it will turn to the client and then in parallel the skual to actually SQL does the invalidations.", "metadataJson": "{\"start\":1624,\"end\":1655}"}, {"text": "And because the SQL does the invalidations asynchronously, the web server just to, as a precaution, doesn't delete of the key in the nandcache immediately. And so when, and the reason for that delete is only because we want to read our own rights.", "metadataJson": "{\"start\":1657,\"end\":1676}"}, {"text": "So when the web server, for example, looks for that key k right after it did the update, then it will miss in Memcache D and it will go and actually retrieve the new value and then install it. And without just a case where a webster immediately reached the key k that just actually updated a little while ago.", "metadataJson": "{\"start\":1688,\"end\":1710}"}, {"text": "Okay, when you're in principle it's not necessary to do this delete the invalidation at some point will happen and will kick out that KK out of the cache. And that's fine for basically other clients, but just with disk clients we want to make sure that actually it reads its own rights. I have a question. Yeah. So why doesn't it set after the delete?", "metadataJson": "{\"start\":1713,\"end\":1739}"}, {"text": "Yeah, that's a very good question, like why doesn't do the update immediately? Correct. And I think that, so that's called like an update scheme and that's in principle possible here too. But I think it's a little bit tricky for them to make work because I think there is going to require some cooperation between the database, the cache and the client. And I think the issue is as follows.", "metadataJson": "{\"start\":1740,\"end\":1763}"}, {"text": "Let's say we have a client c one, we have a client c two, and we'll see similar sort of type races showing up. Well let's say client x, one sits x to one and sends that to the database.", "metadataJson": "{\"start\":1764,\"end\":1778}"}, {"text": "And then so like now let's, so this is a hypothetical update scheme and the main point of this slide will be, or this board will be sort of talk about like doing action update is not completely trivial. Let's say client two at the same time. We're roughly after it says x to two, sends that to the database and let's say decline one has got a little bit delayed. And so we implement Uric scheme. Correct.", "metadataJson": "{\"start\":1781,\"end\":1812}"}, {"text": "And you know, we immediately do a set of k 22 and let me say k zero at the end, in the beginning. So this will update Matcache D. Correct. So the cache is now going to be have a value of, you know, whatever k 22. Then you know, client one actually comes around to do actually its set.", "metadataJson": "{\"start\":1812,\"end\":1834}"}, {"text": "So it will do a set here or put and set, you know, put oops sets k 21. And so this will overwrite the two. And now we have a stale value in the cache. And worse, you know, this value is there um, sort of persistently stable, any, you know, get later on, you know, we'll see actually the state value. And so this is not so desirable.", "metadataJson": "{\"start\":1834,\"end\":1867}"}, {"text": "And so you want to avoid that. And of course you can make maybe the update scheme work by for example, you know, ordering or time stamping or assigning a sequence number, studio updates and then by the database and then the key value server or the memcached could, you know, basically not perform updates that are out of order. But a scheme like that was going to require some participation of the database and required modifications through MySQL. And one of their goals was to actually build everything from off the shelf components. And so they prefer to go this invalidation scheme, which I think is just simpler to implement because basically the database, the only thing it has to do is this additional process data sits on the site and uses the standard delete operation that Memcache D already supports.", "metadataJson": "{\"start\":1868,\"end\":1921}"}, {"text": "Thank you. Does that make sense?", "metadataJson": "{\"start\":1924,\"end\":1926}"}, {"text": "And we'll see a similar issue like this one show up later. Again. Correct, because you remember from the paper there's some discussion about these tokens or leases to deal with state of values, but that's going to be as we'll see, stale values on the read site, or stale values that sort of have an interesting interaction between readers and writers, but can be solved totally in the context of memcached without actually making any database modifications. So why do we have a separate process to basically issue the invalidation? So this sigel, I think it was called.", "metadataJson": "{\"start\":1928,\"end\":1965}"}, {"text": "Yeah, so why, so like, why do we have this process if the front end itself will issue a delete k? Anyway, we'll see later on why this is going to be very useful. And particularly what we're going to do is we'll see is that the cache is going to be replicated and we need to set a new validation to every replica. Okay, I see. Thank you.", "metadataJson": "{\"start\":1965,\"end\":1986}"}, {"text": "It's not going to send a delete to every Memcache replica. The squeal. Yeah, I'll see you in a second. Hold on. And we'll see that in a second.", "metadataJson": "{\"start\":1988,\"end\":1998}"}, {"text": "In fact, I was going to go and talk about it right now.", "metadataJson": "{\"start\":1999,\"end\":2002}"}, {"text": "So far, actually, most of this story is pretty standard. Small changes here and what we've talked about so far is nothing really too exceptional. Things get more interesting right after this. And so we get more into sort of Facebook specific optimizations or performance tricks. And the one first thing that we're going to see go back.", "metadataJson": "{\"start\":2005,\"end\":2039}"}, {"text": "The first thing actually that sort of becomes unusual is that actually Facebook basically replicates a complete data center. At the time of the writing of this paper, there were basically two data centers. One on the west coast, switch back to blue. So data center one, they called regions, here's data center two. And they basically have, you know, they're all have a client layer.", "metadataJson": "{\"start\":2039,\"end\":2073}"}, {"text": "I'm just going to a lot of front ends, maybe this is the one on the west coast.", "metadataJson": "{\"start\":2073,\"end\":2085}"}, {"text": "And then, you know, there's the Memcache D or the Memcache layer. And both have their own memcache layer. So here's this front end, again, a lot of front ends. So here's our lot of memcached these, a lot of memcached these automached easier. And then, you know, there's the storage layer, which is our sort of sharded, you know, databases.", "metadataJson": "{\"start\":2088,\"end\":2115}"}, {"text": "So a lot of machines here too. And basically, you know, the data center two, the one on the east coast is a direct replica of the one on the west coast and the scheme that they use for rights. Because now we have two replicas really of the data. The data of the database is stored in two places. So we need to keep in some way these two copies up to date.", "metadataJson": "{\"start\":2118,\"end\":2144}"}, {"text": "And the basic plan, at least on the right side, is to all the rights are going through the primary. And one of the regions is the primary, the other is the backup region. So this is region two. And in fact, I think in the paper the west coast is the primary and the east coast is the backup. And so all writes actually go through the storage layer on the primary.", "metadataJson": "{\"start\":2145,\"end\":2173}"}, {"text": "So you can write issued by the frontends on the east coast. You know, go to the database here. And so the database there on the primary just runs the transaction and we know, and then basically propagates these invalidation methods. Or first of all, it takes the log that actually sits on this site and basically copies it or transmits it over to the other side. And so this is the squeal process that basically does that.", "metadataJson": "{\"start\":2173,\"end\":2200}"}, {"text": "And that process basically applies the log to the storage database on the other side. So keeping the two databases in sync. And as a side effect, it might actually send invalidation messages or delete messages to delayed keys.", "metadataJson": "{\"start\":2201,\"end\":2217}"}, {"text": "And so you might wonder like why do it in this way? Why not keep, for example, everything on the west coast and basically double the number of memcaches and all that kind of stuff? But the one primary reason to do this is this gets good read performance for users, good resource for users that are actually sitting on the east coast. So, you know, they will connect, you know, to one of these guys. They will look up the data in the cache, their memcache on the east coast and basically return the data straight out of the memcache.", "metadataJson": "{\"start\":2220,\"end\":2256}"}, {"text": "So we're basically going to get really good, you know, one, we can still get our good read performance. In fact, you know, we can also get low latency because we're basically, you know, reading from a replica that is close by. Of course, these caches might get a little bit more out of sync than an example there in the single data center because this whole update and invalidation, it all happens asynchronously.", "metadataJson": "{\"start\":2257,\"end\":2279}"}, {"text": "But that's more or less a little bit. Okay, correct. Because we already said that we're actually not looking for strict consistency or serializability. I have a question. So if, so if someone in the east coast, if a client in the east coast writes, it writes directly to the storage on west.", "metadataJson": "{\"start\":2285,\"end\":2309}"}, {"text": "Right? Yeah. Which doesn't invalidate. This guy also was invalidates to its cash. Oh, but we said, right, right.", "metadataJson": "{\"start\":2309,\"end\":2323}"}, {"text": "But we said like the client itself to read its own rights. Yeah. So where does the, where does this go? That of course goes through here, correct? Yeah, yeah, yeah.", "metadataJson": "{\"start\":2323,\"end\":2334}"}, {"text": "That makes sense. Okay. Okay. Okay. I've got a question to you.", "metadataJson": "{\"start\":2334,\"end\":2342}"}, {"text": "Do clients always talk? So will a given client always talk to the same memcache server? No, because going back a little bit earlier and we'll talk about this in a second because this actually is a problem as we'll see. So the front end basically talks. The keys are sharded across the memcache servers.", "metadataJson": "{\"start\":2343,\"end\":2367}"}, {"text": "Correct. And so like whatever key k, one k one lives in c, one k two lives in c two, et cetera, et cetera. And typically a front end, when it needs to construct a web page, it needs to get a whole bunch of keys. And so it sends actually these requests basically parallel to the different memcached keys and gets all the responses back.", "metadataJson": "{\"start\":2367,\"end\":2389}"}, {"text": "And so in fact the frontends are very likely to talk to every memcached in the system. I see. But for a given key it would always talk to the same server. Yes. Yeah.", "metadataJson": "{\"start\":2392,\"end\":2405}"}, {"text": "They actually happen to use consistent hashing. So if like if we'll talk a little bit about a second, a little bit more, but like one of the one memcached serves goes down. Correct. You know, can't talk to that one anymore. And so it might be over time that the assignment from sharks to servers will change a little bit.", "metadataJson": "{\"start\":2405,\"end\":2423}"}, {"text": "Sorry. Actually just a follow up on that. So the requirement for clients to read their own rights is kind of like a weak guarantee, right. Because if the server that it deletes from goes down and then it has to read from a different replica, it might end up not reading its right. If in the presence of a failure.", "metadataJson": "{\"start\":2425,\"end\":2445}"}, {"text": "Hold on, hold on. Hold that thought for a little while. Okay. There will seem to number of couches or races, if you will. And they have different techniques for solving those races.", "metadataJson": "{\"start\":2445,\"end\":2454}"}, {"text": "Sorry, final question. Yeah, final.", "metadataJson": "{\"start\":2455,\"end\":2458}"}, {"text": "So we're doing for read our own writes, we make sure that we go directly to the storage servers right after a write, correct? Yes, validating the cache. But you also said. No, no, hold on, hold on. When you do a write, you do the update in the database.", "metadataJson": "{\"start\":2463,\"end\":2485}"}, {"text": "Then you delete the k from your system. So example in this particular case you would do a write to the primary, delete the key k from your local cache. So when the next time you do a get, you're gonna read from the storage server again, right? Exactly. Yeah.", "metadataJson": "{\"start\":2485,\"end\":2507}"}, {"text": "But I was curious. So you also said the write to storage happened asynchronously, right.", "metadataJson": "{\"start\":2507,\"end\":2516}"}, {"text": "In this replication happens asynchronously and invalidations happen asynchronously, not the writes. Okay. The writes are synchronous, so you do the delete after you finish the write. Okay, great, thanks.", "metadataJson": "{\"start\":2519,\"end\":2531}"}, {"text": "So if you do a write and you're from the, so you're not, you're not from the primary region, you do a write to the primary storage and then you invalidate your memcache and then do a read, but you do the read from your storage and maybe your storage isn't up to date yet, so you got a race. And so we'll see how they solve that. Okay, that's correct. But first, let's talk more about performance because this is not good enough yet. They want more performance.", "metadataJson": "{\"start\":2534,\"end\":2566}"}, {"text": "And so, you know, if you sort of, broadly speaking, there's sort of two strategies for getting performance and just tapping back a little bit.", "metadataJson": "{\"start\":2568,\"end\":2578}"}, {"text": "And we already seen them a little bit in a very high level. So there are two basic plans. One is to partition or shark, and that's very cool because in fact, we see that it's being used basically both on the storage layer and the memcached layer. And so if you need more capacity, you should buy another server, change the hashing function, and suddenly you got more capacity in your memcached and you can hold more data and that data can be accessed in parallel. So you get a lot of capacity plus in the capacity plus in the parallelism side.", "metadataJson": "{\"start\":2585,\"end\":2635}"}, {"text": "But, you know, if you have a particular key that's extremely hot, like a lot of clients actually need to get that key, you know, whatever a particular person on Facebook who has a timeline that, you know, everybody's, you know, following, then you know, that key is going to hit a lot and it's being served, you know, luckily in this case being served maybe by two different servers, one in the west coast and one in the east coast, but presumably a lot of clients on the east coast and in the west coast, we're going to hit the same. Or the two, the memcached server on the west coast and the memcached server in East coast that hold that key, that's not going to be that good because that single server might actually get overloaded. And it turns out the key distribution varies widely. That's not so good to solve problems like that. The second approach is to replicate, replicate data.", "metadataJson": "{\"start\":2639,\"end\":2693}"}, {"text": "Here's partition data and replicate data. That's great for hotkeys. You can take the same key, replicate it on a bunch of different memcached servers. Then the clients that all meet that key can be spread across those memcached servers and get the keys basically in parallel. That works actually good for hotkeys.", "metadataJson": "{\"start\":2694,\"end\":2718}"}, {"text": "It doesn't really increase your capacity, right. So you just takes more. And in some ways we can see that in the previous picture. Again, we have replication in action here. Correct.", "metadataJson": "{\"start\":2720,\"end\":2731}"}, {"text": "We have replicated one data center from the west coast completely to the east coast that hasn't introduced, increased the total capacity for the mancache. These, because both the mancast layers store stored the same amount of data. You didn't increase the capacity of the memcache layer, but you're allowed to now read from these two different memcached layers on the east and west coast in parallel. We see a little bit of a form of replication going on and you might wonder what else is left to be done. This comes to this question that was asked a little bit earlier.", "metadataJson": "{\"start\":2731,\"end\":2769}"}, {"text": "Let's say you need more capacity now. So well, in one, one solution to more capacity even in a single data center. So forget that there's two data centers. Just look for think from the perspective of a single data center. We want more capacity.", "metadataJson": "{\"start\":2769,\"end\":2781}"}, {"text": "Well, one option correct would be to whatever, just buy more memcached servers and just keep buying more of them. And that turns out to be slightly problematic. And one reason that is problematic is because these front ends talk to basically every Memcache server. And so they almost, at least for rights, we know that they have TCP connections open and so there's a large number of TCP connections. And furthermore, and as we said before, if actually there's a particular key hit hard, then that doesn't really solved by sharding.", "metadataJson": "{\"start\":2781,\"end\":2829}"}, {"text": "So you can buy more machines. But if that one key is hot and lives in one machine, that actually is not going to really improve your performance. So the next step in terms of performance improvement is to actually replicate within, inside a single data center more performance. This is this idea of clusters and this is really the story about replication.", "metadataJson": "{\"start\":2829,\"end\":2858}"}, {"text": "And so what they actually do is like if we look in a single data center, we got our storage layer, and then within the storage layer, basically we're going to replicate a set of frontends. So here's our front end layer and here's our memcache layer.", "metadataJson": "{\"start\":2860,\"end\":2885}"}, {"text": "We're going to take that and just replicate it multiple times.", "metadataJson": "{\"start\":2888,\"end\":2893}"}, {"text": "And then we'll call this a cluster.", "metadataJson": "{\"start\":2895,\"end\":2897}"}, {"text": "And the reason this is good is this actually deal well with it's. Good for popular keys.", "metadataJson": "{\"start\":2900,\"end\":2908}"}, {"text": "Public key will now be replicated potentially in multiple clusters. So that is nice. Second, introduces the number of connections.", "metadataJson": "{\"start\":2911,\"end\":2922}"}, {"text": "And this is actually particularly, there's a multiple reasons why this is important. It avoids what they call the in cast problem, in cast congestion.", "metadataJson": "{\"start\":2928,\"end\":2941}"}, {"text": "As I said before, like one of these front ends may have to retrieve, you know, 500, you know, whatever, 100, you know, tens to 100 of keys. And so it will send them in parallel to all the particular memcaches that are important, and they will all respond. And of course we have many, many more memcaches. We're going to have much more parallelism. A lot of packets will come back exactly at the same time that can easily read into queue queues being overloaded or queues being full, and therefore packets getting dropped by reducing the number of connections, not actually every friend that talks to reduces the number of responses, are going to come back and we avoid this in cast congestion problem.", "metadataJson": "{\"start\":2944,\"end\":2989}"}, {"text": "And in general, it sort of reduces the pressure on the network. It's actually hard to build networks that have a bisection bandwidth that can sustain a huge load. And here by using replication, basically the network for one cluster really has to support that one cluster. Well, now, so this is all good. Of course, this is the downside of a design like this, is that if you have unpopular keys, there's unpopular keys going to get stored in multiple regions and basically do nothing or don't really contribute to improvement in performance.", "metadataJson": "{\"start\":2990,\"end\":3035}"}, {"text": "And so in fact, you know, what they do is they have one additional sort of pool that they have, and they call this the regional pool.", "metadataJson": "{\"start\":3035,\"end\":3043}"}, {"text": "And applications can decide to store not so popular keys into the regional pool to just stick it at the site. And so these, so that they don't, are replicated in times across all clusters. So you can think about a regional pool being shared among multiple clusters and used for the less popular keys or less infrequently used keys. Okay, so this is going to help with popular keys because each cluster is going to have its own memcache. Yeah.", "metadataJson": "{\"start\":3045,\"end\":3086}"}, {"text": "Every cluster has its own map cache, has its own front end, it has its own memcache.", "metadataJson": "{\"start\":3087,\"end\":3094}"}, {"text": "And basically users, you know, the users are basically load balance across all these clusters. But this still does not increase capacity, right? Does not increase capacity. If you want to increase capacity, you, well increase capacity a little bit. Correct.", "metadataJson": "{\"start\":3097,\"end\":3120}"}, {"text": "Because like all the unpopular stuff is not being extra cached and it's stuck in the regional pool. Okay. And so that space is now free to actually store other keys.", "metadataJson": "{\"start\":3120,\"end\":3128}"}, {"text": "So to avoid in cas congestion, they would also reduce the number of shards per cluster. Right? Yeah. Or, you know, don't, don't grow it. The alternative plan.", "metadataJson": "{\"start\":3134,\"end\":3145}"}, {"text": "Correct. Was not to introduce clusters, but basically keep growing. The memcaches sharks the number of shards in a single memcache. And, you know, that has its own limitations. Makes sense.", "metadataJson": "{\"start\":3145,\"end\":3160}"}, {"text": "Thank you.", "metadataJson": "{\"start\":3160,\"end\":3161}"}, {"text": "Okay, well, so this is sort of the base design, except there are sort of all kinds of performance challenges that they had to resolve. Most of these perform challenges really have to do with, I think, the way to think about it is protecting the database.", "metadataJson": "{\"start\":3166,\"end\":3184}"}, {"text": "So go back to this picture. Correct. We have now a design that apparently can support billions requests per second, but the storage layer itself is sharded because certainly not sustain billions requests per second. It would be a disaster if, for example, let's say all the memcaches failed in some way or another where a whole cluster failed and all the front ends would hit the storage servers, then the storage servers would fail over, couldn't handle that kind of load. And so they got to be very, very careful with actually doing anything that requires more load than the storage servers.", "metadataJson": "{\"start\":3186,\"end\":3243}"}, {"text": "So one, for example, challenge I'm going to talk about a number of them is to bring up a new cluster.", "metadataJson": "{\"start\":3245,\"end\":3250}"}, {"text": "The easy way to bring up a new cluster would be just to build the cluster, turn the machines on, install the software and be done, and basically rely on the fact that if the data is not in the cache, you will have a miss, and the miss will go to the database and actually collect the necessary data. And, you know, what is the problem with that kind of design?", "metadataJson": "{\"start\":3255,\"end\":3281}"}, {"text": "It's going to have a lot of cache misses because there's nothing in the cache. Yeah. For example, let's say you had one cluster and you entered the second cluster and you moved half of your users to the second cluster. Then 50% of your requests are going to miss in the cache and they're going to hit the database and the database will fall over. Correct.", "metadataJson": "{\"start\":3285,\"end\":3306}"}, {"text": "So how do, how do they deal with this?", "metadataJson": "{\"start\":3306,\"end\":3309}"}, {"text": "Gutter? No, not the gutter. This is the, I think they were making the new cluster read some entries from the cache of an old cluster. Yeah. Correct.", "metadataJson": "{\"start\":3311,\"end\":3325}"}, {"text": "So gets in a new cluster, if they miss in the new cluster, they go to the old cluster from an existing one and then they set in the new cluster. So basically, one way to think about is they fill up a new cluster or warm up a new cluster by reading from an existing cluster. And so that maybe increase the load on an existing cluster a little bit. But at least with Boeing, Dex putting a lot of pressure on the database. And as we'll see in a second, that also introduces again some consistency issues and we'll see that a little bit later.", "metadataJson": "{\"start\":3325,\"end\":3368}"}, {"text": "That's one example of performance challenge that you addressed. The other performance is a popular term used in many contexts that are called the fundering hurt problem.", "metadataJson": "{\"start\":3370,\"end\":3382}"}, {"text": "What's the funding hurt problem?", "metadataJson": "{\"start\":3385,\"end\":3387}"}, {"text": "I guess when there are a lot of writes and reads approximately at the same time, and because there are a lot of writes, the data will be invalidated many times and the database will be assaulted with requests. Yeah, you can make it even simpler. Like a single write cause an invalidation of a key, correct. And anybody, any client that reads the key right after it. So you could have the following situation.", "metadataJson": "{\"start\":3391,\"end\":3420}"}, {"text": "You have a very, very popular key. You invalidate the key, so you delete the key from your cache. All the machines or all the front ends that need that popular key will do a get on that key, all get back nil. And then they all want to like re select from the database. Correct.", "metadataJson": "{\"start\":3420,\"end\":3440}"}, {"text": "And that, you know, might cost, you know, the, puts a lot of pressure on the database. So they want to avoid that problem. And so how do they, how do they avoid that problem? They use leases.", "metadataJson": "{\"start\":3440,\"end\":3454}"}, {"text": "Go ahead, say more. Oh yeah, yeah. I think like they gave like a, like a time, like for key specific for the user and then some time what I understood, it was kind of a lock. And then if another user tries to use it, they will wait. And then hopefully it would be updated fast enough so that in the next retry they will get it.", "metadataJson": "{\"start\":3456,\"end\":3480}"}, {"text": "Yeah. So basically if you do get and you get nil back, you get two situations. Either you got a lease. The first client basically that does a get and misses, gets a lease from Memcache D and that Memcache D. That lease basically gives you the right to do an update or tells the client like, you're responsible for doing the update.", "metadataJson": "{\"start\":3480,\"end\":3502}"}, {"text": "And if you don't the first one, then you get basically a retry message or result. And that basically tells the client like you should retry it soon and not immediately and maybe spread around a little bit. They probably do some binary backup type style thing and retry to get, and most cases the client that the first client that missed will have updated the key k reasonably soon, like in the order of milliseconds. And then these retries actually will succeed. And there's no really, there's no explosion on the number of requests to the database.", "metadataJson": "{\"start\":3503,\"end\":3544}"}, {"text": "With this scheme, of course, it introduces, as we'll see in a second, more race conditions. But first, let's keep focusing on performance.", "metadataJson": "{\"start\":3544,\"end\":3555}"}, {"text": "There was another thing about leases where they addressed stealth sets. Yeah. So leases form two roles as we'll see in a second. One for consistency and one for performance. This one is for performance.", "metadataJson": "{\"start\":3558,\"end\":3573}"}, {"text": "And so we'll talk about consistency in a second and then we'll see the second reviews as one way to solve one of these race conditions.", "metadataJson": "{\"start\":3575,\"end\":3581}"}, {"text": "Okay, one more. There are many more in the paper, but just one more, that's sort of interesting. At least I find interesting. What happens if they have a memcache or Memcache server failure?", "metadataJson": "{\"start\":3586,\"end\":3599}"}, {"text": "It depends if it's the whole data center that the whole collection of cache servers that failed. Just consider a handful that someone mentioned before. Set the memcache, the failed Memcache, but they don't delete them. Yeah, so the scenario. Correct.", "metadataJson": "{\"start\":3608,\"end\":3632}"}, {"text": "The problematic scenario is like a memcached server fails that will result in a bunch of misses. Those misses will hit the database and they want to avoid hitting the database. Any client that actually has a couple of keys in those servers is going to try to retrieve the key will fail and then have to do something. So when a get fails, the easiest solution is to go to the database, but we want to protect the database. And so that doesn't seem to be a great idea.", "metadataJson": "{\"start\":3632,\"end\":3664}"}, {"text": "And so what they do is actually they have a small other cluster or another pool, like the regional pool they call the gutter pool.", "metadataJson": "{\"start\":3664,\"end\":3671}"}, {"text": "And the gutter pool is basically sort of a handful of mancast machines that is just available and they're available for the short period of time that the system reconfigures and repairs itself and adds new memcached servers to replace the ones that failed. But in that period of time, that's when you order minutes or maybe a little bit more. They don't want the get request or the selects to go to the database. Instead, what they do when the get fails, you go try first the gutter pool, and the gutter pool will, the first one that hits the gutter pool will fail or miss. Do the select in the database, get the results, stick it into the gutter pool, and then subsequent request gets will actually then be answered from the gutter pool.", "metadataJson": "{\"start\":3674,\"end\":3724}"}, {"text": "And at some point the memcached machine that was failed has either been replaced or replaced with another machine or recovered, and then the load shifts back to this Mancache d server and the gutter pool sits again in the site to carry over between these traditional pair kills. Okay, so this sort of gets us to the, I guess the reading question for today, as you just mentioned, the gutter polls, you don't do a delete from the gutter pool. Invalidations are actually also not sent to the gutter pool. And the question is like, you know, why? Or can we speculate on why?", "metadataJson": "{\"start\":3724,\"end\":3763}"}, {"text": "And so maybe this is a good time for a quick breakout room, a couple minutes, you know, to either discuss other aspects from the Mancash de design that you want to discuss, or try to figure out what the answer to that question is. So maybe we can do a breakout. Yes. Thank you, Lilly.", "metadataJson": "{\"start\":3763,\"end\":3778}"}, {"text": "It it it.", "metadataJson": "{\"start\":3911,\"end\":4040}"}, {"text": "Okay, is everybody back?", "metadataJson": "{\"start\":4143,\"end\":4144}"}, {"text": "Yep, looks like it. Yep. Okay, good. Okay, so anybody, you know, the paper doesn't answer this question very precisely, but anybody wants to dare to speculate what the answer is? The deletes, no deletes or no invalidations to the gutter cluster.", "metadataJson": "{\"start\":4150,\"end\":4166}"}, {"text": "We said that. Go ahead. What we said was something like, if you do, then it would have a lot of pressure on the gutter pool because there are so few machines and for every cache miss, there are two requests and for a cache header, just one. So if you do that after every write, you would have an extra request on the gutter pool. And it's so small, so you don't want to do that.", "metadataJson": "{\"start\":4168,\"end\":4193}"}, {"text": "And also you would protect the database as well because you would constantly query it after a write request. Yeah, exactly. I think in general, the delete messages also will have to go to two pools. Correct. The original memcached deep pool, all the memcached deep pools that need to be validated into the gutter.", "metadataJson": "{\"start\":4193,\"end\":4211}"}, {"text": "And so, you know, also double the delete traffic. So I think that's a perfectly, I think that's the reason, you know, it's a small set of machines. It's just there to sort of over, basically get you through that transformation from a deleted memcached server, failed memcached server to a new memcached server. Good. Okay, so I want to say about performance, even though there's more in the paper about performance.", "metadataJson": "{\"start\":4211,\"end\":4239}"}, {"text": "Instead, I want to talk a little bit about sort of these races that sort of come about because of this trying to achieve the high performance that we've been talking about. They're going to be three razors that I want to talk about. And in fact, I think all three you already identified. And so, amazingly, most of the discussion is presumably going to be about how they avoid them. And so raise one is what they call stale sets and scenarios.", "metadataJson": "{\"start\":4240,\"end\":4270}"}, {"text": "Swallows. We have a client one, just one region or one cluster, nothing particularly special setup. So the client one does get k, you know, that turns out to get a nil. In the scenario it will read the value from the database. Maybe this is the client that actually got the token and it's the one that actually is allowed to set it.", "metadataJson": "{\"start\":4271,\"end\":4299}"}, {"text": "But before it actually gets to set, another client comes in and write, you know, k is two to the database. And then there's a put of k on two.", "metadataJson": "{\"start\":4300,\"end\":4313}"}, {"text": "And then, you know, the other client, you know, actually finally gets around doing actually it's put. So it does a put k comma and this is like maybe v one. So k v one. And now we have a stale value in the cache and that, you know, state value is sort of permanent there until somebody else does an update. Okay.", "metadataJson": "{\"start\":4316,\"end\":4342}"}, {"text": "And that's sort of undesirable. Right. That really breaks their sort of contract with the applications and they don't want to go back in time, you know, would be anonymous, that actually the user could observe. And so they want to sort of try to avoid that. And so what, how do they solve this problem with uses?", "metadataJson": "{\"start\":4342,\"end\":4363}"}, {"text": "Yeah, some say the leases help out here. They already have a lease. Correct. Because this guy got a lease for must have gotten lease because otherwise he's not reading from the database. And so at the client, one presents this lease at the put or can put lease at the put.", "metadataJson": "{\"start\":4364,\"end\":4384}"}, {"text": "And in fact it will. And what is the additional step, basically to check that the lease hasn't expired or something? Because if the other client was able to. Oh yeah, sorry, I just realized I made a mistake. So this is why the question is also not so good.", "metadataJson": "{\"start\":4385,\"end\":4408}"}, {"text": "Let me see. Client two doesn't do a put. Correct. That was an invalidation consistency. I got myself confused here.", "metadataJson": "{\"start\":4409,\"end\":4415}"}, {"text": "So what does the client actually decline to do after it sets the database going back to delete? Yeah, it does a delete for the reason that we talked about earlier. Correct. So it doesn't delete of k. And what's the side effect of what happens with the lease of key that's being deleted?", "metadataJson": "{\"start\":4415,\"end\":4436}"}, {"text": "It doesn't like verify or it doesn't, yeah. Actually what happens at the site as a side effect of delete? The lease is invalidated. So it invalidates the lease.", "metadataJson": "{\"start\":4439,\"end\":4454}"}, {"text": "And so when the boot comes along. So my timeline is a little bit, you know, this happens like well before. So the put happens after the delete. The put will present the lease that it got at the get, but that lease has been invalidated by the delete. And so this put gets rejected.", "metadataJson": "{\"start\":4457,\"end\":4476}"}, {"text": "So basically one way to think about this, is that they leverage, you know, the lease mechanism to avoid the funding hurt problem they extended to basically also avoid this stale set problem.", "metadataJson": "{\"start\":4481,\"end\":4495}"}, {"text": "Does that make sense?", "metadataJson": "{\"start\":4498,\"end\":4499}"}, {"text": "Even if we don't have this lease invalidation mechanism, we would still obey the, like the weak consistency that you would have ordered rights. That happened at some point in the past. But I believe that the thing that this thing ensures is that you observe your own rights. Right. That also ensures that you don't go back in time.", "metadataJson": "{\"start\":4503,\"end\":4522}"}, {"text": "Right. Like if you read something and like everybody else that comes now after this client too, will see, you know, the old v one. And so a client might get well behind and not see that new write for a long, long period of time. In fact, might see not at all. Yeah, but I mean, would this be actually going back in time?", "metadataJson": "{\"start\":4523,\"end\":4544}"}, {"text": "Because clients did not read anything after. Maybe back in time is the wrong word, but it won't observe v two for a long, long time. I see. Okay. That was not something that we wanted to happen.", "metadataJson": "{\"start\":4544,\"end\":4557}"}, {"text": "It's okay behind a little bit, but not, you know, for a long, long time. Okay. Second race, which you guys already mentioned, already identified two. Advantage of many, I guess, lab debugging, you know, know all about races. Race two.", "metadataJson": "{\"start\":4558,\"end\":4579}"}, {"text": "And this is the cold cluster race.", "metadataJson": "{\"start\":4579,\"end\":4581}"}, {"text": "And sort of in a similar style. We have two clients, client one with client two.", "metadataJson": "{\"start\":4585,\"end\":4593}"}, {"text": "And let's say k is v one originally. And so we're in both clients or in the cold cluster line one sets the key to a new value in the database, deletes the k in the cold cluster. Right. In the current cluster that's actually in. And then, uh, this client does get in the cold cluster, sees that it actually is not there, and it's going to do get from the warm cluster, get the value back, maybe that, you know, get actually gets there before actually the, uh, the cold cluster or the warm cluster actually has been updated.", "metadataJson": "{\"start\":4597,\"end\":4655}"}, {"text": "And so now it will do a set of the k to v one or put, sorry, let me be consistent. Put of k to v one in the cold cluster. And now we have sort of the same situation as before where we have sort of a permanent scale value in the cold cluster.", "metadataJson": "{\"start\":4656,\"end\":4681}"}, {"text": "And how do they solve that problem?", "metadataJson": "{\"start\":4686,\"end\":4689}"}, {"text": "Anybody remember?", "metadataJson": "{\"start\":4695,\"end\":4696}"}, {"text": "So they have a small extension that avoids this problem. Any guesses what it could be if you.", "metadataJson": "{\"start\":4707,\"end\":4713}"}, {"text": "What's a c one in the warm cluster or cold cluster? No, both in the cold clusters.", "metadataJson": "{\"start\":4718,\"end\":4723}"}, {"text": "Oh, and they're different. They're in different called clusters. Right. I'm not sure that matters. Yeah, actually it doesn't.", "metadataJson": "{\"start\":4726,\"end\":4736}"}, {"text": "Anybody?", "metadataJson": "{\"start\":4769,\"end\":4770}"}, {"text": "I think they mentioned like the hold off for 2 seconds, though I'm not entirely sure about all the details of that.", "metadataJson": "{\"start\":4792,\"end\":4798}"}, {"text": "Yeah, basically this actually causes, they put a hold, they call this a hold off, a two second hold off on any sets to that key. So this particular, after you do a delete in the cold cluster, you can't do any sets to that key for 2 seconds. And so this particular put will be rejected.", "metadataJson": "{\"start\":4801,\"end\":4829}"}, {"text": "And this is only during the warm up phase. Correct. So when the cluster comes up, it's cold, you know, for a couple hours, it runs, you know, to start warming up and get its content in place. And once you know, sort of warmed up, then, you know, they stop doing this trick, but basically sort of to, you know, this pace, this problem over. They think that 2 seconds is sufficient and that's sufficient for basically that.", "metadataJson": "{\"start\":4831,\"end\":4858}"}, {"text": "Right. To propagate to the cold database too. Okay, good. There's one more. Right problem.", "metadataJson": "{\"start\":4858,\"end\":4871}"}, {"text": "Let me quickly mention that, because again, you already mentioned it. So race number three that they talked about in the paper, and I'm sure there are more, but you know, the one they say, the one that I talked in the paper about, and this is between regions, and it has to do with the primary and the backup, frame rate backup. And it's sort of a similar problem.", "metadataJson": "{\"start\":4871,\"end\":4893}"}, {"text": "And where the client, one does write to the database, to database. And this is a client in the backup. So this is a backup client sits in the backup region, so it doesn't write to the database in the primary region. So it goes off and then it does delete of the k in of course the backup region from its cache. And then in principle, if we do immediately, and this is like one of you mentioned this, like if you immediately do get that particular k right, then it won't see the will fetch from the, you know, it won't see actually the result of that write.", "metadataJson": "{\"start\":4896,\"end\":4950}"}, {"text": "So it won't see its own rights because that write is still on the way to the backup or to the primary. The primary will, you know, send through the SQL thing, squeal thing, and propagate the update to the database in the backup area. And so only then, you know, again, in the backup, they'll actually, in the backup area in the back of region, we'll actually see the K change. And so we are, so we have a problem here, correct, where if this k would proceed without any modifications, then we would see not our own rights. And anybody remember how they solved this problem?", "metadataJson": "{\"start\":4951,\"end\":4990}"}, {"text": "Was this the remote marker? Yeah, absolutely it is. So when the delete is k key. They keep it in the memcached of the backup and mark it as remote.", "metadataJson": "{\"start\":4994,\"end\":5007}"}, {"text": "And so when the client one does a get, it will see, hey, I'm going to get a basically gets remote back from its local memcache and then it knows basically to fetch it from the primary, from primary region.", "metadataJson": "{\"start\":5010,\"end\":5029}"}, {"text": "Okay, but then the remote marker will be removed when it's safe to read from the backup. Yes, I think when the database, the backup database gets the data from the primary, it can remove the marker because then it's safe to read it from the primary database, from the backup database.", "metadataJson": "{\"start\":5034,\"end\":5061}"}, {"text": "Does that make sense?", "metadataJson": "{\"start\":5064,\"end\":5065}"}, {"text": "Okay, so let me do a quick summary because I'm running a little bit over time. So quick summaries, you know, caching is vital.", "metadataJson": "{\"start\":5067,\"end\":5079}"}, {"text": "We basically get, you know, this kind of capacity that we're talking about this paper like billions of operations per second. There's sort of two strategies to sort of get this high capacity. One is partitioning, which gives you parallelism or sharding.", "metadataJson": "{\"start\":5081,\"end\":5100}"}, {"text": "And the other strategy is replication, which is really good for hotkeys, keys that are being requested by lots and lots of clients so that the keys get replicated on multiple machines.", "metadataJson": "{\"start\":5103,\"end\":5115}"}, {"text": "And we also see there's sort of a bunch of almost ad hoc techniques to sort of get around some of the serious consistency issues that pop up, even if the system is only designed to give weak consistency. And so this whole sort of consistency between the database, between DB and caches, memcache is tricky, maybe much more tricky than you might have thought, because memcache is just a cache. What could be the problem? But as you can see, you know, it's actually pretty tricky. And in fact, you know, there's quite a bit of research going on trying to figure out like how could you do better?", "metadataJson": "{\"start\":5118,\"end\":5162}"}, {"text": "Okay. With that I want to conclude so that people that need to run can run or go to their next Zoom meeting. And. But I'll stay around and answer any questions if you have any questions remaining. And otherwise, I'll see you first.", "metadataJson": "{\"start\":5162,\"end\":5175}"}, {"text": "Thank you.", "metadataJson": "{\"start\":5177,\"end\":5178}"}, {"text": "Sorry, I have a question about. So for example, for the last thing we talked about with the remote marker, how did they know that this is going to be a relevant database or how did they decide that it is going to be more useful to do this additional steps of remote marker versus just getting stable data? Well, I think that is because they have this requirement right up front, although the paper didn't really stipulate it very clearly, they really want this.", "metadataJson": "{\"start\":5182,\"end\":5217}"}, {"text": "Like for example, you do a user add something to their timeline read it again and it's not there. So that is a thing that could just be observed directly by users. Strange inconsistency. They want to avoid that.", "metadataJson": "{\"start\":5219,\"end\":5237}"}, {"text": "Okay, that makes sense. That makes sense. And my other question was on the, one of the first slides where you had invalidation of the memcache. Let me find where I had that.", "metadataJson": "{\"start\":5239,\"end\":5251}"}, {"text": "A year. Well, yeah, this is the invalidation light. A little bit wild now, but. Oh, no, it was one of the later ones. This also has invalidation on it, or no, it isn't.", "metadataJson": "{\"start\":5255,\"end\":5268}"}, {"text": "Maybe the next one. Yes, you're. Oh, yeah, yeah, yeah. So the client is going to set the invalidation only for its local region, and the squeal is going to do it for the transfer and for the non local. Yeah.", "metadataJson": "{\"start\":5270,\"end\":5290}"}, {"text": "Okay. That makes sense. Thank you so much. You're welcome, professor. Add two.", "metadataJson": "{\"start\":5290,\"end\":5299}"}, {"text": "You had your final question.", "metadataJson": "{\"start\":5299,\"end\":5300}"}, {"text": "Sorry. No, go ahead. Please ask more questions. These are after class. They don't count.", "metadataJson": "{\"start\":5303,\"end\":5311}"}, {"text": "So first. So servers in, um, in a region are assigned. When we have clusters, each one are assigned to a cluster, right? Yeah, yeah. So every, every cluster is really a replica.", "metadataJson": "{\"start\":5313,\"end\":5327}"}, {"text": "Okay, nice. Yeah. And, like, like, servers are assigned to one single replica. Yeah, yeah, exactly. Nice.", "metadataJson": "{\"start\":5328,\"end\":5336}"}, {"text": "And then the second one was, like, straight from the paper and very precise. But it says, like, okay, here in, I think, generic cash. In page two, it says, like, memcache is a more general evalu store. And in particular, they say it takes little effort for new services to leverage the existing marcher infrastructure without the burden of tuning, optimizing, provisioning, and maintaining a large server fleet. I wasn't sure, and I looked up and I couldn't find what, like, what's existing marcher infrastructure?", "metadataJson": "{\"start\":5336,\"end\":5374}"}, {"text": "I don't actually know exactly what they're referring to, so. Okay. All right, cool. Thanks. Yep.", "metadataJson": "{\"start\":5375,\"end\":5382}"}, {"text": "See you. I wanted to follow up on a question that I think William asked about a certain failure mode. If cache d server fails. Um, I think there's a die. I'm trying to think which slide would be helpful to look at.", "metadataJson": "{\"start\":5387,\"end\":5402}"}, {"text": "Uh, um, earlier. Oh, maybe that one. The one you were just. Just anything that kind of shows the Memcache, the overall system diagram, I guess. Okay, well, there's multiple dose, but this is the one, basically, if you think about it as a single cluster, if you will.", "metadataJson": "{\"start\":5402,\"end\":5422}"}, {"text": "Yeah. Okay.", "metadataJson": "{\"start\":5422,\"end\":5423}"}, {"text": "We can look at another one. But I think this is sort of probably good enough then. Yeah, I think this is good. Yeah, I think his question was, so shoot.", "metadataJson": "{\"start\":5425,\"end\":5438}"}, {"text": "But it was something about, like, if a client. If a front end writes.", "metadataJson": "{\"start\":5440,\"end\":5445}"}, {"text": "Yeah, if a client writes to its memcached server and that memcached server crashes and then the client immediately tries, then it presumably switches to another memcached server and then reads it again. What mechanism makes sure it doesn't see that? It sees Jerusalem, its previous. Right. I think what happens is it would probably go to the gutter, because when the memcached fails, correct.", "metadataJson": "{\"start\":5448,\"end\":5474}"}, {"text": "That client will get no response back. And when that no response comes back, it actually goes through the gutter, which has nothing in it, probably in the first try, and we'll read it from whatever database.", "metadataJson": "{\"start\":5474,\"end\":5489}"}, {"text": "Okay. Okay, that makes sense. And it's a little bit unclear exactly what happens when a new machine gets added. You know, they don't really talk much about it in the paper, but I presume this is actually the consistent hashing part where keys will sort of be automatically shifted from one machine to another.", "metadataJson": "{\"start\":5491,\"end\":5509}"}, {"text": "I guess if there were multiple clusters, wouldn't it not, maybe I just need to reread about the gutter. But wouldn't it potentially shift to another memcache B in the. Oh, actually it wouldn't. No, no, I think always when a get fails, the client goes through the gutter. Right, okay.", "metadataJson": "{\"start\":5512,\"end\":5532}"}, {"text": "Yeah, these clusters are kind of self contained with the front end and the memcache. Okay. Okay that makes, yeah, that makes perfect sense. All right, thank you. Welcome to follow up on that.", "metadataJson": "{\"start\":5532,\"end\":5543}"}, {"text": "When it falls back to the gutter, what if like two different clusters, their memcache server fails at the same time? And so they both go to the gutter and now doing like concurrent writes to the gutter, how do we ensure that those rights don't go out of order? You know, they do sets. Correct. And the writes always go to the database, to the primary, and the primary orders, all of them.", "metadataJson": "{\"start\":5543,\"end\":5573}"}, {"text": "So I think writes are always ordered. The only thing that the clients might do is set a value in or put a value into the KV server. But then they have done that after they've read from the database.", "metadataJson": "{\"start\":5575,\"end\":5591}"}, {"text": "Right. So what if someone's like doing a, they do a read and then they're setting it into the database, but like let's say two different clusters fail and then, I'm not sure if this is possible actually, but let's say like one cluster, one first reads from a key, gets back the value and then there's a write in between and then the second cluster then reads and then they both try to put into the memcache servers.", "metadataJson": "{\"start\":5593,\"end\":5620}"}, {"text": "But let's say those servers fail. Yeah, maybe. Good question. I think there's all kinds of little corner cases that actually not describe the case. I think maybe leases will help out because that server you're going to set to doesn't have the lease for the, the first client who did again got a lease back.", "metadataJson": "{\"start\":5622,\"end\":5642}"}, {"text": "Correct. To do the set. And if in the meantime the service gets replaced, the replacement server does not know that actually the lease was granted and so we'll reject the set. I'm just speculating. Correct.", "metadataJson": "{\"start\":5642,\"end\":5659}"}, {"text": "Ok. Yeah. So for the gutter, how does a control leases there? I don't know. Oh, okay, I see.", "metadataJson": "{\"start\":5660,\"end\":5668}"}, {"text": "Sorry. I can speculate, but you know, certainly. I don't know. What would you say if you had to speculate? Well, I would first have to go sit down and think a little bit about it.", "metadataJson": "{\"start\":5668,\"end\":5682}"}, {"text": "Okay. That makes sense. Yeah. Thank you.", "metadataJson": "{\"start\":5683,\"end\":5686}"}, {"text": "I have a bit of a tangential question, which is I thought it was really cool that they were using UDP for the get requests and then TCP for the others. And I was wondering like how common that is. Is that like a very standard thing to do? Yes, yes and no.", "metadataJson": "{\"start\":5688,\"end\":5704}"}, {"text": "It is. I think people prefer in general to use TCP because it provides reliability ordering and all the good great stuff, but there's real overheads with it, you know, like the, you know, the state that needs to be maintained per connections, per connection. And so there's always a little bit of a struggle, like when machines have a lot of incoming TCP connection or a lot of outgoing connections. That always causes problems. And the default, you know, if you run into that problem, is to basically do UDP type stuff.", "metadataJson": "{\"start\":5707,\"end\":5736}"}, {"text": "Isn't like novel from this paper? No, it's not novel from this paper.", "metadataJson": "{\"start\":5738,\"end\":5743}"}, {"text": "Some people like to roll their own sort of like reliably transport protocol over UDP. Like quic. Yeah, example.", "metadataJson": "{\"start\":5745,\"end\":5753}"}, {"text": "Because they mentioned they also do like sequence numbers, but presumably they don't maintain congestion windows and all the other scaling and all the other TCP features that TCP has. Thank you. You're welcome.", "metadataJson": "{\"start\":5756,\"end\":5772}"}, {"text": "Another. Oops, sorry. Go ahead.", "metadataJson": "{\"start\":5778,\"end\":5781}"}, {"text": "Okay. I guess I just wanted to quickly ask about kind of the replication between the different clusters, but basically they don't do any formal replication. Yeah. Correct. Well, yeah, no.", "metadataJson": "{\"start\":5785,\"end\":5799}"}, {"text": "Well, yes and no. Right. Because, you know, the databases need to be updated. Hold on, hold on. Let me actually go back and make sure you know what you're talking about.", "metadataJson": "{\"start\":5799,\"end\":5809}"}, {"text": "Let's see. Clusters. Yeah, we got multiple clusters. Yeah. There's no real replication going on between the clusters.", "metadataJson": "{\"start\":5811,\"end\":5817}"}, {"text": "Right. Because there's one single storage layer. Right. And so there are, are, they're kind of like depending on these leases to keep the caches up to date or every cluster is completely independent, they have nothing to do with each other. And users are divided over these clusters.", "metadataJson": "{\"start\":5817,\"end\":5839}"}, {"text": "And so one user talks in one cluster, and then within the cluster they use leases where.", "metadataJson": "{\"start\":5840,\"end\":5849}"}, {"text": "And the database invalidates leases and keys. Right. Got it. Okay. Yeah.", "metadataJson": "{\"start\":5851,\"end\":5857}"}, {"text": "So it's like the squeal and the storage that all the consistency. Yeah. All the writes, basically, in the end, just go through the storage server. Correct. All rights go through here, they get ordered and they pop out as a validation messages.", "metadataJson": "{\"start\":5857,\"end\":5874}"}, {"text": "Got it. Victim. Thank you.", "metadataJson": "{\"start\":5877,\"end\":5879}"}, {"text": "Yeah, go ahead, Brandon. Yeah. So, tangential question.", "metadataJson": "{\"start\":5884,\"end\":5888}"}, {"text": "I'm not sure this is because of the way we've kind of presented papers in the class, but it kind of seems like the way these systems are developed is like, okay, we have like, these systems, our needs are continuing to scale. So let's. Maybe this is also not an accurate representation, but it sounds like let's add another layer to kind of handle this flow or to kind of cache something or add another layer of complexity on top of it. Is it fair to say that system development has generally been like, let's just add another layer to kind of deal with no. Yes and no.", "metadataJson": "{\"start\":5890,\"end\":5930}"}, {"text": "I think the designer of the system took a very pragmatic approach to figure out, like going to run into a real problem and solve the real problem. And basically, if you think about it, you know, not a lot of additional mechanism. Right. To actually make it all work. So, in terms of.", "metadataJson": "{\"start\":5931,\"end\":5947}"}, {"text": "I mean, I think it's pretty impressive that you get this kind of performance and they have off the shelf components.", "metadataJson": "{\"start\":5948,\"end\":5953}"}, {"text": "Absolutely. People also go back once in a while, say, okay, how would I design a system to get better performance and don't, for example, have this inconsistencies between the database and the caches. And it actually turns out to be a research problem because people haven't really figured out how to do that. And so you'll see recent research papers that describe alternative solutions or new components of a solution, because, you know, any of the proposal I know of cannot support a building operation per second. Yeah, right.", "metadataJson": "{\"start\":5956,\"end\":5989}"}, {"text": "Okay, that's interesting. Thank you. Yeah, this is fascinating stuff. It's like a real world system design. I have one more question, if you don't mind.", "metadataJson": "{\"start\":5990,\"end\":6001}"}, {"text": "Yeah, go ahead. So, in the design here, where they replicate across different regions. So sorry, just to clarify, when they. The first clarification question I have is, when they replicate against different regions, each region has a bunch of internal clusters, right? Yes.", "metadataJson": "{\"start\":6001,\"end\":6016}"}, {"text": "Yes. And then my follow up question to that is, it seems like everything is. Yeah, everything's hitting the primary storage. If, let's say we wanted to scale up so that we didn't have all the writes hitting the primary storage, how would you, like, go about designing that work? Yeah.", "metadataJson": "{\"start\":6016,\"end\":6035}"}, {"text": "My suspicion is that actually the design. Okay, so there's a bunch of points of that. There's a whole other paper on this topic about actually how to do the replication. So this is not the only Facebook paper on scaling things up. There's a system that was published in 2015, or wormhole, where, you know, they have a scalable design to propagate these writes.", "metadataJson": "{\"start\":6035,\"end\":6059}"}, {"text": "My suspicion is also that they will or have shark the users to particular regions and make some regions the primary for those users.", "metadataJson": "{\"start\":6060,\"end\":6073}"}, {"text": "I see. So they assign different regions primaries for different shards. Yeah, I think so. That's what I would do or try to do. And I'm speculating here.", "metadataJson": "{\"start\":6076,\"end\":6085}"}, {"text": "Would it be a wise decision to do like a consensus protocol across the storage layers or would that just be like, too high? You could do that spent or does that. Correct. Right. And how fast is spanner?", "metadataJson": "{\"start\":6088,\"end\":6101}"}, {"text": "Pretty fast. Maybe look back in the back end of the table. How many transactions per second could you do? I do not remember the exact number. I think about 100.", "metadataJson": "{\"start\":6103,\"end\":6115}"}, {"text": "Effect for, I think wide area. Ten. Right. This is for write transactions, right? Yeah.", "metadataJson": "{\"start\":6118,\"end\":6123}"}, {"text": "Okay. Right. Right. The write transactions are very slow. Yep.", "metadataJson": "{\"start\":6124,\"end\":6127}"}, {"text": "Sorry. I think I realized that I did not understand race one. Okay, let me see if I can replicate it. Oh, let's see where we race one. I think I'm just confused.", "metadataJson": "{\"start\":6132,\"end\":6148}"}, {"text": "What is v two?", "metadataJson": "{\"start\":6149,\"end\":6150}"}, {"text": "V two. Is this right? Hold on, I'll mark it.", "metadataJson": "{\"start\":6152,\"end\":6157}"}, {"text": "Okay.", "metadataJson": "{\"start\":6159,\"end\":6160}"}, {"text": "And the problem is that it is like wedged in between the first one. Yeah.", "metadataJson": "{\"start\":6162,\"end\":6168}"}, {"text": "Okay.", "metadataJson": "{\"start\":6171,\"end\":6172}"}, {"text": "Okay. So like, we wanted it to be deleted so that the next person can re fetch it from the database, but now it's there with the old value. Yeah. So basically we have a permanent stale value. Really the issue is this permanent business or permanent between quotes?", "metadataJson": "{\"start\":6175,\"end\":6194}"}, {"text": "Correct. Because it's a cache, but they're like, now this, this put that came after, you know, basically the k being K. The k, like k being updated, you know, to v two. So we have k two here. Really?", "metadataJson": "{\"start\":6194,\"end\":6210}"}, {"text": "Correct. That's what the rag value should be. And here actually, this is v one. So let's say this is one. And what we've done here is we put a k one in after.", "metadataJson": "{\"start\":6210,\"end\":6221}"}, {"text": "And that's just not the right thing.", "metadataJson": "{\"start\":6223,\"end\":6224}"}, {"text": "Everybody that comes after now and does a get on K. Correct. Is going to get one back instead of two. Okay, that makes sense. Okay, that makes sense.", "metadataJson": "{\"start\":6228,\"end\":6236}"}, {"text": "Including client two, which is going to be bizarre. Right, right. Okay, that makes sense. Thank you so much. You're welcome.", "metadataJson": "{\"start\":6236,\"end\":6244}"}, {"text": "The part in the beginning about the evolution was also pretty helpful, I think. Okay, good, good. Thank you so much. You're welcome.", "metadataJson": "{\"start\":6245,\"end\":6253}"}]}